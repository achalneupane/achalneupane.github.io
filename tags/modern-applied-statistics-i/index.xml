<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Modern Applied Statistics I | Achal Neupane</title>
    <link>/achalneupane.github.io/tags/modern-applied-statistics-i/</link>
      <atom:link href="/achalneupane.github.io/tags/modern-applied-statistics-i/index.xml" rel="self" type="application/rss+xml" />
    <description>Modern Applied Statistics I</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Mon, 18 Nov 2019 17:26:23 -0500</lastBuildDate>
    <image>
      <url>/achalneupane.github.io/img/icon-192.png</url>
      <title>Modern Applied Statistics I</title>
      <link>/achalneupane.github.io/tags/modern-applied-statistics-i/</link>
    </image>
    
    <item>
      <title>STAT_601_Final</title>
      <link>/achalneupane.github.io/post/stat_601_final/</link>
      <pubDate>Mon, 18 Nov 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/stat_601_final/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Final Projects (Fall 2019)- Statistics 601. Due on December 18, 2019 at 5 pm central time.&lt;/p&gt;
&lt;p&gt;Work on this exam by yourself and be sure to reference any material you use on your exam. Only discuss the exam with me, Ms. Fuglsby, or Ms. Hajebi and not the other students in the class.
Do two of the following three problems. Turn in one pdf and RMD file for each problem clearly labeling which problems you have chosen to do.
1: The data shown in schizophrenia.csv were collected in a follow-up study of women patients with schizophrenia and summarized in Davis (2002), Statistical Methods for the Analysis of Repeated Measurements, Springer, New York. The binary response recorded at 0, 2, 6, 8 and 10 months after hospitalization was thought disorder (absent or present). The single covariate is the factor indicating whether a patient had suffered early or late onset of her condition (age of onset less than 20 years or age of onset 20 years or above). The question of interest is whether the course of the illness differs between patients with early and late onset? Investigate the question of interest.
i) Provide a two to three-page write-up (including graphs) explaining your analysis of the experiment and the conclusions you can draw from it.&lt;br /&gt;
ii) As a secondary component provide annotated code that replicates your analysis.
Make sure to discuss any concerns about the modeling assumptions used in your analysis.
The .csv file has the following variables.
subject
- the patient ID, a factor with levels 1 to 44.
onset
- the time of onset of the disease, a factor with levels &amp;lt; 20 yrs and &amp;gt; 20 yrs.
disorder
- whether thought disorder was absent or present, the response variable.
month
- month after hospitalization.&lt;/p&gt;
&lt;p&gt;Please note that you may have already explored this dataset in the class. Even so, please do a complete and extended analysis answering the questions, with the focus of writing and explaining the what you have found in your analysis.&lt;/p&gt;
&lt;p&gt;For this analysis, first I splitted the schizophrenia data into younger and older patients percentage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;schizophrenia2 &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/schizophrenia.csv&amp;quot;, header = TRUE, sep =&amp;quot;,&amp;quot;)

cat(&amp;quot;Calculating percentage data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calculating percentage data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;younger_percentage &amp;lt;- c( nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 0 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 0))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 2 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 2))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 6 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 6))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 8 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 8))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 10 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 10))))

older_percentage &amp;lt;- c( nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 0 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 0))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 2 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 2))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 6 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 6))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 8 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 8))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 10 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 10))))

younger_percentage %&amp;gt;% summary() %&amp;gt;% as.matrix() %&amp;gt;% t() %&amp;gt;% knitr::kable(caption = &amp;quot;Summary statistics for presence of younger onset data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary statistics for presence of younger onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.59375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;older_percentage %&amp;gt;% summary() %&amp;gt;% as.matrix() %&amp;gt;% t() %&amp;gt;% knitr::kable(caption = &amp;quot;Summary statistics for presence of older onset data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary statistics for presence of older onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0833333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2520833&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5833333&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x = seq(1:5), y=younger_percentage, type=&amp;quot;l&amp;quot;, col=1, xaxt=&amp;quot;n&amp;quot;, ylim=c(0,0.8),
     xlab=&amp;quot;Months&amp;quot;, ylab=&amp;quot;Percent present&amp;quot;, main = &amp;quot;Percent present by age at Onset&amp;quot;)
lines(older_percentage, col=4)
legend(&amp;quot;topright&amp;quot;, legend=c(&amp;quot;Percent present at Onset &amp;lt; 20 years&amp;quot;, &amp;quot;Percent Present at Onset &amp;gt; 20 years&amp;quot;), 
       col=   c(&amp;quot;1&amp;quot;, &amp;quot;4&amp;quot;), lty = c(1, 1))
axis(side=1, at=seq(1:5), labels = c(0, 2, 6, 8, 10))
axis(side=2, at=seq(1:11), labels = c(0, 0.1, .2, .3, .4, .5, .6, .7, .8, .9, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this analysis, we see that for the older-onset population, the mean and the median of the percentage of &lt;code&gt;presence of schizophrenia&lt;/code&gt; are lower. Looking at the percentage plot, it appears that at each sample date (in months) the percentage is lower in the older-onset population.
Looking at the graph of the precentages, it looks like the precentage is lower in the older-onset popullation at every sample date (in months). Both of these seem to indicate that the question is worth investigating more closely.
The mosaic plot shows an increase of ‘absence’ for both populations, through 8 months. Then, a light diminishing of absences at 10 months.&lt;/p&gt;
&lt;p&gt;We can then use GEE approach to further investigate the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GEE Independent (identity)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.7889&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GEE Exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.6867&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GEE Unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;164.011&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In exchangeable and unstructured model, the QIC score is the lowest. So for the selection of model, either model can be a good choice. However, comparing parameters such as CIC, QICu and QICC, I think the exchangeable model seems better.&lt;/p&gt;
&lt;p&gt;Additionally, we can also use linear mixed effects model (lmer) and develop different models by selecting different variables, and then compare our models.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00345048 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Without interaction term:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  179.591302394191&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  184.624563351416&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;With interaction term: month v. onset:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  184.587333241198&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  189.620892549092&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see whether which models works best in our case, we can do model comparison with ANOVA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (no intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.117087&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0774174&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Interaction Term v. No Interaction Term (fixed slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.079143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7784622&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope, No interaction Term v. Random Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.195931&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1579996&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Random Slope, No interaction term v. Fixed Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.037944&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0247979&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## No interaction term v. Interaction term (random slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0788446&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7788693&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (with intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.116788&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.077429&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model with a fixed effect and no interaction term has the lowest BIC score. This would be an indicator that the concept of contact is not likely to add much to the template if anything. The most significant difference based on ANOVA analysis is between random slope, no interaction term and fixed slope, with interaction term (fourth table). This is indicated with a p-score of 0.025 i.e., [P&amp;lt;0.05].&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;(Vole Data)- Consider the “microtus” dataset in the “Flury” library in R.
Background from Airoldi_Flury_Salvioni_JTheorBiol_1995: Discrimination Between Two Species of Microtus using both Classified and Unclassified Observations.
“1. Introduction
Microtus subterraneus and M. multiplex are now considered to be two distinct species (Niethammer, 1982; Krapp, 1982), contrary to the older view of Ellerman &amp;amp; Morrison-Scott (1951). The two species differ in the number of chromosomes: 2n=52 or 54 for M. subterraneus, and 2n=46 or 48 for M. multiplex. Hybrids from the laboratory have reduced fertility (Meylan, 1972), and hybrids from the field, whose karyotypes would be clearly recognizable, have never been found (Krapp, 1982).
The geographic ranges of distribution of M. subterraneus and M. multiplex overlap to some extent in the Alps of southern Switzerland and northern Italy (Niethammer, 1982; Krapp, 1982). M. subterraneus is smaller than M. multiplex in most measurements, and occurs at elevations from 1000 m to over 2000 m, except in the western part of its range (for example, Belgium and Brittany), where it is found in lower elevations. M. multiplex is found at similar elevations, but also at altitudes from 200–300 m south of the Alps (Ticino, Toscana).
The two chromosomal types of M. subterraneus can be crossed in the laboratory (Meylan, 1970, 1972), but no hybrids have so far been found in the field. In M. multiplex, the two chromosomal types show a distinct distribution range, but they are morphologically indistinguishable, and a hybrid has been found in the field (Storch &amp;amp; Winking, 1977).
No reliable criteria based on cranial morphology have been found to distinguish the two species. Saint Girons (1971) pointed out a difference in the sutures of the posterior parts of the premaxillary and nasal bones compared to the frontal one, but this criterion does not work well in many cases. For both paleontological and biogeographical research it would be useful to have a good rule for discriminating between the two species, because much of the data available are in form of skull remains, either fossilized or from owl pellets.
The present study was initiated by a data collection consisting of eight morphometric variables measured by one of the authors (Salvioni) using a Nikon measure-scope (accuracy 1/1000 mm) and dial calipers (accuracy 1/100 mm). The sample consists of 288 specimens collected mostly in Central Europe (Alps and Jura mountains) and in Toscana. One peculiar aspect of this data set is that the chromosomes of 89 specimens were analyzed to identify the species. Only the morphometric characteristics are available for the remaining 199 specimens…”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Develop a model from the 89 specimens that you can use to predict the group membership of the remaining 199 specimens’.
a) Explain your GLM and assess the quality of the fit with the classified observations.&lt;br /&gt;
You may want to use Cross Validation to predict the accuracy of your model.
b) Provide a two to three-page write-up (including graphs) explaining your analysis of the dataset and your recommendations on the usefulness of your predictions.
c) Provide predictions for the unclassified observations.&lt;br /&gt;
d) As a secondary component provide annotated code that replicates your analysis.&lt;/p&gt;
&lt;p&gt;The microtus data set for this final contains 288 observations. Only 89 of these obs are classified as either Subterraneus or Multiplex. The other 199 obs are unknown. The data set has 8 variables that are all measurements of the two possible species. The objective of this final project will be to predict the species of the unknown obs by modeling the 89 classified obs.&lt;/p&gt;
&lt;p&gt;The microtus dataset for this final assignement has 288 rows and 9 columns. We are using 89 of these observations which are labeled as either Subterraneus or Multiplex and the rest of the 199 observartions are unknown. We are splitting our data into known and unknown groups for prediction purposes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR2&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked _by_ &amp;#39;.GlobalEnv&amp;#39;:
## 
##     schizophrenia2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked _by_ &amp;#39;.GlobalEnv&amp;#39;:
## 
##     schizophrenia2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:HSAUR2&amp;#39;:
## 
##     agefat, aspirin, BtheB, gardenflowers, GHQ, HSAURtable, mastectomy,
##     polyps3, pottery, respiratory, schooldays, womensrole&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: splines&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: RcmdrMisc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: car&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;car&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     recode&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: sandwich&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: effects&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lattice theme set by effectsTheme()
## See ?effectsTheme for details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The Commander GUI is launched only in interactive sessions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type &amp;#39;citation(&amp;quot;pROC&amp;quot;)&amp;#39; for a citation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;pROC&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     cov, smooth, var&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;GGally&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     nasa&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, we check the relationship between the variables in our data using ggpairs plot where different classes are shown in different colours. Based on this plot (looking at the lower left points), we can say that the all of these variables are somewhat correlated which can be confirmed by their corresponding values in the upper right. Another interesting aspect of this plot is the density distribution which shows us the distribution of suberraneus, multiplex and the unknown groups, and also whether the variables shown here appear similar or different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ############################
# Now we do logistic regression of our Model followed by AIC and p value checking


model=glm(Group~M1Left+Height+Foramen,data=known,family=binomial)
AIC(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 29.09982&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Group ~ M1Left + Height + Foramen, family = binomial, 
##     data = known)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.20430  -0.01969   0.01008   0.10191   1.24456  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept) 80.578764  31.313670   2.573  0.01007 * 
## M1Left      -0.042164   0.014111  -2.988  0.00281 **
## Height      -0.026826   0.028312  -0.948  0.34336   
## Foramen      0.004990   0.003317   1.504  0.13253   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.28  on 88  degrees of freedom
## Residual deviance:  21.10  on 85  degrees of freedom
## AIC: 29.1
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# MSE of the model
MSE.glm &amp;lt;- mean((predict(model, newdata = known, type = &amp;quot;response&amp;quot;)-known$Group)^2)
MSE.glm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03622752&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Cross validation with 10 fold: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross validation with 10 fold:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;boot&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:car&amp;#39;:
## 
##     logit&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(cv.err.10 &amp;lt;- mean(cv.glm(known, model, K = 10)$delta))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04652552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My logistic regression model (GLM) with formula &lt;code&gt;Group~M1Left+Height+Foramen&lt;/code&gt; has an MSE(Mean square error) of 0.036 which was quite similar to the error values by Cross validation with 10 fold (0.051). The AIC calculated for this model was 29.09982. I then used stepwise regression below to enquire whether this model(and variables in the model) was the best model I could choose from.&lt;/p&gt;
&lt;p&gt;Now, we can also construct the classification tree by selecting all variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(party)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: modeltools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: stats4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;modeltools&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:car&amp;#39;:
## 
##     Predict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     refit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: strucchange&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: zoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;zoo&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     as.Date, as.Date.numeric&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(caret)
# model &amp;lt;- train(
#   factor(Group) ~., data = known, method = &amp;quot;ctree2&amp;quot;,
#   trControl = trainControl(&amp;quot;cv&amp;quot;, number = 10),
#   tuneGrid = expand.grid(maxdepth = 3, mincriterion = 0.95 )
#   )
# 
# plot(model$finalModel)

p1.1.tree &amp;lt;- rpart(Group~.,data=known,control = rpart.control( minsplit = 10))

library(party)
library(partykit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: libcoin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;partykit&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:party&amp;#39;:
## 
##     cforest, ctree, ctree_control, edge_simple, mob, mob_control,
##     node_barplot, node_bivplot, node_boxplot, node_inner, node_surv,
##     node_terminal, varimp&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.party(p1.1.tree),
     tp_args = list(id = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # Make predictions on the test data
# library(dplyr)
# predicted.classes &amp;lt;- model %&amp;gt;% predict(uk)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In above figure, using a tree, I want to see what variables are being chosen. These results seemed to follow along with the summary results from the model seleted below with step regression which is why the step model seems better than my GLM above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: randomForest&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MuMIn&amp;#39;:
## 
##     importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit=randomForest(factor(Group)~.,data=known)
varImpPlot(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, based on random forest model, I want to see what variables are being chosen. Based on this, Foramen seems to be the least iportant while M1left seems to the most important variables. Here, Mean DecreaseinGini is the average (mean) of a variable’s total decrease in node impurity, weighted by the proportion of samples reaching that node in each individual decision tree in the random forest. A higher Mean Decrease in Gini indicates higher variable importance.&lt;/p&gt;
&lt;p&gt;In an attempt to improve selection of variables in my model above, I performed a stepwise selection of all variables. This process determined that the lowest AIC (27.70) was found when using &lt;code&gt;M1Left + M3Left + Foramen + Length + Height&lt;/code&gt;. This model indicates that the omission of Foramen and Length as in my original model is not a good idea. Nonetheless the error for this model is even smaller (0.027) than what we had for original model, and with a much improved cross validated error of .048.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#use stepwise regression for the glm
step_known &amp;lt;- step(glm(Group ~., data = known, family = &amp;quot;binomial&amp;quot;), direction=&amp;quot;both&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start:  AIC=32.96
## Group ~ M1Left + M2Left + M3Left + Foramen + Pbone + Length + 
##     Height + Rostrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## - M2Left   1   14.965 30.965
## - Pbone    1   15.288 31.288
## - Rostrum  1   15.627 31.627
## &amp;lt;none&amp;gt;         14.962 32.962
## - Length   1   17.330 33.330
## - Height   1   18.744 34.744
## - Foramen  1   19.434 35.434
## - M3Left   1   20.654 36.654
## - M1Left   1   40.753 56.753&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Step:  AIC=30.97
## Group ~ M1Left + M3Left + Foramen + Pbone + Length + Height + 
##     Rostrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## - Pbone    1   15.306 29.306
## - Rostrum  1   15.627 29.627
## &amp;lt;none&amp;gt;         14.965 30.965
## - Length   1   18.268 32.268
## - Height   1   18.945 32.945
## + M2Left   1   14.962 32.962
## - Foramen  1   19.965 33.965
## - M3Left   1   20.763 34.763
## - M1Left   1   42.436 56.436&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Step:  AIC=29.31
## Group ~ M1Left + M3Left + Foramen + Length + Height + Rostrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## - Rostrum  1   15.703 27.703
## &amp;lt;none&amp;gt;         15.306 29.306
## - Length   1   18.625 30.625
## - Height   1   18.951 30.951
## + Pbone    1   14.965 30.965
## + M2Left   1   15.288 31.288
## - M3Left   1   20.855 32.855
## - Foramen  1   21.418 33.418
## - M1Left   1   42.970 54.970&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Step:  AIC=27.7
## Group ~ M1Left + M3Left + Foramen + Length + Height&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## &amp;lt;none&amp;gt;         15.703 27.703
## - Length   1   18.960 28.960
## - Height   1   19.019 29.019
## + Rostrum  1   15.306 29.306
## + Pbone    1   15.627 29.627
## + M2Left   1   15.694 29.694
## - M3Left   1   21.039 31.039
## - Foramen  1   21.463 31.463
## - M1Left   1   46.843 56.843&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# step_known &amp;lt;- step(glm(Group ~., data = known, family = &amp;quot;binomial&amp;quot;),trace=F, direction=&amp;quot;both&amp;quot;)

#extract the formula with the lowest aic
form_known &amp;lt;- formula(step_known)
form_known&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Group ~ M1Left + M3Left + Foramen + Length + Height&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_step &amp;lt;- glm(Group ~ M1Left + M3Left + Foramen + Length + Height, data=known,family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(model_step)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27.70264&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model_step)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Group ~ M1Left + M3Left + Foramen + Length + Height, 
##     family = binomial, data = known)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.26335  -0.00138   0.00013   0.05223   1.14144  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept) 187.830585 101.914533   1.843   0.0653 .
## M1Left       -0.058382   0.026760  -2.182   0.0291 *
## M3Left        0.024869   0.016656   1.493   0.1354  
## Foramen       0.011898   0.007164   1.661   0.0968 .
## Length       -0.041467   0.029516  -1.405   0.1600  
## Height       -0.092972   0.071107  -1.307   0.1910  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.279  on 88  degrees of freedom
## Residual deviance:  15.703  on 83  degrees of freedom
## AIC: 27.703
## 
## Number of Fisher Scoring iterations: 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# MSE from step regression best model
MSE.glm_step &amp;lt;- mean((predict(model_step, newdata = known, type = &amp;quot;response&amp;quot;)-known$Group)^2)
MSE.glm_step&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02740134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Cross validation with 10 fold: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross validation with 10 fold:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(cv.err_step.10 &amp;lt;- mean(cv.glm(known, model_step, K = 10)$delta))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: algorithm did not converge&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06012331&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now finally, the best model that was selected after step regression will be used to predict the rest of the data set that is unclassified (the unknown data: uk). The AIC for this model was 27.70 which is smaller (thus better) than our original model. Also, the MSE for this model was only 0.027 which is smaller (thus better) than our original model. Additionally, the cross validation error was 048 which is a bit higher than our MSE for this model and, but still better than the CV of our orignal model. Anyway, I will be selcting this model with AIC 0.27 from obtained step regression for the prediction purpose.&lt;/p&gt;
&lt;p&gt;The specific observation after predictions using the model will be shown below. I will also show a table of count of each species class at the bottom.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##################################################
# ### ROC curve
# # source: https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/
# model=glm(Group~M1Left+Height+Foramen, data=known, family=binomial)
# pred=predict(model,data=known, type=&amp;quot;response&amp;quot;)
# (roc(known$Group,pred))$auc ## this gives the area under curve
# plot(roc(known$Group,pred))

#######################
## Final Model
uk=subset(mc,Group==2)
known=subset(mc,Group!=2)

# Using chosen model from stepwise regression
Final_formula=Group~M1Left+M2Left+M3Left+Foramen+Height
model=glm(Final_formula,data=known,family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred=predict(model,newdata=uk,type=&amp;quot;response&amp;quot;)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Final_formula, family = binomial, data = known)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.46039  -0.00606   0.00410   0.09077   1.56483  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept) 121.314960  49.465349   2.453  0.01419 * 
## M1Left       -0.049655   0.017862  -2.780  0.00544 **
## M2Left       -0.017500   0.017327  -1.010  0.31251   
## M3Left        0.012076   0.007616   1.586  0.11283   
## Foramen       0.006361   0.003937   1.615  0.10620   
## Height       -0.060146   0.044815  -1.342  0.17957   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.279  on 88  degrees of freedom
## Residual deviance:  17.754  on 83  degrees of freedom
## AIC: 29.754
## 
## Number of Fisher Scoring iterations: 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(j in 1:nrow(uk)){
  if(pred[j]&amp;lt;0.5)
  {uk[j,1]=&amp;quot;multiplex&amp;quot;}
  if(pred[j]&amp;gt;=0.5){uk[j,1]=&amp;quot;subterraneous&amp;quot;}
}

cat(&amp;quot;This is the first 6 rows of the predictions for unclassified observations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is the first 6 rows of the predictions for unclassified observations&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(uk,6) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;M1Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;M2Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;M3Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Foramen&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pbone&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Height&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rostrum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1841&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1562&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1585&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5024&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;821&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;91&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;subterraneous&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1770&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1459&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1542&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3856&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4542&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;755&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;405&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;subterraneous&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1785&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1573&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4165&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3928&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;767&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;425&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;93&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2095&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1660&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1870&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3937&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5218&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2355&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;842&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;490&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1976&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1666&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1704&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4058&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;814&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;481&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1980&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1643&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1950&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3569&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6020&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2355&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;815&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;460&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Count of predicted class:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Count of predicted class:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(uk$Group)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;multiplex&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;subterraneous&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;128&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ## decouple the predictions 
# uk_multi=subset(uk, Group==&amp;quot;multiplex&amp;quot;)
# uk_subten=subset(uk, Group==&amp;quot;subterraneous&amp;quot;)
# 
# nrow(uk_multi) ## number of samples predicted as multiplex
# nrow(uk_subten) ## number of samples predicted as subterraneous
# 
# rownames(uk_multi) ## sample numbers of the multiplex classified 
# rownames(uk_subten) ## sample numbers of the subterraneous classified 
############################################################################&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Data Analysis and Mixed Models II</title>
      <link>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_ii/</link>
      <pubDate>Fri, 08 Nov 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_ii/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the  data from the  package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Investigate the use of other correlational structures than the independence and exchangeable structures used in the text for the respiratory data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which model is the best? Compare the following models: independent, exchangable, and what ever models you tried in part (a). Justify your answer. (Hint: use QIC (in ), MSE, misclassification rate, comparison of naive vs robust Z-score, or another method, be sure to state your method)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1a.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gee)
library(geepack)
library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MuMIn)
library(multcomp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: TH.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:HSAUR3&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;respiratory&amp;quot;, package=&amp;quot;HSAUR3&amp;quot;)
head(respiratory)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     centre treatment gender age status month subject
## 1        1   placebo female  46   poor     0       1
## 112      1   placebo female  46   poor     1       1
## 223      1   placebo female  46   poor     2       1
## 334      1   placebo female  46   poor     3       1
## 445      1   placebo female  46   poor     4       1
## 2        1   placebo female  28   poor     0       2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(respiratory)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 555   7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub &amp;lt;- subset(respiratory, month &amp;gt; &amp;quot;0&amp;quot;)
resp_sub$baseline &amp;lt;- rep(subset(respiratory, month == &amp;quot;0&amp;quot;)$status,rep(4, 111))

resp_sub$nstat &amp;lt;- as.numeric(resp_sub$status == &amp;quot;good&amp;quot;)
resp_sub$month &amp;lt;- resp_sub$month[, drop = TRUE]
head(resp_sub, n = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     centre treatment gender age status month subject baseline nstat
## 112      1   placebo female  46   poor     1       1     poor     0
## 223      1   placebo female  46   poor     2       1     poor     0
## 334      1   placebo female  46   poor     3       1     poor     0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(resp_sub)[names(resp_sub) == &amp;quot;treatment&amp;quot;] &amp;lt;- &amp;quot;trt&amp;quot;
levels(resp_sub$trt)[2] &amp;lt;- &amp;quot;trt&amp;quot;

cat(&amp;quot;Now, fitting the models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Now, fitting the models&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_glm &amp;lt;- glm(status ~ centre + trt + gender + baseline + age, data = resp_sub, family = &amp;quot;binomial&amp;quot;)

resp_sub_gee1 &amp;lt;- gee(nstat ~ centre + trt+ gender + baseline + age, data = resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;independence&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_gee2 &amp;lt;- gee(nstat ~ centre + trt + gender + baseline + age, data =resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;exchangeable&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_gee3 &amp;lt;- gee(nstat ~ centre + trt + gender + baseline + age, data =resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;unstructured&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_gee4 &amp;lt;- gee(nstat ~ centre + trt + gender + baseline + age, data =resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;AR-M&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot; Summary&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_glm)$coeff, caption = &amp;quot;Summary coeff of GLM binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GLM binomial&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;z value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|z|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9001713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3376530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.6659658&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0076767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6716010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2395666&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.8034004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0050567&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2992159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2368410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.4856047&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1192436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2946710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4046671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6857223&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8820286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2412902&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.7998563&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0181659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0088644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.0493065&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0404322&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee1)$coef, caption = &amp;quot;Summary coeff of GEE binomial independent&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial independent&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9001713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3376531&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.665965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4603270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9555041&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6716010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2395666&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.803400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3568191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8821889&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2992159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2368410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.485603&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3507780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.7038127&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1192436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2946710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.404667&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4432023&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2690501&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8820286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2412902&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.799855&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3500515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.3764332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0181659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0088644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.049306&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0130043&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3969169&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee2)$coef, caption = &amp;quot;Summary coeff of GEE binomial exchangeable&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9001713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4784634&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.8813796&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4603270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9555041&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6716010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3394723&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9783676&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3568191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8821889&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2992159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3356101&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8712064&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3507780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.7038127&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1192436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4175568&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2855747&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4432023&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2690501&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8820286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3419147&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5043802&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3500515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.3764332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0181659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0125611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.4462014&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0130043&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3969169&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee3)$coef, caption = &amp;quot;Summary coeff of GEE binomial unstructured&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9312798&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4791852&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9434655&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4612499&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.0190352&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6727947&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3390779&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9841895&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3548202&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8961568&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2789154&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3354409&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8126404&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3494500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.6597956&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0946735&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4172964&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2268736&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4436295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2134068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9346252&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3428184&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.6432949&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3480468&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5585200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0168892&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0125574&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3449620&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0129054&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3086948&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee4)$coef, caption = &amp;quot;Summary coeff of GEE binomial AR-M&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial AR-M&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9629448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4455161&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.1614142&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4611607&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.0880894&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7427015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3146448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.3604438&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3562300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.0848932&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2472824&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3112665&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.0071210&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3518974&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.5444492&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1132323&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3883671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2915599&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4494506&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2519348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9113953&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3167921&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0335952&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3501873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.4582088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0169164&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0116652&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.4501525&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0129273&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3085816&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the difference between Naive z and Robust z with relevant variables for the above models, resp_sub_gee3 with unstructured comparison correlation structure model tends to have a minimal difference between Naive z and Robust z than others. Therefore, in this case, unstructured model fits best.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;b. Which model is the best? Compare the following models: independent, exchangable, and what ever models you tried in part (a). Justify your answer. (Hint: use QIC (in \textbf{MESS}), MSE, misclassification rate, comparison of naive vs robust Z-score, or another method, be sure to state your method)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MESS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MuMIn&amp;#39;:
## 
##     QIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE Independent&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE independent&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;508.5300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.2182&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.6091&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.6559&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;508.7222&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE Exchangeable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.795035&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.218168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.609084&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.288434&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;496.051916&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE Unstructured&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE Unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.741778&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.336132&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.668066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.202823&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;496.465676&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE ar1&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE ar1&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;496.834891&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.642425&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.821212&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.596233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;497.091771&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As mentioned above in 1a., resp_sub_gee3 with unstructured correlation structure model has slightly minimal difference of Naive z and Robust z than others. I used the QIC value to compare the result, it looks like the third model with unstructured correlation structure model has a lower QIC value of 495.7418 than the other three models. So, the model with a corr unstructured. So, the model with unstructured correlation structure is the best among four.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The data set  from  package were collected in a follow-up study of women patients with schizophrenia (Davis, 2002). The binary response recorded at 0, 2, 6, 8 and 10 months after hospitalization was ``thought disorder’’ (absent or present). The single covariate is the factor indicating whether a patient had suffered early or late onset of her condition (age of onset less than 20 years or age of onset 20 years or above). The question of interest is whether the course of the illness differs between patients with early and late onset schizophrenia. (&lt;a href=&#34;https://www.rdocumentation.org/packages/HSAUR3/versions/1.0-9/topics/schizophrenia2&#34; class=&#34;uri&#34;&gt;https://www.rdocumentation.org/packages/HSAUR3/versions/1.0-9/topics/schizophrenia2&lt;/a&gt;)
Investigate this question using&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;plots and summary statistics&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the GEE approach&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;mixed effects model (lmer) from previous chapter&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is there a difference? What model(s) work best? Describe your results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2a.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calculating percentage data&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 3: &lt;/span&gt;Summary statistics for presence of younger onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.59375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 3: &lt;/span&gt;Summary statistics for presence of older onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0833333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2520833&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5833333&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_II_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;For the older-onset population, the mean and the median of the percentage of’ presence of schizophrenia’ are lower. Looking at the percentage plot, it appears that at each sample date (in months) the percentage is lower in the older-onset population.&lt;/p&gt;
&lt;p&gt;2b. the GEE approach&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 4: &lt;/span&gt;QIC from GEE Independent (identity)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.788881&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;159.831542&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-75.915771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.978669&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;164.046945&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 4: &lt;/span&gt;QIC from GEE Exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;159.800970&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.156189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-76.078094&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.822391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.190581&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 4: &lt;/span&gt;QIC from GEE Unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.480447&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.652702&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-76.326351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.913873&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.956957&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;In exchangeable and unstructured model, the QIC score is the lowest. So for the selection of model, either model can be a good choice. However, comparing parameters such as CIC, QICu and QICC, I think the exchangeable model seems better.&lt;/p&gt;
&lt;p&gt;2c. mixed effects model (lmer) from previous chapter&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00345048 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Without interaction term:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  179.591302394191&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  184.624563351416&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;With interaction term: month v. onset:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  184.587333241198&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  189.620892549092&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Answer: Please see the outputs from lmer&lt;/p&gt;
&lt;p&gt;2d
&lt;strong&gt;Answer to 2d: Model comparison with ANOVA&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (no intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.117087&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0774174&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Interaction Term v. No Interaction Term (fixed slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.079143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7784622&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope, No interaction Term v. Random Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.195931&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1579996&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Random Slope, No interaction term v. Fixed Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.037944&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0247979&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## No interaction term v. Interaction term (random slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0788446&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7788693&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (with intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.116788&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.077429&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;The model with a fixed effect and no interaction term has the lowest BIC score. This would be an indicator that the concept of contact is not likely to add much to the template if anything. The most significant difference based on ANOVA analysis is between random slope, no interaction term and fixed slope, with interaction term (fourth table). This is indicated with a p-score of 0.025 i.e., [P&amp;lt;0.05].&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Data Analysis and Mixed Models I</title>
      <link>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_i/</link>
      <pubDate>Mon, 28 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_i/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Following up with the Beat the Blues data from the video (package HSAUR3) do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Construct boxplots to compare the factor variable  in an analogous way to how we constructed boxplots in the video for the treatment variable. Discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat (a) for the  variable. Discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the  function to fit a model to the Beat the Blues data that assumes that the repeated measurements are independent. Compare the results to those from fitting the random intercept model  from the video.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Investigate and discuss whether there is any evidence of an interaction between treatment and time for the Beat the Blues data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Construct a plot of the mean profiles of both treatment groups in the Beat the Blues data, showing also standard deviation bars at each time point.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;gee&amp;quot;)
library(&amp;quot;lme4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Matrix&amp;quot;)
library(&amp;quot;multcomp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: TH.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library (HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:TH.data&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(plyr)
library(tidyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;tidyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# 1.a&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1.a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a. Construct boxplots to compare the factor variable \textbf{drug} in an
# analogous way to how we constructed boxplots in the video for the treatment
# variable. Discuss the results.
data(&amp;quot;BtheB&amp;quot;)

layout(matrix(1:2, nrow=1))

ylim=range(BtheB[ , grep(&amp;quot;bdi&amp;quot;, names(BtheB))],
           na.rm=TRUE)

yes=subset(BtheB, drug==&amp;quot;Yes&amp;quot;)[ , grep(&amp;quot;bdi&amp;quot;, names(BtheB))]
boxplot(yes, main=&amp;quot;Drugs Used&amp;quot;, ylab=&amp;quot;BDI&amp;quot;,
        xlab=&amp;quot;Time (in Months)&amp;quot;, names=c(0,2,3,5,8), 
        ylim=ylim)

no=subset(BtheB, drug==&amp;quot;No&amp;quot;)[ , grep(&amp;quot;bdi&amp;quot;, names(BtheB))]
boxplot(no, main=&amp;quot;Drugs Not Used&amp;quot;, ylab=&amp;quot;BDI&amp;quot;,
        xlab=&amp;quot;Time (in Months)&amp;quot;, names=c(0,2,3,5,8), 
        ylim=ylim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# b. Repeat (a) for the \textbf{length} variable. Discuss the results.

layout(matrix(1:2, nrow = 1))
lengthlow&amp;lt;-subset(BtheB,length==&amp;quot;&amp;lt;6m&amp;quot;)[,grep(&amp;quot;bdi&amp;quot;,names(BtheB))]
boxplot(lengthlow, main =&amp;quot;Length&amp;lt;6m&amp;quot;, ylab= &amp;quot;BDI&amp;quot;, xlab = &amp;quot;Time (in months)&amp;quot;,names=c(0,2,3,5,8))

lengthhigh&amp;lt;-subset(BtheB,length==&amp;quot;&amp;gt;6m&amp;quot;)[,grep(&amp;quot;bdi&amp;quot;,names(BtheB))]
boxplot(lengthhigh, main =&amp;quot;Length&amp;gt;6m&amp;quot;, ylab= &amp;quot;BDI&amp;quot;, xlab = &amp;quot;Time (in months)&amp;quot;,names=c(0,2,3,5,8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.c&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# c. Use the \textit{lm} function to fit a model to the Beat the Blues
# data that assumes that the repeated measurements are independent.
# Compare the results to those from fitting the random intercept model
# \textit{BtheB\_lmer1} from the video.

BtheB$subject=factor(rownames(BtheB))
nobs=nrow(BtheB)
cat(&amp;quot;Let change the data in wide format&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Let change the data in wide format&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_long=reshape(BtheB, idvar = &amp;quot;subject&amp;quot;,
                   varying = c(&amp;quot;bdi.2m&amp;quot;, &amp;quot;bdi.3m&amp;quot;, &amp;quot;bdi.5m&amp;quot;, &amp;quot;bdi.8m&amp;quot;),
                   direction = &amp;quot;long&amp;quot;)
BtheB_long$time=rep(c(2, 3, 5, 8), rep(nobs, 4))


BtheB_lm=lm(bdi ~ bdi.pre + time + treatment + drug + 
              length, data=BtheB_long, na.action=na.omit)

summary(BtheB_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bdi ~ bdi.pre + time + treatment + drug + length, 
##     data = BtheB_long, na.action = na.omit)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -23.6779  -5.4177   0.0151   5.3268  27.2688 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     7.32290    1.72765   4.239 3.08e-05 ***
## bdi.pre         0.57396    0.05497  10.440  &amp;lt; 2e-16 ***
## time           -0.93784    0.23650  -3.965 9.36e-05 ***
## treatmentBtheB -3.32254    1.10069  -3.019  0.00278 ** 
## drugYes        -3.56866    1.14717  -3.111  0.00206 ** 
## length&amp;gt;6m       1.71067    1.11056   1.540  0.12463    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 8.674 on 274 degrees of freedom
##   (120 observations deleted due to missingness)
## Multiple R-squared:  0.395,  Adjusted R-squared:  0.384 
## F-statistic: 35.78 on 5 and 274 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_lmer1=lmer(bdi ~ bdi.pre + time + treatment + drug +
                   length + (1 | subject), data=BtheB_long,
                 REML=FALSE, na.action=na.omit)

cftest(BtheB_lmer1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lmer(formula = bdi ~ bdi.pre + time + treatment + drug + length + 
##     (1 | subject), data = BtheB_long, REML = FALSE, na.action = na.omit)
## 
## Linear Hypotheses:
##                     Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) == 0     5.59239    2.24244   2.494   0.0126 *  
## bdi.pre == 0         0.63968    0.07789   8.212 2.22e-16 ***
## time == 0           -0.70476    0.14639  -4.814 1.48e-06 ***
## treatmentBtheB == 0 -2.32908    1.67036  -1.394   0.1632    
## drugYes == 0        -2.82495    1.72684  -1.636   0.1019    
## length&amp;gt;6m == 0       0.19708    1.63832   0.120   0.9043    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Univariate p values reported)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Comparison of lm and lmer&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Comparison of lm and lmer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(BtheB_lmer1,BtheB_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: BtheB_long
## Models:
## BtheB_lm: bdi ~ bdi.pre + time + treatment + drug + length
## BtheB_lmer1: bdi ~ bdi.pre + time + treatment + drug + length + (1 | subject)
##             Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&amp;gt;Chisq)    
## BtheB_lm     7 2012.3 2037.7 -999.15   1998.3                            
## BtheB_lmer1  8 1887.5 1916.6 -935.75   1871.5 126.8      1  &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;We can plot the residuals from Linear model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## We can plot the residuals from Linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(BtheB_lm, which=2)
# Assumptions for Mixed-model

residuals &amp;lt;- function(object, obs) obs-predict(object) 
cat(&amp;quot;We can also plot the residuals from linear mixed-model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## We can also plot the residuals from linear mixed-model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:2, ncol = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qint &amp;lt;- ranef(BtheB_lmer1)$subject[[&amp;quot;(Intercept)&amp;quot;]]
qres &amp;lt;- residuals(BtheB_lmer1,BtheB_long$bdi.pre)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in obs - predict(object): longer object length is not a multiple of
## shorter object length&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqnorm(qint, ylab = &amp;quot;Estimated random intercepts&amp;quot;,
     xlim = c(-3, 3), ylim = c(-20, 20),
     main = &amp;quot;Random intercepts&amp;quot;)
qqline(qint, col=&amp;quot;red&amp;quot;, lwd=3)
qqnorm(qres, xlim = c(-3, 3), ylim = c(-20, 20),
     ylab = &amp;quot;Estimated residuals&amp;quot;,
     main = &amp;quot;Residuals&amp;quot;)
qqline(qres, col=&amp;quot;red&amp;quot;, lwd=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.d&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# d. Investigate and discuss whether there is any evidence of an interaction
# between treatment and time for the Beat the Blues data.

cat(&amp;quot;Testing fitted linear model with lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Testing fitted linear model with lm&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_lm2=lm(bdi ~ bdi.pre + time + treatment + drug +
                   length + time*treatment + subject, 
                 data=BtheB_long, REML=FALSE, na.action=na.omit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
##  extra argument &amp;#39;REML&amp;#39; will be disregarded&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(BtheB_lm2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bdi ~ bdi.pre + time + treatment + drug + length + 
##     time * treatment + subject, data = BtheB_long, na.action = na.omit, 
##     REML = FALSE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.142  -2.135   0.000   1.475  18.851 
## 
## Coefficients: (4 not defined because of singularities)
##                      Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)          29.76770    9.56265   3.113 0.002154 ** 
## bdi.pre              -1.65000    0.70622  -2.336 0.020565 *  
## time                 -0.95949    0.21348  -4.494 1.24e-05 ***
## treatmentBtheB      -19.42712   11.68602  -1.662 0.098158 .  
## drugYes               6.00000    3.53108   1.699 0.091000 .  
## length&amp;gt;6m            22.48102   12.10631   1.857 0.064941 .  
## subject10             3.10000    4.52199   0.686 0.493881    
## subject11            19.06898    4.40270   4.331 2.45e-05 ***
## subject12           100.14125   35.32423   2.835 0.005105 ** 
## subject13            18.05000    5.42455   3.327 0.001061 ** 
## subject14            23.31898    5.41836   4.304 2.75e-05 ***
## subject15            15.80000    6.07510   2.601 0.010070 *  
## subject16             5.45000    4.11791   1.323 0.187343    
## subject17            46.96818   12.31014   3.815 0.000186 ***
## subject18            19.78102   12.16794   1.626 0.105760    
## subject19            42.11898    7.12772   5.909 1.67e-08 ***
## subject2             34.65000   11.83833   2.927 0.003862 ** 
## subject20            52.83102   25.34385   2.085 0.038512 *  
## subject21             8.46022    7.50845   1.127 0.261336    
## subject22            33.95000    9.83645   3.451 0.000694 ***
## subject23            14.00000    4.99370   2.804 0.005606 ** 
## subject24             3.67026    8.82128   0.416 0.677853    
## subject25            27.12026    6.73771   4.025 8.36e-05 ***
## subject26            11.81818    5.55378   2.128 0.034694 *  
## subject27             0.95128    6.84960   0.139 0.889699    
## subject28            62.16624   12.24689   5.076 9.51e-07 ***
## subject29            24.00000   14.55901   1.648 0.100992    
## subject3             27.40128    9.26385   2.958 0.003511 ** 
## subject30            12.58102   10.30362   1.221 0.223663    
## subject31            28.60000   10.49865   2.724 0.007078 ** 
## subject32            29.28102   15.42181   1.899 0.059198 .  
## subject33            10.23102    9.57607   1.068 0.286765    
## subject34             1.19125    8.26315   0.144 0.885531    
## subject35            56.95000   20.08684   2.835 0.005101 ** 
## subject36            72.85128   18.52482   3.933 0.000120 ***
## subject37            19.95000    6.66242   2.994 0.003135 ** 
## subject38            13.91898    5.59943   2.486 0.013833 *  
## subject39             3.29920    8.84068   0.373 0.709448    
## subject4             16.25000    6.11601   2.657 0.008590 ** 
## subject40            32.76898    7.02198   4.667 5.94e-06 ***
## subject41           100.19125   33.25146   3.013 0.002956 ** 
## subject42            26.05000    6.07510   4.288 2.93e-05 ***
## subject43           -18.13102   11.53675  -1.572 0.117793    
## subject44            17.51022    6.19857   2.825 0.005261 ** 
## subject45            38.15000   18.69805   2.040 0.042771 *  
## subject46            69.79125   23.95835   2.913 0.004031 ** 
## subject47            61.53102   23.96835   2.567 0.011061 *  
## subject48            10.84579   11.62898   0.933 0.352242    
## subject49            -7.40875    7.23325  -1.024 0.307078    
## subject5             27.71022    9.01742   3.073 0.002447 ** 
## subject50            55.96898    6.58205   8.503 6.75e-15 ***
## subject51            59.90128   16.51807   3.626 0.000374 ***
## subject52            48.24125   19.27469   2.503 0.013206 *  
## subject53            43.46898    4.56947   9.513  &amp;lt; 2e-16 ***
## subject54             3.64920    8.34740   0.437 0.662512    
## subject55            28.08102    7.50271   3.743 0.000244 ***
## subject56             0.63102    7.46940   0.084 0.932767    
## subject57             9.23102    4.40270   2.097 0.037412 *  
## subject58            73.13102   19.55676   3.739 0.000247 ***
## subject59            52.24125   17.72452   2.947 0.003627 ** 
## subject6             -3.36898    7.46940  -0.451 0.652502    
## subject60            -6.08978    5.33361  -1.142 0.255056    
## subject61             7.36898    6.58205   1.120 0.264386    
## subject62            10.01898    6.06957   1.651 0.100536    
## subject63             9.18060    4.07475   2.253 0.025457 *  
## subject64           -19.51709   15.51635  -1.258 0.210070    
## subject65             5.61022    5.42631   1.034 0.302566    
## subject66           -34.50043   16.19268  -2.131 0.034471 *  
## subject67            -0.15000    4.78979  -0.031 0.975052    
## subject68            55.89920   24.10162   2.319 0.021495 *  
## subject69            36.70128   11.36703   3.229 0.001476 ** 
## subject7              2.60000    4.78979   0.543 0.587920    
## subject70            64.84125   21.93711   2.956 0.003534 ** 
## subject71            10.18102    9.70541   1.049 0.295574    
## subject72            54.96818   19.13108   2.873 0.004549 ** 
## subject73           -24.20000   13.65756  -1.772 0.078092 .  
## subject74            -4.99872    6.27981  -0.796 0.427075    
## subject75            -3.33102   11.44996  -0.291 0.771446    
## subject76            23.80000    9.18080   2.592 0.010310 *  
## subject77             4.85000    3.23629   1.499 0.135711    
## subject78             6.90000    5.51572   1.251 0.212560    
## subject79            19.10128    9.01832   2.118 0.035536 *  
## subject8              3.31898    7.69955   0.431 0.666937    
## subject80             9.46898    7.12772   1.328 0.185695    
## subject81            20.36898    4.56947   4.458 1.45e-05 ***
## subject82            -0.08978    5.78228  -0.016 0.987630    
## subject83           -18.18102   14.10427  -1.289 0.199028    
## subject84             8.95000    6.07510   1.473 0.142427    
## subject85            82.36818   21.62732   3.809 0.000191 ***
## subject86            40.75000   16.18143   2.518 0.012659 *  
## subject87            16.76022    7.47517   2.242 0.026168 *  
## subject88             5.56898    5.86055   0.950 0.343253    
## subject89            50.53102   23.96835   2.108 0.036387 *  
## subject9             29.28102   13.89048   2.108 0.036408 *  
## subject90            30.93102   16.08662   1.923 0.056078 .  
## subject92            60.94920   22.05961   2.763 0.006320 ** 
## subject93            21.76246   13.32765   1.633 0.104232    
## subject94             3.20000    4.94351   0.647 0.518249    
## subject95                  NA         NA      NA       NA    
## subject96                  NA         NA      NA       NA    
## subject98                  NA         NA      NA       NA    
## subject99                  NA         NA      NA       NA    
## time:treatmentBtheB   0.64358    0.29757   2.163 0.031867 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.994 on 181 degrees of freedom
##   (120 observations deleted due to missingness)
## Multiple R-squared:  0.8675, Adjusted R-squared:  0.7958 
## F-statistic:  12.1 on 98 and 181 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Testing fitted linear mixed-model with lmer subject as a random effect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Testing fitted linear mixed-model with lmer subject as a random effect&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_lmer2=lmer(bdi ~ bdi.pre + time + treatment + drug +
                   length + time*treatment + (1 | subject), 
                 data=BtheB_long, REML=FALSE, na.action=na.omit)

cftest(BtheB_lmer2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lmer(formula = bdi ~ bdi.pre + time + treatment + drug + length + 
##     time * treatment + (1 | subject), data = BtheB_long, REML = FALSE, 
##     na.action = na.omit)
## 
## Linear Hypotheses:
##                          Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) == 0          6.47824    2.31905   2.793  0.00521 ** 
## bdi.pre == 0              0.64046    0.07843   8.166 2.22e-16 ***
## time == 0                -0.95555    0.20831  -4.587 4.49e-06 ***
## treatmentBtheB == 0      -4.09804    1.98490  -2.065  0.03896 *  
## drugYes == 0             -2.79209    1.73987  -1.605  0.10854    
## length&amp;gt;6m == 0            0.21905    1.65044   0.133  0.89441    
## time:treatmentBtheB == 0  0.49000    0.28959   1.692  0.09064 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Univariate p values reported)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.e&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.e&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# e. Construct a plot of the mean profiles of both treatment groups in the Beat
# the Blues data, showing also standard deviation bars at each time point.

data_sum=ddply(BtheB_long, c(&amp;quot;time&amp;quot;, &amp;quot;treatment&amp;quot;), summarise, 
            N=sum(!is.na(bdi)),
            mean=mean(bdi, na.rm=TRUE),
            sd=sd(bdi, na.rm=TRUE),
            se=sd/sqrt(N))

pd &amp;lt;- position_dodge(0.1)

ggplot(data_sum, aes(x=time, y=mean, colour=treatment)) +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=0.1, position=pd) +
  geom_line(position=pd) +
  geom_point(position=pd) + 
  theme_classic() +
  labs(title = &amp;quot;Mean profiles of both treatment groups&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;1a.&lt;/p&gt;
&lt;p&gt;The plots demonstrate that BDI (median or average) declines at a faster rate they do not take the medication.&lt;/p&gt;
&lt;p&gt;1b.&lt;/p&gt;
&lt;p&gt;The plots demonstrate that when patients live for less than 6 months, BDI (median or average) declines at a higher pace than when patients survive for more than months.&lt;/p&gt;
&lt;p&gt;1c.&lt;/p&gt;
&lt;p&gt;In linear model, Bdi.pre, time, treatmentBtheB, and drugYes are significant, but in the lmer model, only bdi.pre and time are significant. This means that the linear model understates the standard errors because it does not account for the correlation over time or the repeated measures.&lt;/p&gt;
&lt;p&gt;1d.&lt;/p&gt;
&lt;p&gt;In the first output, the linear model shows that there is interation between time and treatment with P &amp;lt; 0.05. However, mixed effect model (lmer) indicates that there is no time and treatment effect because p is 0.09.&lt;/p&gt;
&lt;p&gt;1e.&lt;/p&gt;
&lt;p&gt;The mean profile shows that the results are very different between the two categories, but the standard deviation bars converge.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the  data from the package HSAUR3. This data shows the plasma inorganic phosphate levels for 33 subjects, 20 of whom are controls and 13 of whom have been classified as obese (Davis, 2002). Perform the following on this dataset&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Construct boxplots by group and discuss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce separate plots of the profiles of the individuals in each group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Guided by how these plots fit, which linear mixed effects models do you think might be sensible? (Hint: Discuss intercept and slope, intercept and interaction).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convert the data to long version and fit the model of your choice and discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;phosphate&amp;quot;, package = &amp;quot;HSAUR3&amp;quot;)
library(lme4)
library(Matrix)
library(multcomp)
attach(phosphate)
par(mfrow = c(1,2))
ylim &amp;lt;-range(phosphate[,-1], na.rm = TRUE)
cat(&amp;quot;#2.a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2.a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a. Construct boxplots by group and discuss. 
boxplot(phosphate[group == &amp;quot;control&amp;quot;, -1], 
        xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Phosphate level&amp;quot;,
        main = &amp;quot;base R: Control group&amp;quot;, ylim = ylim)
boxplot(phosphate[group == &amp;quot;obese&amp;quot;,-1],
        xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Phosphate level&amp;quot;,
        main = &amp;quot;Obese group&amp;quot;, ylim = ylim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version:

phosphate %&amp;gt;%
  gather(time,phosphate_level, 2:9) %&amp;gt;%
  mutate(time = factor(gsub(time,pattern=&amp;quot;[[:alpha:]]&amp;quot;,replacement=&amp;#39;&amp;#39;))) %&amp;gt;%
ggplot(aes(x=time,y=phosphate_level)) +
  geom_boxplot() +
  facet_grid(.~group) +
  labs(title=&amp;#39;ggplot: Phosphate Data by Groups&amp;#39;,
       x=&amp;#39;Time (hours)&amp;#39;,
       y=&amp;#39;Phosphate level&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2.b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2.b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# b. Produce separate plots of the profiles of the individuals in each group
layout(matrix(1:2, nrow=1))

rg=range(phosphate[ ,-1])
plot(c(1, 8), c(rg), type=&amp;quot;n&amp;quot;, xlab=&amp;quot;Time (hours)&amp;quot;, 
     ylab=&amp;quot;Phosphate Levels&amp;quot;, main=&amp;quot;base R: Control Group&amp;quot;)
for(i in 1:sum(phosphate$group==&amp;quot;control&amp;quot;)){
  lines(1:8, phosphate[i, -1])
}

plot(c(1, 8), c(rg), type=&amp;quot;n&amp;quot;, xlab=&amp;quot;Time (hours)&amp;quot;, 
     ylab=&amp;quot;Phosphate Levels&amp;quot;, main=&amp;quot;Obese Group&amp;quot;)
for(i in 1:sum(phosphate$group==&amp;quot;obese&amp;quot;)){
  lines(1:8, phosphate[nrow(phosphate)-i, -1])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phosphate_t &amp;lt;- phosphate
phosphate_t$subject &amp;lt;- factor(rownames(phosphate_t))

phosphate_t %&amp;gt;%
  gather(time,phosphate_level, 2:9) %&amp;gt;%
  mutate(time = factor(gsub(time,pattern=&amp;quot;[[:alpha:]]&amp;quot;,replacement=&amp;#39;&amp;#39;))) %&amp;gt;%
ggplot(aes(x=time,y=phosphate_level)) +
  geom_line(aes(group=subject,color=subject),size=1) +
  facet_grid(.~group) +
  labs(title=&amp;#39;ggplot: Phosphate by group&amp;#39;,
       x=&amp;#39;Time (in hours)&amp;#39;,
       y=&amp;#39;Phosphate Level&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2.d&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#d. Convert the data to long version and fit the model of your choice and
#discuss the results.
data(phosphate)
ph=phosphate
ph$subject=as.factor(rownames(ph))
##LOng form
a=ph[,c(3:9)]

ph_long=reshape(ph,idvar=&amp;quot;subject&amp;quot;,v.names=&amp;quot;p_level&amp;quot;,
                varying=list(names(a)),direction=&amp;quot;long&amp;quot;)

timer=seq(1:7)
timer[1]=0.5
timer[2]=1
timer[3]=1.5
for(i in 4:7){
  timer[i]=i-2}

for(i in 1:7){
  for(j in 1:33){
    ph_long[(33*(i-1)+j),4]=timer[i]
  }
}

ph_non_inter=lmer(p_level~time+group+t0+(1|subject),data=ph_long,
                  REML=FALSE,na.actio=na.omit)

ph_inter=lmer(p_level~time+group+t0+time*group+(1|subject),
              data=ph_long,REML=FALSE,na.actio=na.omit)

ph_cept_slope=lmer(p_level~time+group+t0+
                     (time|subject),
              data=ph_long,REML=FALSE,na.actio=na.omit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## unable to evaluate scaled gradient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge: degenerate Hessian with 1 negative eigenvalues&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ph_cept_slope_inter=lmer(p_level~time+group+t0+time*group+
                           (time|subject),
                         data=ph_long,REML=FALSE,na.actio=na.omit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.0119258 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(ph_cept_slope_inter))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: p_level ~ time + group + t0 + time * group + (time | subject)
##    Data: ph_long
## 
##      AIC      BIC   logLik deviance df.resid 
##    391.4    422.4   -186.7    373.4      222 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4111 -0.5836 -0.0419  0.5861  3.5259 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  subject  (Intercept) 0.009286 0.09636      
##           time        0.004514 0.06718  0.99
##  Residual             0.250017 0.50002      
## Number of obs: 231, groups:  subject, 33
## 
## Fixed effects:
##                  Estimate Std. Error t value
## (Intercept)      0.001779   0.339884   0.005
## time             0.174229   0.031570   5.519
## groupobese       0.636604   0.137868   4.617
## t0               0.691589   0.079454   8.704
## time:groupobese -0.211030   0.050299  -4.196
## 
## Correlation of Fixed Effects:
##             (Intr) time   gropbs t0    
## time        -0.144                     
## groupobese   0.150  0.356              
## t0          -0.970  0.000 -0.304       
## time:gropbs  0.091 -0.628 -0.568  0.000
## convergence code: 0
## Model failed to converge with max|grad| = 0.0119258 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(cftest(ph_cept_slope_inter))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lmer(formula = p_level ~ time + group + t0 + time * group + (time | 
##     subject), data = ph_long, REML = FALSE, na.action = na.omit)
## 
## Linear Hypotheses:
##                       Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) == 0      0.001779   0.339884   0.005    0.996    
## time == 0             0.174229   0.031570   5.519 3.41e-08 ***
## groupobese == 0       0.636604   0.137868   4.617 3.88e-06 ***
## t0 == 0               0.691589   0.079454   8.704  &amp;lt; 2e-16 ***
## time:groupobese == 0 -0.211030   0.050299  -4.196 2.72e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Univariate p values reported)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(ph_inter,ph_non_inter,ph_cept_slope,ph_cept_slope_inter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: ph_long
## Models:
## ph_non_inter: p_level ~ time + group + t0 + (1 | subject)
## ph_inter: p_level ~ time + group + t0 + time * group + (1 | subject)
## ph_cept_slope: p_level ~ time + group + t0 + (time | subject)
## ph_cept_slope_inter: p_level ~ time + group + t0 + time * group + (time | subject)
##                     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)
## ph_non_inter         6 411.77 432.43 -199.89   399.77                         
## ph_inter             7 393.19 417.29 -189.59   379.19 20.585      1  5.705e-06
## ph_cept_slope        8 404.23 431.76 -194.11   388.23  0.000      1  1.0000000
## ph_cept_slope_inter  9 391.45 422.43 -186.72   373.45 14.775      1  0.0001211
##                        
## ph_non_inter           
## ph_inter            ***
## ph_cept_slope          
## ph_cept_slope_inter ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred=predict(ph_cept_slope_inter)
MSE=mean((pred-ph_long$p_level)^2)
message(&amp;quot;Mean Squared Error: &amp;quot;,MSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Mean Squared Error: 0.225552916473035&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;Mean Phosphate Level: &amp;quot;,mean(ph_long$p_level))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Mean Phosphate Level: 3.48744588744589&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.a&lt;/p&gt;
&lt;p&gt;Based on the plot, the control group has the median level of phosphate falling rapidly from a higher level of 4.3 to midway between t1 and t2 and starts to rise more rapidly over time. The obese group reached a relatively higher median phosphate rate close to 5 and dropped and grew steadily over time. The Obese group also has two outliers.&lt;/p&gt;
&lt;p&gt;2.c&lt;/p&gt;
&lt;p&gt;I think linear mixed model would be suitable because the slopes and intercepts in each group vary widely. Additionally, there is an interaction between control and obese groups based on time. So fitting something of the form lmer(phosphatelevel t0+time∗group+(1= phos) would be suitable.&lt;/p&gt;
&lt;p&gt;2.d&lt;/p&gt;
&lt;p&gt;Based on the model I used:
In the fixed effect we see that there is a positive relation between the time and the phosphate level. It makes sense sinse we saw that after decreasing for a while the phosphate level increases for longer than decreasing time period. Based on the f-test with cftest , time, group, baseline phosphate level and the interaction between time and group all are significant.&lt;/p&gt;
&lt;p&gt;Comparing this with three other models, (ph_non_inter=random intercept, ph_inter=random intercept with interction, ph_cept_slope=random intercept and random slope) using anova, the model I used has the smallest AIC (about 391) indicating a better fit of among four models. However, considering BIC value, we also see that random intercept with interaction is also a good model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantile Regression</title>
      <link>/achalneupane.github.io/post/quantile_regression/</link>
      <pubDate>Fri, 18 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/quantile_regression/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the {} data from the {} package&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Review the linear model fitted to this data in Chapter 6 of the text book and report the model and findings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a median regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare the two results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1     ✔ purrr   0.3.3
## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mboost)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: parallel&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: stabs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mboost 2.9-1. See &amp;#39;package?mboost&amp;#39; and &amp;#39;news(package  = &amp;quot;mboost&amp;quot;)&amp;#39;
## for a complete list of changes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;mboost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     extract&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;quantreg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: SparseM&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;SparseM&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:base&amp;#39;:
## 
##     backsolve&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;TH.data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;survival&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:quantreg&amp;#39;:
## 
##     untangle.specials&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:HSAUR3&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamlss.data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gamlss.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:HSAUR3&amp;#39;:
## 
##     plasma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:datasets&amp;#39;:
## 
##     sleep&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lattice)
data(clouds)
head(clouds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   seeding time  sne cloudcover prewetness echomotion rainfall
## 1      no    0 1.75       13.4      0.274 stationary    12.85
## 2     yes    1 2.70       37.9      1.267     moving     5.52
## 3     yes    3 4.10        3.9      0.198 stationary     6.29
## 4      no    4 2.35        5.3      0.526     moving     6.11
## 5     yes    6 4.25        7.1      0.250     moving     2.45
## 6      no    9 1.60        6.9      0.018 stationary     3.61&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1a. Review the linear model fitted to this data in Chapter 6&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1a. Review the linear model fitted to this data in Chapter 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clouds_formula &amp;lt;- rainfall ~ seeding + seeding:(sne + cloudcover + prewetness + echomotion) + time
clouds.lm &amp;lt;- glm(clouds_formula, data = clouds)
summary(clouds.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = clouds_formula, data = clouds)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5259  -1.1486  -0.2704   1.0401   4.3913  
## 
## Coefficients:
##                                 Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)                     -0.34624    2.78773  -0.124  0.90306   
## seedingyes                      15.68293    4.44627   3.527  0.00372 **
## time                            -0.04497    0.02505  -1.795  0.09590 . 
## seedingno:sne                    0.41981    0.84453   0.497  0.62742   
## seedingyes:sne                  -2.77738    0.92837  -2.992  0.01040 * 
## seedingno:cloudcover             0.38786    0.21786   1.780  0.09839 . 
## seedingyes:cloudcover           -0.09839    0.11029  -0.892  0.38854   
## seedingno:prewetness             4.10834    3.60101   1.141  0.27450   
## seedingyes:prewetness            1.55127    2.69287   0.576  0.57441   
## seedingno:echomotionstationary   3.15281    1.93253   1.631  0.12677   
## seedingyes:echomotionstationary  2.59060    1.81726   1.426  0.17757   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 4.860684)
## 
##     Null deviance: 222.335  on 23  degrees of freedom
## Residual deviance:  63.189  on 13  degrees of freedom
## AIC: 115.34
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;It looks like the seedingyes variable is the most significant variable in the model followed by seedingyes:sne&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It looks like the seedingyes variable is the most significant variable in the model followed by seedingyes:sne&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Now we choose continous variable sne to fit our linear model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Now we choose continous variable sne to fit our linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clouds.lm &amp;lt;- glm(rainfall ~ sne, data = clouds)
summary(clouds.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = rainfall ~ sne, data = clouds)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -6.4927  -2.1116   0.0556   1.2295   6.5036  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   8.7430     2.1508   4.065 0.000515 ***
## sne          -1.3695     0.6524  -2.099 0.047512 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 8.419897)
## 
##     Null deviance: 222.33  on 23  degrees of freedom
## Residual deviance: 185.24  on 22  degrees of freedom
## AIC: 123.16
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSE.lm &amp;lt;- mean((predict(clouds.lm, data = clouds)-clouds$rainfall)^2)
cat(&amp;quot;Linear model MSE: &amp;quot;,MSE.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear model MSE:  7.718239&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=clouds, aes(x=sne, y=rainfall, col=seeding)) + geom_point() + geom_smooth(method=&amp;#39;lm&amp;#39;) + labs(title=&amp;#39;Rainfall determined by suitability criterion&amp;#39;,x=&amp;#39;S-NE Criterion&amp;#39;, y=&amp;#39;Rainfall&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1b. Fit a median regression (quantile regression) model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1b. Fit a median regression (quantile regression) model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median.reg &amp;lt;- rq(rainfall ~ sne, data = clouds, tau = 0.5)
cat(&amp;quot;Summary of the model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Summary of the model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(median.reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = rainfall ~ sne, tau = 0.5, data = clouds)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  8.86133      3.14768 14.86666
## sne         -1.38667     -2.46926  0.13118&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Summary of model with bootstrapped standard error&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Summary of model with bootstrapped standard error&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(median.reg, se = &amp;quot;boot&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = rainfall ~ sne, tau = 0.5, data = clouds)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             Value    Std. Error t value  Pr(&amp;gt;|t|)
## (Intercept)  8.86133  3.62423    2.44502  0.02295
## sne         -1.38667  1.06195   -1.30577  0.20512&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSE.mrm &amp;lt;- mean((predict(median.reg, data = clouds, type = &amp;quot;response&amp;quot;)-clouds$rainfall)^2)
cat(&amp;quot;MSE of median regression Model: &amp;quot;,MSE.mrm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MSE of median regression Model:  7.722559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1c. We can also plot this model to compare with previous linear model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1c. We can also plot this model to compare with previous linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_linear &amp;lt;- ggplot(data=clouds, aes(x=sne, y=rainfall, col=seeding)) + geom_point() + geom_smooth(method=&amp;#39;lm&amp;#39;,se=FALSE) + labs(title=&amp;#39;Linear regression model: Rainfall\n determined by suitability criterion&amp;#39;,x=&amp;#39;S-NE Criterion&amp;#39;, y=&amp;#39;Rainfall&amp;#39;) + theme_classic()


plot_median &amp;lt;- ggplot(data=clouds, aes(x=sne, y=rainfall, col=seeding)) + geom_point()  + labs(title=&amp;#39;Median regression model: Rainfall\n determined by suitability criterion&amp;#39;,x=&amp;#39;S-NE Criterion&amp;#39;, y=&amp;#39;Rainfall&amp;#39;) + stat_quantile(quantiles=c(0.50), method=&amp;#39;rq&amp;#39;) + theme_classic()

grid.arrange(plot_linear, plot_median, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;In addition to fitting the models asked in 1a and 1b, I used graphical methods to compare a linear regression fit vs median regression fit for 1c. As shown in the plots, a median regression model on the data splits the absence of seeding in such a way that the median regression line has a positive slope, whereas, the simple linear regression model seems to have the negative slope. This indicates that there is higher variability in the rainfall data when cloud seeding is absent. Additionally, in presence of seeding, the median regression line is not weighted by the outliers, therefore seems better at explaining the overall data due to the high variability of the rainfall variable.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Reanalyze the {} data from the {} package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Compare the regression tree approach from chapter 9 of the textbook to median regression and summarize the different findings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose one independent variable. For the relationship between this variable and DEXfat, create linear regression quantile models for the 25%, 50% and 75% quantiles. Plot DEXfat vs that independent variable and plot the lines from the models on the graph.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;bodyfat&amp;quot;)
head(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    age DEXfat waistcirc hipcirc elbowbreadth kneebreadth anthro3a anthro3b
## 47  57  41.68     100.0   112.0          7.1         9.4     4.42     4.95
## 48  65  43.29      99.5   116.5          6.5         8.9     4.63     5.01
## 49  59  35.41      96.0   108.5          6.2         8.9     4.12     4.74
## 50  58  22.79      72.0    96.5          6.1         9.2     4.03     4.48
## 51  60  36.42      89.5   100.5          7.1        10.0     4.24     4.68
## 52  61  24.13      83.5    97.0          6.5         8.8     3.55     4.06
##    anthro3c anthro4
## 47     4.50    6.13
## 48     4.48    6.37
## 49     4.60    5.82
## 50     3.91    5.66
## 51     4.15    5.91
## 52     3.64    5.14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncol(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;rpart&amp;quot;)
# ?bodyfat

cat(&amp;quot;First, fit regression tree model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## First, fit regression tree model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bodyfat_formula &amp;lt;- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth
bodyfat_rp  &amp;lt;- rpart(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data = bodyfat, control = rpart.control(minsplit=10))
summary(bodyfat_rp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## rpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + 
##     kneebreadth, data = bodyfat, control = rpart.control(minsplit = 10))
##   n= 71 
## 
##           CP nsplit  rel error    xerror       xstd
## 1 0.66289544      0 1.00000000 1.0149614 0.16976190
## 2 0.09376252      1 0.33710456 0.4071906 0.09251759
## 3 0.07703606      2 0.24334204 0.4135264 0.08910155
## 4 0.04507506      3 0.16630598 0.2974999 0.05920879
## 5 0.01844561      4 0.12123092 0.2640916 0.05859875
## 6 0.01818982      5 0.10278532 0.2665689 0.06079305
## 7 0.01000000      6 0.08459549 0.2558127 0.06188322
## 
## Variable importance
##    waistcirc      hipcirc  kneebreadth elbowbreadth          age 
##           34           30           24            7            4 
## 
## Node number 1: 71 observations,    complexity param=0.6628954
##   mean=30.78282, MSE=120.2251 
##   left son=2 (40 obs) right son=3 (31 obs)
##   Primary splits:
##       waistcirc    &amp;lt; 88.4   to the left,  improve=0.6628954, (0 missing)
##       hipcirc      &amp;lt; 108.25 to the left,  improve=0.6254333, (0 missing)
##       kneebreadth  &amp;lt; 9.35   to the left,  improve=0.5142133, (0 missing)
##       age          &amp;lt; 40.5   to the left,  improve=0.1570344, (0 missing)
##       elbowbreadth &amp;lt; 6.55   to the left,  improve=0.1169918, (0 missing)
##   Surrogate splits:
##       hipcirc      &amp;lt; 107.85 to the left,  agree=0.915, adj=0.806, (0 split)
##       kneebreadth  &amp;lt; 9.35   to the left,  agree=0.831, adj=0.613, (0 split)
##       elbowbreadth &amp;lt; 6.55   to the left,  agree=0.648, adj=0.194, (0 split)
##       age          &amp;lt; 47     to the left,  agree=0.592, adj=0.065, (0 split)
## 
## Node number 2: 40 observations,    complexity param=0.07703606
##   mean=22.92375, MSE=32.88394 
##   left son=4 (17 obs) right son=5 (23 obs)
##   Primary splits:
##       hipcirc      &amp;lt; 96.25  to the left,  improve=0.4999238, (0 missing)
##       waistcirc    &amp;lt; 71.5   to the left,  improve=0.4408508, (0 missing)
##       kneebreadth  &amp;lt; 9.15   to the left,  improve=0.3123752, (0 missing)
##       age          &amp;lt; 41     to the left,  improve=0.2212005, (0 missing)
##       elbowbreadth &amp;lt; 6.65   to the left,  improve=0.0757275, (0 missing)
##   Surrogate splits:
##       waistcirc    &amp;lt; 71.5   to the left,  agree=0.775, adj=0.471, (0 split)
##       age          &amp;lt; 41     to the left,  agree=0.700, adj=0.294, (0 split)
##       kneebreadth  &amp;lt; 8.25   to the left,  agree=0.675, adj=0.235, (0 split)
##       elbowbreadth &amp;lt; 5.75   to the left,  agree=0.600, adj=0.059, (0 split)
## 
## Node number 3: 31 observations,    complexity param=0.09376252
##   mean=40.92355, MSE=50.39231 
##   left son=6 (28 obs) right son=7 (3 obs)
##   Primary splits:
##       kneebreadth  &amp;lt; 11.15  to the left,  improve=0.51233840, (0 missing)
##       hipcirc      &amp;lt; 109.9  to the left,  improve=0.45671770, (0 missing)
##       waistcirc    &amp;lt; 106    to the left,  improve=0.44843720, (0 missing)
##       elbowbreadth &amp;lt; 6.35   to the left,  improve=0.16017880, (0 missing)
##       age          &amp;lt; 45.5   to the right, improve=0.06131694, (0 missing)
## 
## Node number 4: 17 observations,    complexity param=0.01844561
##   mean=18.20765, MSE=16.81845 
##   left son=8 (11 obs) right son=9 (6 obs)
##   Primary splits:
##       age          &amp;lt; 59.5   to the left,  improve=0.55069560, (0 missing)
##       waistcirc    &amp;lt; 70.35  to the left,  improve=0.39973880, (0 missing)
##       elbowbreadth &amp;lt; 6.65   to the left,  improve=0.22215850, (0 missing)
##       hipcirc      &amp;lt; 92.6   to the left,  improve=0.16823720, (0 missing)
##       kneebreadth  &amp;lt; 8.55   to the left,  improve=0.08112073, (0 missing)
##   Surrogate splits:
##       elbowbreadth &amp;lt; 6.55   to the left,  agree=0.824, adj=0.500, (0 split)
##       waistcirc    &amp;lt; 71.5   to the left,  agree=0.765, adj=0.333, (0 split)
## 
## Node number 5: 23 observations,    complexity param=0.01818982
##   mean=26.40957, MSE=16.16806 
##   left son=10 (13 obs) right son=11 (10 obs)
##   Primary splits:
##       waistcirc    &amp;lt; 80.75  to the left,  improve=0.41753840, (0 missing)
##       hipcirc      &amp;lt; 101.35 to the left,  improve=0.34272770, (0 missing)
##       kneebreadth  &amp;lt; 9.5    to the left,  improve=0.30544320, (0 missing)
##       elbowbreadth &amp;lt; 7.1    to the right, improve=0.06644785, (0 missing)
##       age          &amp;lt; 57     to the right, improve=0.03572739, (0 missing)
##   Surrogate splits:
##       hipcirc      &amp;lt; 101.75 to the left,  agree=0.783, adj=0.5, (0 split)
##       kneebreadth  &amp;lt; 9.5    to the left,  agree=0.696, adj=0.3, (0 split)
##       age          &amp;lt; 66     to the left,  agree=0.652, adj=0.2, (0 split)
##       elbowbreadth &amp;lt; 6.25   to the left,  agree=0.652, adj=0.2, (0 split)
## 
## Node number 6: 28 observations,    complexity param=0.04507506
##   mean=39.26036, MSE=21.98307 
##   left son=12 (13 obs) right son=13 (15 obs)
##   Primary splits:
##       hipcirc      &amp;lt; 109.9  to the left,  improve=0.62509140, (0 missing)
##       waistcirc    &amp;lt; 99     to the left,  improve=0.47879840, (0 missing)
##       kneebreadth  &amp;lt; 9.85   to the left,  improve=0.28389460, (0 missing)
##       elbowbreadth &amp;lt; 6.35   to the left,  improve=0.18101920, (0 missing)
##       age          &amp;lt; 49.5   to the right, improve=0.04758482, (0 missing)
##   Surrogate splits:
##       waistcirc    &amp;lt; 99     to the left,  agree=0.821, adj=0.615, (0 split)
##       elbowbreadth &amp;lt; 6.45   to the left,  agree=0.714, adj=0.385, (0 split)
##       kneebreadth  &amp;lt; 9.95   to the left,  agree=0.714, adj=0.385, (0 split)
##       age          &amp;lt; 49.5   to the right, agree=0.607, adj=0.154, (0 split)
## 
## Node number 7: 3 observations
##   mean=56.44667, MSE=48.76009 
## 
## Node number 8: 11 observations
##   mean=15.96, MSE=8.818582 
## 
## Node number 9: 6 observations
##   mean=22.32833, MSE=5.242981 
## 
## Node number 10: 13 observations
##   mean=24.13077, MSE=9.046699 
## 
## Node number 11: 10 observations
##   mean=29.372, MSE=9.899016 
## 
## Node number 12: 13 observations
##   mean=35.27846, MSE=10.48431 
## 
## Node number 13: 15 observations
##   mean=42.71133, MSE=6.297998&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Plot the regression tree model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plot the regression tree model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(partykit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: libcoin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;partykit&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:mboost&amp;#39;:
## 
##     varimp&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.party(bodyfat_rp), tp_args = list(id=FALSE), main = &amp;quot;Regression tree of the model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Print CP-table&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Print CP-table&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(bodyfat_rp$cptable)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           CP nsplit  rel error    xerror       xstd
## 1 0.66289544      0 1.00000000 1.0149614 0.16976190
## 2 0.09376252      1 0.33710456 0.4071906 0.09251759
## 3 0.07703606      2 0.24334204 0.4135264 0.08910155
## 4 0.04507506      3 0.16630598 0.2974999 0.05920879
## 5 0.01844561      4 0.12123092 0.2640916 0.05859875
## 6 0.01818982      5 0.10278532 0.2665689 0.06079305
## 7 0.01000000      6 0.08459549 0.2558127 0.06188322&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;CP value with lowest xerror&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CP value with lowest xerror&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min.cp &amp;lt;- which.min(bodyfat_rp$cptable[,&amp;#39;xerror&amp;#39;])
min.cp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 7 
## 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(
  &amp;quot;Following the methods in the book, we can fit the model\n using lowest xerror rate from CP for prunning tree model&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Following the methods in the book, we can fit the model
##  using lowest xerror rate from CP for prunning tree model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#extract the lowest CP
cp &amp;lt;- bodyfat_rp$cptable[min.cp, &amp;#39;CP&amp;#39;]
bodyfat_prune &amp;lt;- prune(bodyfat_rp, cp=cp)

cat(&amp;quot;summary of the median regression model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## summary of the median regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Median Regression model
bodyfat_rpart_qrm &amp;lt;- rq(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data=bodyfat, tau = 0.50)
summary(bodyfat_rpart_qrm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + 
##     kneebreadth, tau = 0.5, data = bodyfat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##              coefficients lower bd  upper bd 
## (Intercept)  -57.30032    -87.22119 -36.39320
## age            0.06839     -0.04338   0.14943
## waistcirc      0.28332      0.07991   0.48638
## hipcirc        0.51073      0.21307   0.75030
## elbowbreadth  -0.11982     -3.62882   2.18220
## kneebreadth    0.76453     -2.30145   2.33329&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_rpart_qrm_summary &amp;lt;- summary(bodyfat_rpart_qrm) 


#Predict Pruned regression tree model on the bodyfat data set
RegressionTree &amp;lt;- predict(bodyfat_prune, data=bodyfat)

#Create observed value and the predicted value
observed &amp;lt;- bodyfat$DEXfat
predict &amp;lt;- RegressionTree



#Regression.Tree MSE
RegressionTree.MSE &amp;lt;- mean((observed - predict)^2)

#Median Regression MSE
MedianRegression.MSE &amp;lt;- mean(bodyfat_rpart_qrm_summary$residuals^2)

df &amp;lt;- data.frame(
  RegressionTree.MSE,
  MedianRegression.MSE
)

cat(&amp;quot;MSE of both models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MSE of both models&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   RegressionTree.MSE MedianRegression.MSE
## 1            10.1705              15.0245&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Based on this, pruned regression tree has lower MSE than median regression model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Based on this, pruned regression tree has lower MSE than median regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Plot based on the regression tree prunning&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plot based on the regression tree prunning&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.party(bodyfat_prune), tp_args = list(id=FALSE), main = &amp;quot;Plot based on the regression tree prunning&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(
  &amp;quot;Based on the above pruned tree, the variables waist circumference and hip circumference splits explain the majority of the data and I will be choosing one of these variables for quantile regression.&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Based on the above pruned tree, the variables waist circumference and hip circumference splits explain the majority of the data and I will be choosing one of these variables for quantile regression.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Additionally, I will check with linear regression for the variable with significant effect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Additionally, I will check with linear regression for the variable with significant effect&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check.lm &amp;lt;- lm(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data = bodyfat)

summary(check.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + 
##     kneebreadth, data = bodyfat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.1782 -2.4973  0.2089  2.5496 11.6504 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -59.57320    8.45359  -7.047 1.43e-09 ***
## age            0.06381    0.03740   1.706   0.0928 .  
## waistcirc      0.32044    0.07372   4.347 4.96e-05 ***
## hipcirc        0.43395    0.09566   4.536 2.53e-05 ***
## elbowbreadth  -0.30117    1.21731  -0.247   0.8054    
## kneebreadth    1.65381    0.86235   1.918   0.0595 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.988 on 65 degrees of freedom
## Multiple R-squared:  0.8789, Adjusted R-squared:  0.8696 
## F-statistic: 94.34 on 5 and 65 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Looks like waistcirc is the most significant, so we will choose this variable&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Looks like waistcirc is the most significant, so we will choose this variable&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Now we can run a median quantile regression&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Now we can run a median quantile regression&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_qrm_25 &amp;lt;- rq(DEXfat ~ age + waistcirc, data = bodyfat, tau = 0.25)
summary(bodyfat_qrm_25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc, tau = 0.25, data = bodyfat)
## 
## tau: [1] 0.25
## 
## Coefficients:
##             coefficients lower bd  upper bd 
## (Intercept) -34.95268    -41.72329 -30.15887
## age           0.07777      0.01124   0.14609
## waistcirc     0.67033      0.60935   0.74444&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_qrm_50 &amp;lt;- rq(DEXfat ~ age + waistcirc, data = bodyfat, tau = 0.50)
summary(bodyfat_qrm_50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc, tau = 0.5, data = bodyfat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd  upper bd 
## (Intercept) -28.39189    -39.03217 -17.59073
## age           0.05514     -0.06704   0.15053
## waistcirc     0.63962      0.54298   0.76936&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_qrm_75 &amp;lt;- rq(DEXfat ~ age + waistcirc, data = bodyfat, tau = 0.75)
summary(bodyfat_qrm_50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc, tau = 0.5, data = bodyfat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd  upper bd 
## (Intercept) -28.39189    -39.03217 -17.59073
## age           0.05514     -0.06704   0.15053
## waistcirc     0.63962      0.54298   0.76936&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;DEXfat explained by waist circumference at quantile 25%, 50%, and 75% regression lines&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## DEXfat explained by waist circumference at quantile 25%, 50%, and 75% regression lines&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(data=bodyfat, DEXfat~waistcirc, main=&amp;quot;baseR: Quantile regression- DEXfat Explained by waist circumference&amp;quot;, xlab=&amp;#39;Waist circumference&amp;#39;)
abline(rq(DEXfat ~ waistcirc, data=bodyfat, tau = 0.25), col=&amp;#39;blue&amp;#39;)
abline(rq(DEXfat ~ waistcirc, data=bodyfat, tau = 0.50), col=&amp;#39;green&amp;#39;)
abline(rq(DEXfat ~ waistcirc, data=bodyfat, tau = 0.75), col=&amp;#39;black&amp;#39;)
legend(&amp;#39;topleft&amp;#39;, legend = c(&amp;#39;25% Quantile&amp;#39;, &amp;#39;50% Quantile&amp;#39;, &amp;#39;75% Quantile&amp;#39;),
       fill=c(&amp;#39;blue&amp;#39;,&amp;#39;green&amp;#39;,&amp;#39;black&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=bodyfat, aes(x=waistcirc, y=DEXfat)) + geom_point() + stat_quantile(quantiles=c(0.25), method=&amp;#39;rq&amp;#39;, aes(colour=&amp;#39;25%&amp;#39;)) + stat_quantile(quantiles=c(0.50), method=&amp;#39;rq&amp;#39;, aes(colour=&amp;#39;50%&amp;#39;)) + 
  stat_quantile(quantiles=c(0.75), method=&amp;#39;rq&amp;#39;, aes(colour=&amp;#39;75%&amp;#39;)) +
  labs(title=&amp;quot;ggplot: Quantile regression- DEXfat Explained by waist circumference&amp;quot;, x=&amp;#39;Waist circumference&amp;#39;, y=&amp;#39;DEXfat&amp;#39;) +
scale_color_manual(name=&amp;quot;Quantile Percent&amp;quot;, values = c(&amp;#39;25%&amp;#39; = &amp;quot;blue&amp;quot;, &amp;#39;50%&amp;#39; = &amp;quot;green&amp;quot;, &amp;#39;75%&amp;#39; = &amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x
## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;Based on the above pruned tree, the variables waist circumference and hip circumference splits explain the majority of the data and I chose waist circumference for quantile regression. Based on this analysis, pruned regression tree has lower MSE than median regression model. The relationship of Dexfat to Age by Waist Circumference, all three quantiles regression lines have a positive, and seemingly similar slopes.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Consider {} data from the lecture notes (package {}). Refit the additive quantile regression models presented ({}) with varying values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (lambda) in {}. How do the estimated quantile curves change?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Q3 - db data from gamlss.data package
data(db)
head(db)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   head  age
## 1 33.6 0.03
## 2 33.6 0.04
## 3 33.7 0.04
## 4 35.0 0.04
## 5 36.1 0.04
## 6 36.6 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(db)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7040    2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tau &amp;lt;- c(.03, .15, .5, .85, .97)


cat(&amp;quot;Parameters: lambda = 0&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parameters: lambda = 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rqssmod &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(tau))
db$lage &amp;lt;- with(db, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] &amp;lt;- rqss(head ~ qss(lage, lambda = 0), data = db, tau = tau[i])

gage &amp;lt;- seq(from = min(db$age), to = max(db$age), length = 50)
p &amp;lt;- sapply(1:length(tau), function(i) { predict(rqssmod[[i]], newdata = data.frame(lage = gage^(1/3)))
})

pfun &amp;lt;- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
}

xyplot(head ~ age, data = db, main = &amp;quot;Head circumference curve with lambda = 0&amp;quot;,
       xlab = &amp;quot;Age (years)&amp;quot;, ylab = &amp;quot;Head circumference (cm)&amp;quot;, pch = 19,
       scales = list(x = list(relation = &amp;quot;free&amp;quot;)),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Parameters: lambda = 1; and tau same as before&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parameters: lambda = 1; and tau same as before&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rqssmod &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(tau))
db$lage &amp;lt;- with(db, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] &amp;lt;- rqss(head ~ qss(lage, lambda = 1), data = db, tau = tau[i])

gage &amp;lt;- seq(from = min(db$age), to = max(db$age), length = 50)
p &amp;lt;- sapply(1:length(tau), function(i) { predict(rqssmod[[i]], newdata = data.frame(lage = gage^(1/3)))
})

pfun &amp;lt;- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
}

xyplot(head ~ age, data = db, main = &amp;quot;Head circumference curve with lambda=1&amp;quot;,
       xlab = &amp;quot;Age (years)&amp;quot;, ylab = &amp;quot;Head circumference (cm)&amp;quot;, pch = 19,
       scales = list(x = list(relation = &amp;quot;free&amp;quot;)),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Parameters: lambda = 20; and tau same as before&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parameters: lambda = 20; and tau same as before&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rqssmod &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(tau))
db$lage &amp;lt;- with(db, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] &amp;lt;- rqss(head ~ qss(lage, lambda = 20), data = db, tau = tau[i])

gage &amp;lt;- seq(from = min(db$age), to = max(db$age), length = 50)
p &amp;lt;- sapply(1:length(tau), function(i) { predict(rqssmod[[i]], newdata = data.frame(lage = gage^(1/3)))
})

pfun &amp;lt;- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
}

xyplot(head ~ age, data = db, main = &amp;quot;Head circumference curve with lambda=20&amp;quot;,
       xlab = &amp;quot;Age (years)&amp;quot;, ylab = &amp;quot;Head circumference (cm)&amp;quot;, pch = 19,
       scales = list(x = list(relation = &amp;quot;free&amp;quot;)),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;Here, lambda acts as a shinkage factor which causes the quantiles to become smoother (at higher lambda) rather than becoming wavy or rough with lower values of labda (lambda = 0). So, I found that by increasing the penalty term lambda, which is assigned to the slope of the coefficients, the overall fit of the additive quantile regression model can be smoothen.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Read the paper by Koenker and Hallock (2001), posted on D2L. Write a one page summary of the paper. This should include but not be limited to introduction, motivation, case study considered and findings.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Answer:&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;In Koenker and Hallock (2001), the authors discuss the utility of quantile regression. In this study, they made a comparison on linear regression and quantile regression analyses.The quantile regression allows large sample groups or population size to into fractals distribution or smaller quantiles represented by a parameter &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; (tau) and maintaining the same &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; for observations above and below the quantile and minimizing the sum of weighted absolute residuals. They show the utility of quantile regression by looking at various types of data sets such as household food spending on household income and child birthweight.&lt;/p&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;p&gt;The reason to look at these data sets was to illustrate and explain that in some situations, quantile regression is more useful than linear regression. Quantile regression, which measures the median rather than the mean, will yield more suitable fits to results when a distribution includes outliers that could force the mean in a specific direction. This was addressed in the paper when referring to the’ Quantile Engel Curves’ where two different points of high income and low food consumption influenced the least square estimate. Quantile regression could eliminate this type of bias by minimizing the sum of absolute residuals.&lt;/p&gt;
&lt;p&gt;Case study&lt;/p&gt;
&lt;p&gt;The Infant Birth weight case study aimed to analyze the association between low birth weights of children and several variables including public policy measures. Infants weighing less than 5 pounds, 9 ounces at birth (2500 grams) were defined as low birth weight. Quantile regression was relevant in this analysis because due to the lower tail of the birth weight distribution, all the least square estimates had been skewed.&lt;/p&gt;
&lt;p&gt;Findings&lt;/p&gt;
&lt;p&gt;The most notable finding was that, based on the least square calculation, boys were usually born heavier than girls by 100 grams. The figure, however, changes with the quantile of 0.05, where boys were only 45 grams larger and even greater at the quantile of 095, where boys were 130 grams larger. The least squares (linear regression) in this case does a poor job of explaining the distribution variability. Additionally, infants born to black and white mothers with 5th percentile birth weights differed by 1/3 of a kilogram. Furthermore, the age of mother, education beyond high school, prenatal care, marital status, and smoking all contribute to the birth weight of an infant.&lt;/p&gt;
&lt;p&gt;Conclusion&lt;/p&gt;
&lt;p&gt;In conclusion, some distributions with longer tails can weigh the mean so that the regression of the least squares can provide a false representation of the results. A more precise description of the data and underlying patterns can be obtained using quantile regression. This is particularly useful in econometrics where there are often large outliers that can have a significant impact on a least squares model. Basically, we need to assess our needs when dealing with the datasets:
i. Are we concerned about percentiles or median value?
ii. Whether we are interested in average values and minimizing the residuals errors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Survival Analysis</title>
      <link>/achalneupane.github.io/post/survival_analysis/</link>
      <pubDate>Fri, 11 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/survival_analysis/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;An investigator collected data on survival of patients with lung cancer at Mayo Clinic. The investigator would like you, the statistician, to answer the following questions and provide some graphs. Use the  data located in the  package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the probability that someone will survive past 300 days?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Provide a graph, including 95% confidence limits, of the Kaplan-Meier estimate of the entire study.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is there a difference in the survival rates between males and females? Provide a formal statistical test with a p-value and visual evidence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is there a difference in the survival rates for the older half of the group versus the younger half? Provide a formal statistical test with a p-value and visual evidence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survminer&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggpubr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: magrittr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package &amp;#39;mclust&amp;#39; version 5.4.5
## Type &amp;#39;citation(&amp;quot;mclust&amp;quot;)&amp;#39; for citing this R package in publications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1a.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call: survfit(formula = Surv(time, status == 2) ~ 1, data = cancer)
## 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##   300     92     101    0.531  0.0346        0.467        0.603&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1b.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 1c.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## SEX values: male=1, female=2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = Surv(time, status == 2) ~ sex, data = cancer)
## 
##         N Observed Expected (O-E)^2/E (O-E)^2/V
## sex=1 138      112     91.6      4.55      10.3
## sex=2  90       53     73.4      5.68      10.3
## 
##  Chisq= 10.3  on 1 degrees of freedom, p= 0.001&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = Surv(time, status == 2) ~ Age_Group, data = dup.cancer)
## 
##               N Observed Expected (O-E)^2/E (O-E)^2/V
## Age_Group=1 117       80     88.8     0.865      1.88
## Age_Group=2 111       85     76.2     1.007      1.88
## 
##  Chisq= 1.9  on 1 degrees of freedom, p= 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;1.a&lt;/p&gt;
&lt;p&gt;The survival probability after 300 days is 53.1% determined from the dataset.&lt;/p&gt;
&lt;p&gt;1.b&lt;/p&gt;
&lt;p&gt;The plot based on the Kaplan-Meier estimator for the survival function shows that the survivability is in downward direction. The plot function generates the 95% confidence interval, and I have h and v lines drawn for survival probability at 300 days as well . The plot also indicates that within this timeline, the interval is higher for later days of survival time indicating larger variability in survival uncertainty (after 400 days of survivability).&lt;/p&gt;
&lt;p&gt;1.c&lt;/p&gt;
&lt;p&gt;Yes Males and Females have different survival probabilities which can be determined based on Kaplan-Maier estimator on male and female groups. Second plot was also generated by using &lt;code&gt;plot(survfit(Surv(time,status==2)~sex, data=cancer)&lt;/code&gt;. In this plot, the black line (males) lies below the red line (female) throughout the interval. This tells us that women have higher survival compared to males. The log rank test also indicates significant differece between sex groups with p-value of 0.001 to reject the NULL hypothesis (Null hypothesis: no difference in male and female survivability) with 95% confidence.&lt;/p&gt;
&lt;p&gt;1.d&lt;/p&gt;
&lt;p&gt;Here, data was divided into younger and older groups for the analysis based on median age. After properly splitting the data, a plot was generated based on Kaplan-Meir estimator for the two groups. Based on this plot, we can tell that the younger group (black line) of patients have longer survivability than the older patient groups(red line).However, based on the log rank test, the difference is not significant (P=0.17) to reject the NULL hypothesis ( NULL hypothesis: no difference in terms of survivability between the two age groups).&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A healthcare group has asked you to analyse the  data from the  package, which is the survival times (in months) after a mastectomy of women with breast cancer. The cancers are classified as having metastasized or not based on a histochemical marker. The healthcare group requests that your report should not be longer than one page, and must only consist of one plot, one table, and one paragraph. Do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Plot the survivor functions of each group only using GGPlot, estimated using the Kaplan-Meier estimate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use a log-rank test to compare the survival experience of each group more formally. Only present a formal table of your results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write one paragraph summarizing your findings and conclusions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## Call: survfit(formula = Surv(time, event) ~ metastasized, data = mt)
## 
##                 metastasized=no 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##    23     12       1    0.917  0.0798        0.773        1.000
##    47     11       1    0.833  0.1076        0.647        1.000
##    69     10       1    0.750  0.1250        0.541        1.000
##   148      6       1    0.625  0.1545        0.385        1.000
##   181      5       1    0.500  0.1667        0.260        0.961
## 
##                 metastasized=yes 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##     5     32       1    0.969  0.0308        0.910        1.000
##     8     31       1    0.938  0.0428        0.857        1.000
##    10     30       1    0.906  0.0515        0.811        1.000
##    13     29       1    0.875  0.0585        0.768        0.997
##    18     28       1    0.844  0.0642        0.727        0.979
##    24     27       1    0.812  0.0690        0.688        0.960
##    26     26       2    0.750  0.0765        0.614        0.916
##    31     24       1    0.719  0.0795        0.579        0.893
##    35     23       1    0.688  0.0819        0.544        0.868
##    40     22       1    0.656  0.0840        0.511        0.843
##    41     21       1    0.625  0.0856        0.478        0.817
##    48     20       1    0.594  0.0868        0.446        0.791
##    50     19       1    0.562  0.0877        0.414        0.764
##    59     18       1    0.531  0.0882        0.384        0.736
##    61     17       1    0.500  0.0884        0.354        0.707
##    68     16       1    0.469  0.0882        0.324        0.678
##    71     15       1    0.438  0.0877        0.295        0.648
##   113     10       1    0.394  0.0892        0.253        0.614
##   118      8       1    0.345  0.0906        0.206        0.577
##   143      7       1    0.295  0.0900        0.162        0.537&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## log rank test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = Surv(time, event == 1) ~ metastasized, data = mt, 
##     rho = 0)
## 
##                   N Observed Expected (O-E)^2/E (O-E)^2/V
## metastasized=no  12        5      9.2      1.91      3.04
## metastasized=yes 32       21     16.8      1.05      3.04
## 
##  Chisq= 3  on 1 degrees of freedom, p= 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cox&amp;#39;s regression test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(time, event) ~ metastasized, data = mt)
## 
##                   coef exp(coef) se(coef)     z    p
## metastasizedyes 0.8516    2.3434   0.5022 1.696 0.09
## 
## Likelihood ratio test=3.35  on 1 df, p=0.06704
## n= 44, number of events= 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Question 2c:&lt;/p&gt;
&lt;p&gt;Explanation for part 2a
The figure in part 1, shows the survival probability of women with breast cancer under two categories- metastasized and not-metastasized. The red line shows the survival probability of those that have not undergone metastasis and the brown line shows the survival probability of those that have cancer metastasized elsewhere in the organs. Based on this plot we can tell that those with mestasized cancer have lower survival probability (brown line). After about 143 weeks, metastasized patient’s (brown line) survival drops down to only 29.5 % whereas for non-metastasized (red line) pateint’s the survival is still above 60%.&lt;/p&gt;
&lt;p&gt;Explanation for part 2b.&lt;/p&gt;
&lt;p&gt;To determine whether the difference is statistically significant, I have performed log-rank test and got p-value of 0.061 which is not significant at P&amp;lt;0.05 whereas for Cox regression test, the p-value was even higher.&lt;/p&gt;
&lt;p&gt;To summarize answer for part 2c:&lt;/p&gt;
&lt;p&gt;Although we saw from the figure that the the survival probability of women with matastasized breast cancer is lower than those without metastasized breast cancer, the statistical test shows this difference to be statistically not siginificant.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generalized Additive Models and Spline Models</title>
      <link>/achalneupane.github.io/post/generalized_additive_models_and_spline_models/</link>
      <pubDate>Tue, 01 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/generalized_additive_models_and_spline_models/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the body fat data introduced in Chapter 9 ( data from  package).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Explore the data graphically. What variables do you think need to be included for predicting bodyfat? (Hint: Are there correlated predictors).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a generalised additive model assuming normal errors using the following code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assess the  and  of the model (don’t need GGPLOT). Are all covariates informative? Should all covariates be smoothed or should some be included as a linear effect?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Report GCV, AIC, adj-R&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;, and total model degrees of freedom.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use  function to look at the diagnostic plot. Does it appear that the normality assumption is violated?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a discussion on all of the above points.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now remove insignificant variables and remove smoothing for some variables. Report the summary, plot, GCV, AIC, adj-R&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Again fit an additive model to the body fat data, but this time for a log-transformed response. Compare the three models, which one is more appropriate? (Hint: use Adj-R&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;, residual plots, etc. to compare models).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Fit a generalised additive model that underwent AIC-based variable selection (fitted using function  function). What variable was removed by using AIC?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(mgcv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nlme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mgcv 1.8-31. For overview type &amp;#39;help(&amp;quot;mgcv-package&amp;quot;)&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mboost)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: parallel&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: stabs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mboost 2.9-1. See &amp;#39;package?mboost&amp;#39; and &amp;#39;news(package  = &amp;quot;mboost&amp;quot;)&amp;#39;
## for a complete list of changes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;mboost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamclass)
library (TH.data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;survival&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:caret&amp;#39;:
## 
##     cluster&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data (&amp;quot;bodyfat&amp;quot;)
head(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    age DEXfat waistcirc hipcirc elbowbreadth kneebreadth anthro3a anthro3b
## 47  57  41.68     100.0   112.0          7.1         9.4     4.42     4.95
## 48  65  43.29      99.5   116.5          6.5         8.9     4.63     5.01
## 49  59  35.41      96.0   108.5          6.2         8.9     4.12     4.74
## 50  58  22.79      72.0    96.5          6.1         9.2     4.03     4.48
## 51  60  36.42      89.5   100.5          7.1        10.0     4.24     4.68
## 52  61  24.13      83.5    97.0          6.5         8.8     3.55     4.06
##    anthro3c anthro4
## 47     4.50    6.13
## 48     4.48    6.37
## 49     4.60    5.82
## 50     3.91    5.66
## 51     4.15    5.91
## 52     3.64    5.14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(bodyfat) # 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DescTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;DescTools&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:caret&amp;#39;:
## 
##     MAE, RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:mboost&amp;#39;:
## 
##     AUC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;age&amp;quot;          &amp;quot;DEXfat&amp;quot;       &amp;quot;waistcirc&amp;quot;    &amp;quot;hipcirc&amp;quot;      &amp;quot;elbowbreadth&amp;quot;
##  [6] &amp;quot;kneebreadth&amp;quot;  &amp;quot;anthro3a&amp;quot;     &amp;quot;anthro3b&amp;quot;     &amp;quot;anthro3c&amp;quot;     &amp;quot;anthro4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
cat(&amp;quot;# 1A&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1A&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

# plot the variables to see which variables are correlated with which
library(ggplot2)
# base plot
pairs(bodyfat, main = &amp;quot;base R: plots showing correlation of variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggplot
library(GGally)
library(knitr)
p &amp;lt;- ggpairs(bodyfat)
p + labs(title = &amp;quot;ggplot: plots showing correlation of variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;all of the anthros correlate with each other&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## all of the anthros correlate with each other&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;body_f &amp;lt;- glm(DEXfat ~ ., data=bodyfat)
bf_step &amp;lt;- step(body_f, trace = 0)

cat(&amp;quot;step recommends: waistcirc, hipcirc, kneebreadth and anthro3b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## step recommends: waistcirc, hipcirc, kneebreadth and anthro3b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_step&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  glm(formula = DEXfat ~ waistcirc + hipcirc + kneebreadth + anthro3b, 
##     data = bodyfat)
## 
## Coefficients:
## (Intercept)    waistcirc      hipcirc  kneebreadth     anthro3b  
##    -71.7197       0.2037       0.3546       1.8047       7.1264  
## 
## Degrees of Freedom: 70 Total (i.e. Null);  66 Residual
## Null Deviance:       8536 
## Residual Deviance: 676.9     AIC: 373.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(summary(bf_step)$coefficients, caption = &amp;quot;Coefficients from step function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Coefficients from step function&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.7196776&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.0140437&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-14.303760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;waistcirc&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2037314&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0609069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.344966&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0013605&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;hipcirc&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3546222&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0767978&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.617607&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000185&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;kneebreadth&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8047490&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6611504&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.729710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0081187&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;anthro3b&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.1264238&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1295927&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.308844&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
cat(&amp;quot;# 1B&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1B&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

bodyfat_gam &amp;lt;- gam(DEXfat ~ s(age) + s(waistcirc) + s(hipcirc) +
                     s(elbowbreadth) + s(kneebreadth) + s(anthro3a) + s(anthro3c),      
                   data = bodyfat)

# Assess Summary
summary(bodyfat_gam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## DEXfat ~ s(age) + s(waistcirc) + s(hipcirc) + s(elbowbreadth) + 
##     s(kneebreadth) + s(anthro3a) + s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  30.7828     0.2847   108.1   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                   edf Ref.df      F  p-value    
## s(age)          1.000  1.000  0.956 0.332964    
## s(waistcirc)    1.000  1.000 10.821 0.001844 ** 
## s(hipcirc)      1.775  2.235  9.917 0.000152 ***
## s(elbowbreadth) 1.000  1.000  0.001 0.972242    
## s(kneebreadth)  8.754  8.960  6.180 3.59e-06 ***
## s(anthro3a)     1.000  1.000 12.966 0.000725 ***
## s(anthro3c)     7.042  8.041  1.798 0.100242    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.953   Deviance explained = 96.7%
## GCV = 8.4354  Scale est. = 5.7538    n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Assess plot
cat (&amp;quot;Plot of a generalized additive model for question 1b:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plot of a generalized additive model for question 1b:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:9, ncol = 3))
plot(bodyfat_gam)

# plot looks like we don&amp;#39;t need to smooth all variables. These appear linear:
# age, waistcirc, elbowbreadth &amp;amp; anthro3a don&amp;#39;t need smoothing


# Question: How do we evaluate a covariate&amp;#39;s &amp;#39;informative&amp;#39;-ness?
# Annwer: Look at the p-values of the covariates.


# Report model attributes
cat(&amp;quot;GCV&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam$gcv.ubre            # GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GCV.Cp 
## 8.435412&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam$aic                 # AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 345.708&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Adjusted R-squared&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted R-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bodyfat_gam)$r.sq       # Adjusted r-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9528156&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Total degrees of Freedom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Total degrees of Freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(bodyfat_gam$edf)            # Total degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22.57091&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Some diagnostics for a fitted gam model using cam.check&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Some diagnostics for a fitted gam model using cam.check&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gam.check(bodyfat_gam)  # Response Vs Fitted values&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 41 iterations.
## The RMS GCV score gradient at convergence was 2.767255e-07 .
## The Hessian was positive definite.
## Model rank =  64 / 64 
## 
## Basis dimension (k) checking results. Low p-value (k-index&amp;lt;1) may
## indicate that k is too low, especially if edf is close to k&amp;#39;.
## 
##                   k&amp;#39;  edf k-index p-value  
## s(age)          9.00 1.00    0.81   0.040 *
## s(waistcirc)    9.00 1.00    0.94   0.260  
## s(hipcirc)      9.00 1.78    1.02   0.560  
## s(elbowbreadth) 9.00 1.00    0.81   0.035 *
## s(kneebreadth)  9.00 8.75    1.08   0.690  
## s(anthro3a)     9.00 1.00    1.09   0.770  
## s(anthro3c)     9.00 7.04    0.89   0.185  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Additionally we can also check for the train() method. Please note that
# train() won&amp;#39;t allow for smoothing the variables; it evidently decides which to
# smooth on its own.
bf_train_gam &amp;lt;- train(DEXfat ~ age +  waistcirc + hipcirc + elbowbreadth + 
                        kneebreadth + anthro3a + anthro3c,      
                      data = bodyfat, method = &amp;quot;gam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: model fit failed for Resample22: select= TRUE, method=GCV.Cp Error in magic(G$y, G$X, msp, G$S, G$off, L = G$L, lsp0 = G$lsp0, G$rank,  : 
##   magic, the gcv/ubre optimizer, failed to converge after 400 iterations.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bf_train_gam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## .outcome ~ s(elbowbreadth) + s(kneebreadth) + s(age) + s(hipcirc) + 
##     s(waistcirc) + s(anthro3a) + s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  30.7828     0.2968   103.7   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                       edf Ref.df     F  p-value    
## s(elbowbreadth) 3.186e-07      9 0.000   0.8204    
## s(kneebreadth)  7.825e+00      9 6.978 5.34e-08 ***
## s(age)          6.695e-01      9 0.374   0.0271 *  
## s(hipcirc)      1.843e+00      9 3.740 1.35e-08 ***
## s(waistcirc)    7.598e-01      9 2.512 2.53e-07 ***
## s(anthro3a)     6.742e-01      9 1.446 5.61e-06 ***
## s(anthro3c)     5.959e-01      9 0.975 4.41e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.949   Deviance explained = 95.8%
## GCV = 7.7062  Scale est. = 6.2553    n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# It looks like train() decided to smooth all the variables.
# This resulted in a less effective model.

#
cat(&amp;quot;# 1C&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1C&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
# Removing insignificant elbowbreadth including age variables
bodyfat_gam2 &amp;lt;- gam(DEXfat ~ waistcirc + s(hipcirc) +
                      s(kneebreadth) + s(anthro3a) + s(anthro3c),      
                    data = bodyfat)
# Assess Summary
summary(bodyfat_gam2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## DEXfat ~ waistcirc + s(hipcirc) + s(kneebreadth) + s(anthro3a) + 
##     s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 13.60862    4.74887   2.866 0.006052 ** 
## waistcirc    0.19654    0.05425   3.623 0.000676 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                  edf Ref.df      F  p-value    
## s(hipcirc)     1.610  2.010 10.910 0.000103 ***
## s(kneebreadth) 8.793  8.970  6.780 2.48e-06 ***
## s(anthro3a)    1.000  1.000 18.035 8.73e-05 ***
## s(anthro3c)    7.117  8.103  2.126 0.048737 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.954   Deviance explained = 96.7%
## GCV = 7.9464  Scale est. = 5.6498    n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wastcirc and anthro3a are no longer the most signifcant parameter.

# Assess plot
cat(&amp;quot;Plots for the model after removing insignificant variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plots for the model after removing insignificant variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:4, ncol = 2))
plot(bodyfat_gam2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot looks like we don&amp;#39;t need to smooth all variables.  These appear linear:
# anthro3a and (possibly) hipcirc


# Report model attributes
cat(&amp;quot;GCV of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GCV of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam2$gcv.ubre            # GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GCV.Cp 
## 7.946447&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam2$aic                 # AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 343.2562&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Adjusted r-squared of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted r-squared of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bodyfat_gam2)$r.sq       # Adjusted r-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9536683&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Total Degrees of freedom of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Total Degrees of freedom of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(bodyfat_gam2$edf)            # Total degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20.52&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run gam.check: is the normality assumption violated?
gam.check(bodyfat_gam2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 31 iterations.
## The RMS GCV score gradient at convergence was 7.054782e-07 .
## The Hessian was positive definite.
## Model rank =  38 / 38 
## 
## Basis dimension (k) checking results. Low p-value (k-index&amp;lt;1) may
## indicate that k is too low, especially if edf is close to k&amp;#39;.
## 
##                  k&amp;#39;  edf k-index p-value
## s(hipcirc)     9.00 1.61    1.01    0.48
## s(kneebreadth) 9.00 8.79    1.06    0.70
## s(anthro3a)    9.00 1.00    1.11    0.80
## s(anthro3c)    9.00 7.12    0.91    0.17&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
cat(&amp;quot;# 1D&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1D&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

log_transferred_DEX &amp;lt;- log(bodyfat$DEXfat)
df &amp;lt;- cbind (bodyfat, log_transferred_DEX)

bodyfat_gam3 &amp;lt;- gam(log_transferred_DEX ~ waistcirc + s(hipcirc) +
                      s(kneebreadth) + s(anthro3a) + s(anthro3c),      
                    data = df)
# Assess Summary
summary(bodyfat_gam3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## log_transferred_DEX ~ waistcirc + s(hipcirc) + s(kneebreadth) + 
##     s(anthro3a) + s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 2.973535   0.158102  18.808   &amp;lt;2e-16 ***
## waistcirc   0.004418   0.001806   2.447   0.0176 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                  edf Ref.df      F  p-value    
## s(hipcirc)     2.909  3.616 11.828  8.8e-07 ***
## s(kneebreadth) 2.325  2.962  2.027 0.128320    
## s(anthro3a)    1.000  1.000 15.576 0.000217 ***
## s(anthro3c)    7.358  8.263  4.678 0.000144 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.952   Deviance explained = 96.2%
## GCV = 0.0088137  Scale est. = 0.006878  n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hipcirc, anthro3a and anthro3c are the most significant smoothed terms and
# Kneebreadth is not significant here. Similarly, waistcirc is barely signifcant
# parameter (&amp;lt;0.005).

# Assess plot
#layout(matrix(1:4, ncol = 2))
#plot(bodyfat_gam3)

# plot looks like we don&amp;#39;t need to smooth all variables.  These appear linear:
# anthro3a.  Hipcirc is prominantly NOT linear in this light.


# Report model attributes
cat(&amp;quot;GCV of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GCV of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam3$gcv.ubre            # GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      GCV.Cp 
## 0.008813659&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam3$aic                 # AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -136.47&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Adjusted R-square of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted R-square of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bodyfat_gam3)$r.sq       # Adjusted r-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9522733&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Total degrees of freedom of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Total degrees of freedom of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(bodyfat_gam3$edf)            # Total degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15.59274&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run gam.check: is the normality assumption violated?
gam.check(bodyfat_gam3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 11 iterations.
## The RMS GCV score gradient at convergence was 5.573874e-08 .
## The Hessian was positive definite.
## Model rank =  38 / 38 
## 
## Basis dimension (k) checking results. Low p-value (k-index&amp;lt;1) may
## indicate that k is too low, especially if edf is close to k&amp;#39;.
## 
##                  k&amp;#39;  edf k-index p-value  
## s(hipcirc)     9.00 2.91    0.86    0.10 .
## s(kneebreadth) 9.00 2.33    0.83    0.09 .
## s(anthro3a)    9.00 1.00    1.00    0.48  
## s(anthro3c)    9.00 7.36    0.99    0.42  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Plots for the model with log transferred response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plots for the model with log transferred response&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,4))
plot(bodyfat_gam3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot(bodyfat_gam,select=1)
# plot(bodyfat_gam2,select=1)
# plot(bodyfat_gam3,select=1)
# plot(bodyfat_gam3,select=4)
# #compare with plots
# ggplot(bodyfat) + geom_line(aes(x=seq(1:nrow(bodyfat)), y=DEXfat), lty=1, col=1) +
#   geom_line(aes(x=seq(1:nrow(bodyfat)), y=bodyfat_gam$fitted.values), lty=2, col=2) +
#   geom_line(aes(x=seq(1:nrow(bodyfat)), y=bodyfat_gam2$fitted.values), lty=3, col=3) +
#   geom_line(aes(x=seq(1:nrow(bodyfat)), y=exp(bodyfat_gam2$fitted.values)), lty=4, col=4)
# 
# ggplot(bodyfat, aes(x=seq(1:nrow(bodyfat)), y=bodyfat_gam$fitted.values)) + geom_line(lty=2, col=6)
#
cat(&amp;quot;# 1E&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1E&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

bodyfat_boost &amp;lt;- gamboost(DEXfat ~ ., data=bodyfat)
bodyfat_aic &amp;lt;- AIC(bodyfat_boost)
cat(&amp;quot;printing AIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## printing AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_aic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.268173
## Optimal number of boosting iterations: 51 
## Degrees of freedom (for mstop = 51): 7.637287&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_gam &amp;lt;- bodyfat_boost[mstop(bodyfat_aic)]

cat(&amp;quot;plots for model fitted using gamboost:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## plots for model fitted using gamboost:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:9, ncol = 3))
plot(bf_gam)

cat(&amp;quot;extract variable names&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## extract variable names&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract(bf_gam,what=&amp;#39;variable.names&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bbs(waistcirc, df = dfbase)      bbs(hipcirc, df = dfbase) 
##                    &amp;quot;waistcirc&amp;quot;                      &amp;quot;hipcirc&amp;quot; 
## bbs(elbowbreadth, df = dfbase)  bbs(kneebreadth, df = dfbase) 
##                 &amp;quot;elbowbreadth&amp;quot;                  &amp;quot;kneebreadth&amp;quot; 
##     bbs(anthro3a, df = dfbase)     bbs(anthro3b, df = dfbase) 
##                     &amp;quot;anthro3a&amp;quot;                     &amp;quot;anthro3b&amp;quot; 
##     bbs(anthro3c, df = dfbase)      bbs(anthro4, df = dfbase) 
##                     &amp;quot;anthro3c&amp;quot;                      &amp;quot;anthro4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Here, variable &amp;#39;age&amp;#39; was removed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Here, variable &amp;#39;age&amp;#39; was removed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;#1a&lt;/p&gt;
&lt;p&gt;From the plot, it appears that age and elbowbreadth don’t correlate with DEXfat.
However, waistcirc, hipcirc, and all four anthro variables appear to correlate with DEXfat. Based on the plot, I would select waistcirc, hipcirc, anthro3a and anthro3c. However, I would also test the model using anthro3b rather than anthro3a to see which yields a better model.
Additionally, stepwise regression using step() function recommended: kneebreadth, waistcirc, hipcirc and anthro3b. This also confirm that the anthro variables are sufficiently correlated to be represented by one variable which in this case is anthro3b.&lt;/p&gt;
&lt;p&gt;#1b.&lt;/p&gt;
&lt;p&gt;Not all covariates should be smoothed. Looking at the plot, the covariates that appear linear are: age, elbowbreadth, waistcirc and anthro3a. That leaves anthro3c, kneebreadth and hipcirc to be smoothed.&lt;/p&gt;
&lt;p&gt;Report model attributes:
GCV
8.435412
AIC
345.708
Adjusted R-squared
0.9528156
Total degrees of Freedom
22.57091&lt;/p&gt;
&lt;p&gt;gam.check:
While the residuals plot appears random, the histogram makes the data appear skewed to the left, so the assumption of normality doesn’t entirely hold.&lt;/p&gt;
&lt;p&gt;This summary shows that age, elbowbreadth, and anthro3c are not significant at
the significance level of 0.05. The variables age, waistcirc, elbowbreadth and
anthro3a all have linear relationships as shown in the plots. The model seems
to give a moderate GCV and high AIC which could possibily be adjusted by
variable selection and using smoothing functions on the variables mentioned
above. The adjusted R2 does indicate that the model explains most of the
variance, but as stated previously the model can improve.&lt;/p&gt;
&lt;div id=&#34;c.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1c.&lt;/h1&gt;
&lt;p&gt;GCV of bodyfat_gam2:
7.946447
AIC of bodyfat_gam2:
343.2562
Adjusted r-squared of bodyfat_gam2:
0.9536683
Total Degrees of freedom of bodyfat_gam2:
20.52&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1d.&lt;/h1&gt;
&lt;p&gt;In this case, the log-transformed model is slightly better in that it accounts for slightly more of the deviation.
Report GCV, AIC, adj-R2, and the total model degrees of freedom.
GCV: 0.0088
AIC: -136.4700
adj-R2: 0.9523
Total DF: 15.5927&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1e.&lt;/h1&gt;
&lt;p&gt;Age variable was removed by gamboost() function.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit a logistic additive model to the glaucoma data. (Here use family = “binomial”). Which covariates should enter the model and how is their influence on the probability of suffering from glaucoma? (Hint: since there are many covariates, use  to fit the GAM model.)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;TH.data&amp;quot;)
data(&amp;quot;GlaucomaM&amp;quot;)
# cat(&amp;quot;head(GlaucomaM)&amp;quot;)
# head(GlaucomaM)
# nrow(GlaucomaM) # 196
# names(GlaucomaM)

glau_gamb &amp;lt;- gamboost(Class ~., data = GlaucomaM, family = Binomial())

summary(glau_gamb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Model-based Boosting
## 
## Call:
## gamboost(formula = Class ~ ., data = GlaucomaM, family = Binomial())
## 
## 
##   Negative Binomial Likelihood (logit link) 
## 
## Loss function: { 
##      f &amp;lt;- pmin(abs(f), 36) * sign(f) 
##      p &amp;lt;- exp(f)/(exp(f) + exp(-f)) 
##      y &amp;lt;- (y + 1)/2 
##      -y * log(p) - (1 - y) * log(1 - p) 
##  } 
##  
## 
## Number of boosting iterations: mstop = 100 
## Step size:  0.1 
## Offset:  0 
## Number of baselearners:  62 
## 
## Selection frequencies:
##  bbs(tmi, df = dfbase) bbs(mhcg, df = dfbase) bbs(vars, df = dfbase) 
##                   0.17                   0.11                   0.11 
## bbs(mhci, df = dfbase)  bbs(hvc, df = dfbase) bbs(vass, df = dfbase) 
##                   0.10                   0.08                   0.08 
##   bbs(as, df = dfbase) bbs(vari, df = dfbase)   bbs(mv, df = dfbase) 
##                   0.07                   0.06                   0.04 
## bbs(abrs, df = dfbase) bbs(mhcn, df = dfbase) bbs(phcn, df = dfbase) 
##                   0.03                   0.03                   0.03 
##  bbs(mdn, df = dfbase) bbs(phci, df = dfbase)  bbs(hic, df = dfbase) 
##                   0.03                   0.02                   0.01 
## bbs(phcg, df = dfbase)  bbs(mdi, df = dfbase)  bbs(tms, df = dfbase) 
##                   0.01                   0.01                   0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:9, ncol = 3))
plot(glau_gamb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Covariates that should enter the model as determined by gamboost:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Covariates that should enter the model as determined by gamboost:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract(glau_gamb,what=&amp;#39;variable.names&amp;#39;)
var_names  &amp;lt;- unname(extract(glau_gamb,what=&amp;#39;variable.names&amp;#39;))
var_names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;as&amp;quot;   &amp;quot;abrs&amp;quot; &amp;quot;hic&amp;quot;  &amp;quot;mhcg&amp;quot; &amp;quot;mhcn&amp;quot; &amp;quot;mhci&amp;quot; &amp;quot;phcg&amp;quot; &amp;quot;phcn&amp;quot; &amp;quot;phci&amp;quot; &amp;quot;hvc&amp;quot; 
## [11] &amp;quot;vass&amp;quot; &amp;quot;vars&amp;quot; &amp;quot;vari&amp;quot; &amp;quot;mdn&amp;quot;  &amp;quot;mdi&amp;quot;  &amp;quot;tms&amp;quot;  &amp;quot;tmi&amp;quot;  &amp;quot;mv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Using the variables indicated by gamboost, run a gam model to get summary data
# phcg and phci seem smooth already
glau_gam &amp;lt;- gam(Class ~ s(as) + s(abrs)   + s(hic)  + s(mhcg)  + s(mhcn) + s(mhci)             +   phcg +  s(phcn)  + phci + s(hvc) + s(vass) + s(vars) + s(vari) +                   s(mdn) + s(mdi) + s(tms) +  s(tmi) + s(mv), 
                data=GlaucomaM, family=binomial)

summary(glau_gam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: binomial 
## Link function: logit 
## 
## Formula:
## Class ~ s(as) + s(abrs) + s(hic) + s(mhcg) + s(mhcn) + s(mhci) + 
##     phcg + s(phcn) + phci + s(hvc) + s(vass) + s(vars) + s(vari) + 
##     s(mdn) + s(mdi) + s(tms) + s(tmi) + s(mv)
## 
## Parametric coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    22.55    1581.67   0.014    0.989
## phcg          512.85    7725.61   0.066    0.947
## phci         -290.45    7534.66  -0.039    0.969
## 
## Approximate significance of smooth terms:
##           edf Ref.df Chi.sq p-value
## s(as)   2.095  2.179  0.019   0.995
## s(abrs) 1.000  1.000  0.002   0.963
## s(hic)  1.000  1.000  0.003   0.957
## s(mhcg) 1.522  1.562  0.001   0.997
## s(mhcn) 1.278  1.306  0.001   0.994
## s(mhci) 2.684  2.771  0.002   1.000
## s(phcn) 4.000  4.058  0.006   1.000
## s(hvc)  5.579  5.643  0.020   1.000
## s(vass) 1.000  1.000  0.031   0.861
## s(vars) 1.000  1.000  0.002   0.961
## s(vari) 2.761  2.828  0.013   0.999
## s(mdn)  1.000  1.000  0.001   0.980
## s(mdi)  1.000  1.000  0.003   0.957
## s(tms)  1.000  1.000  0.000   0.995
## s(tmi)  2.618  2.695  0.027   0.997
## s(mv)   1.000  1.000  0.007   0.935
## 
## R-sq.(adj) =      1   Deviance explained =  100%
## UBRE = -0.65774  Scale est. = 1         n = 196&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;Covariates indicated by gamboost() were used to generate a gam model. The names are listed in the summary printed above. The summary also shows the probability of their influence (somewhat high) for suffering from glaucoma. I think with 100 percent of the deviance explained, there should be a concern that this model is extremely over-fitting.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Investigate the use of different types of scatterplot smoothers on the Hubble data from Chapter 6. (Hint: follow the example on men1500m data scattersmoothers page 199 of Handbook).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## GAM model
library(&amp;quot;HSAUR3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:TH.data&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;mgcv&amp;quot;)
library(&amp;quot;GGally&amp;quot;)
library(&amp;quot;mboost&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;wordcloud&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: RColorBrewer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data(&amp;quot;bodyfat&amp;quot;, package = &amp;quot;TH.data&amp;quot;)
data(&amp;quot;hubble&amp;quot;, package = &amp;quot;gamair&amp;quot;)
head(hubble)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Galaxy    y     x
## 1  NGC0300  133  2.00
## 2  NGC0925  664  9.16
## 3 NGC1326A 1794 16.14
## 4  NGC1365 1594 17.95
## 5  NGC1425 1473 21.88
## 6  NGC2403  278  3.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sorted_value&amp;lt;-hubble[order(hubble$x),]
x &amp;lt;- sorted_value$x
y &amp;lt;- sorted_value$y

lowess_value &amp;lt;- lowess(x, y)
plot(y~x, data = sorted_value, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;,main=&amp;quot;base R: Lowess scatterplot smoother&amp;quot;)
lines(lowess_value, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x=x, y=y) + 
  geom_point()+
  geom_line(aes(x = lowess_value$x, y = lowess_value$y)) +
    labs(title = &amp;quot;ggplot: Lowess scatterplot smoother&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(y~x, data = sorted_value, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;,main=&amp;quot;base R: Cubic scatterplot smoother&amp;quot;)
cubic_value = gam(y ~ s(x, bs = &amp;quot;cr&amp;quot;), data = sorted_value)
lines(sorted_value$x, predict(cubic_value), lty=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x=x, y=y) + 
  geom_point()+
  geom_line(aes(x = sorted_value$x, y = predict(cubic_value))) +
    labs(title = &amp;quot;ggplot: Cubic scatterplot smoother&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cubic_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ s(x, bs = &amp;quot;cr&amp;quot;)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   924.38      51.87   17.82 4.27e-14 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##        edf Ref.df    F  p-value    
## s(x) 2.148  2.648 26.8 2.88e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.754   Deviance explained = 77.7%
## GCV =  74317  Scale est. = 64570     n = 24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(y~x, data = sorted_value, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;,main=&amp;quot;base R: Quadratic model scatterplot smoother&amp;quot;)
lm_value = lm(y~x+I(x^2), data = sorted_value)
lines(sorted_value$x, predict(lm_value))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x=x, y=y) + 
  geom_point()+
  geom_line(aes(x = sorted_value$x, y = predict(lm_value))) +
    labs(title = &amp;quot;ggplot: Quadratic scatterplot smoother&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;The 3 different smoothers line shows that quadratic is higher than cubic and lowess, while cubic is higher than lowess. However, I am not sure if it would be logical to explain the smoother one fits better or not.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression&amp;GLM-II</title>
      <link>/achalneupane.github.io/post/logistic_regression_glm_ii/</link>
      <pubDate>Sat, 21 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/logistic_regression_glm_ii/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the  data from the  library to answer the following questions&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Construct graphical and numerical summaries that will show the relationship between tumor size and the number of recurrent tumors. Discuss your discovery. (Hint: mosaic plot may be a great way to assess this)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build a Poisson regression that estimates the effect of size of tumor on the number of recurrent tumors. Discuss your results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;bladdercancer&amp;quot;, package = &amp;quot;HSAUR3&amp;quot;)
# base R plot version
# head(bladdercancer)
cat(&amp;quot;#1a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mosaicplot(xtabs( ~ number + tumorsize, data = bladdercancer),
           main = &amp;quot;base R: The Number of recurrent tumors compared with tumor size&amp;quot;,
           shade = TRUE)

# ggplot version:
# install.packages(&amp;#39;ggmosaic&amp;#39;)
library(ggmosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = bladdercancer) +
  geom_mosaic(aes(x = product(tumorsize, number), fill = tumorsize), na.rm =
                FALSE) +
  labs(x = &amp;quot;Number&amp;quot;, x = &amp;quot;Tumour Size&amp;quot;, title = &amp;#39;ggplot: The Number of recurrent tumors compared with tumor size&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can also visualize this by creating percentage table using `prop.table`
# function.
table_rows_percentage &amp;lt;-
  table(bladdercancer$tumorsize, bladdercancer$number)
colnames(table_rows_percentage) &amp;lt;-
  c(&amp;quot;Tumour_1 (counts)&amp;quot;,
    &amp;quot;Tumour_2 (counts)&amp;quot;,
    &amp;quot;Tumour_3 (counts)&amp;quot;,
    &amp;quot;Tumour_4 (counts)&amp;quot;)
cat(&amp;quot;Table of tumour number and frequency:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Table of tumour number and frequency:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table_rows_percentage&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Tumour_1 (counts) Tumour_2 (counts) Tumour_3 (counts) Tumour_4 (counts)
##   &amp;lt;=3cm                15                 5                 1                 1
##   &amp;gt;3cm                  5                 2                 1                 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bladdercancer %&amp;gt;%
#   group_by(tumorsize,number) %&amp;gt;%
#   summarize(freq = n()) %&amp;gt;%
#   spread(number,freq,sep=&amp;#39;_of_tumors_&amp;#39;)
tt &amp;lt;- prop.table(table_rows_percentage, 1)
colnames(tt) &amp;lt;-
  c(&amp;quot;Tumour_1(%)&amp;quot;, &amp;quot;Tumour_2(%)&amp;quot;, &amp;quot;Tumour_3(%)&amp;quot;, &amp;quot;Tumour_4(%)&amp;quot;)
cat(&amp;quot;Table of tumour number and frequency in %:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Table of tumour number and frequency in %:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Tumour_1(%) Tumour_2(%) Tumour_3(%) Tumour_4(%)
##   &amp;lt;=3cm  0.68181818  0.22727273  0.04545455  0.04545455
##   &amp;gt;3cm   0.55555556  0.22222222  0.11111111  0.11111111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1a. &lt;strong&gt;Discussion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Based on the mosaic plot, frequency table or the percentage table above, we can tell that the observed frequency for 1 or 2 tumors greater than 3cm (&amp;gt;3cm) is lower than expected and the observed frequency for 3 or 4 tumors less than or equal to 3 cm (&amp;lt;=3cm) is also lower than what we would expect for this data.&lt;/p&gt;
&lt;p&gt;1b.&lt;/p&gt;
&lt;p&gt;Building a Poisson regression that estimates the effect of size of tumor on
the number of recurrent tumors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- glm(number ~ tumorsize,data=bladdercancer,family=poisson())
summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = number ~ tumorsize, family = poisson(), data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6363  -0.3996  -0.3996   0.4277   1.7326  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)     0.3747     0.1768   2.120    0.034 *
## tumorsize&amp;gt;3cm   0.2007     0.3062   0.655    0.512  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.80  on 30  degrees of freedom
## Residual deviance: 12.38  on 29  degrees of freedom
## AIC: 87.191
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;model1 (mod1): If we test the model dropping the time variable. It shows that the intercept is significant (P&amp;lt;0.05), but the tumour size is not significant.&lt;/p&gt;
&lt;p&gt;Additionally, we can also test models considering the time interaction&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- glm(number ~time + tumorsize + tumorsize*time,data=bladdercancer,family=poisson(link=log))
summary(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = number ~ time + tumorsize + tumorsize * time, family = poisson(link = log), 
##     data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6943  -0.5581  -0.2413   0.2932   1.4644  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)         0.03957    0.43088   0.092    0.927
## time                0.02138    0.02418   0.884    0.377
## tumorsize&amp;gt;3cm       0.46717    0.66713   0.700    0.484
## time:tumorsize&amp;gt;3cm -0.01676    0.03821  -0.439    0.661
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.800  on 30  degrees of freedom
## Residual deviance: 11.566  on 27  degrees of freedom
## AIC: 90.377
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;model2 (mod2): If we consider time interaction with the tumour size, we can clearly see that none of the variables are significant.&lt;/p&gt;
&lt;p&gt;If we remove time interaction from above model, we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod3 &amp;lt;- glm(number ~ time + tumorsize,data=bladdercancer,family=poisson())
summary(mod3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = number ~ time + tumorsize, family = poisson(), 
##     data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8183  -0.4753  -0.2923   0.3319   1.5446  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    0.14568    0.34766   0.419    0.675
## time           0.01478    0.01883   0.785    0.433
## tumorsize&amp;gt;3cm  0.20511    0.30620   0.670    0.503
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.800  on 30  degrees of freedom
## Residual deviance: 11.757  on 28  degrees of freedom
## AIC: 88.568
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;model3 (mod3): If we drop time interaction from previous model (mod2), we still do not get anything significant with the time or tumour size. However, the AIC value drops to 88.56.&lt;/p&gt;
&lt;p&gt;In all three models we compared, we can also see that the residual and null deviance values are low compared to the degrees of freedom. If our Null Deviance is really small, it means that the Null Model explains the data pretty well. Likewise with your Residual Deviance.&lt;/p&gt;
&lt;p&gt;Additionaly, we can perform a Chi-squared test for the Null deviance to check
whether any of the predictors have an influence on the response variables in
our three models using function &lt;code&gt;pchisq&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Source: https://stat.ethz.ch/education/semesters/as2015/asr/Uebungen/Uebungen/solution8.pdf
pchisq((mod1$null.deviance-mod1$deviance), df = (mod1$df.null-mod1$df.residual), lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5171827&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pchisq((mod2$null.deviance-mod2$deviance), df = (mod2$df.null-mod2$df.residual), lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7448414&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pchisq((mod3$null.deviance-mod3$deviance), df  = (mod3$df.null-mod3$df.residual), lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5935891&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-values in all three models are larger than 0.05, which tells us that there is no significant predictor in our model.&lt;/p&gt;
&lt;p&gt;Additionally, if we can compare all three models we built above for analysis of deviance using ANOVA:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod1,mod2,mod3,test=&amp;#39;Chisq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: number ~ tumorsize
## Model 2: number ~ time + tumorsize + tumorsize * time
## Model 3: number ~ time + tumorsize
##   Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)
## 1        29     12.380                     
## 2        27     11.566  2  0.81458   0.6655
## 3        28     11.757 -1 -0.19095   0.6621&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here as well, we do not find any of these models to be significant.&lt;/p&gt;
&lt;p&gt;Final conclusion: Based on these analysis we can tell that the acceptance of the null hypothesis is evident in this case because there is nothing within the data to explain an increment in the number of tumors. Since we tested both tumour size and time variables, we can tell that &lt;strong&gt;neither time&lt;/strong&gt; nor the &lt;strong&gt;tumour size&lt;/strong&gt; have any effect on increasing &lt;strong&gt;number&lt;/strong&gt; of tumours.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The following data is the number of new AIDS cases in Belgium between the years 1981-1993. Let &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; denote time
&lt;p&gt;Do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Plot the relationship between AIDS cases against time. Comment on the plot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a Poisson regression model &lt;span class=&#34;math inline&#34;&gt;\(log(\mu_i)=\beta_0+\beta_1t_i\)&lt;/span&gt;. Comment on the model parameters and residuals (deviance) vs Fitted plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now add a quadratic term in time ( ) and fit the model. Comment on the model parameters and assess the residual plots.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare the two models using AIC. Which model is better?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use -function to perform &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; test for model selection. Did adding the quadratic term improve model?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240)
t = 1:13

data &amp;lt;- as.data.frame(cbind(t, y))

cat(&amp;quot;#2a (base R plot version)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2a (base R plot version)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(y ~ t,
     main = &amp;quot;base R: Number of AIDs cases from 1981-1993&amp;quot;,
     xlab = &amp;quot;Time in Years from 1981&amp;quot;,
     ylab = &amp;quot;Number of Aids cases&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2a ggplot version&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2a ggplot version&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x = t, y = y) + geom_point() + labs(title = &amp;quot;ggplot: Number of AIDs cases from 1981-1993&amp;quot;, x = &amp;quot;Time in Years from 1981&amp;quot;, y = &amp;quot;Number of Aids cases&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Poisson model
aids.pois &amp;lt;- glm(y ~ t, data = data, family = &amp;quot;poisson&amp;quot;)
summary(aids.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ t, family = &amp;quot;poisson&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.6784  -1.5013  -0.2636   2.1760   2.7306  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) 3.140590   0.078247   40.14   &amp;lt;2e-16 ***
## t           0.202121   0.007771   26.01   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 872.206  on 12  degrees of freedom
## Residual deviance:  80.686  on 11  degrees of freedom
## AIC: 166.37
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coefficients
exp(coef(aids.pois)) # coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           t 
##   23.117491    1.223996&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(confint(aids.pois)) # confidence interval&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 2.5 %    97.5 %
## (Intercept) 19.789547 26.894433
## t            1.205624  1.242922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#use code below for residual plots
plot(aids.pois, which = 1, main = &amp;quot;base R: Residual Vs fitted plot for y ~ t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version
# https://stackoverflow.com/questions/36731027/how-can-i-plot-the-residuals-of-lm-with-ggplot

# cc &amp;lt;- data.frame(aids.pois$residuals, aids.pois$fitted.values)
#
# ggplot(cc, aes(x = aids.pois.fitted.values, y = aids.pois.residuals)) +
#   geom_point() +
#   geom_abline()

# ggplot(cc, aes(x = aids.pois.fitted.values, y = aids.pois.residuals)) +
#   geom_smooth(method=&amp;quot;loess&amp;quot;, color=&amp;quot;red&amp;quot;, se=FALSE) +
#   geom_hline(yintercept = 0, linetype=2, color=&amp;quot;darkgrey&amp;quot;) +
#   geom_point()+ labs(title = &amp;quot;ggplot: Residual Vs fitted plot&amp;quot;)

# ggplot version
ggplot(aids.pois, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth(group = 1, formula = y ~ x) + labs(title = &amp;quot;ggplot: Residual Vs fitted plot for y ~ t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2c&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$t2 &amp;lt;- data$t ^ 2
aids2.pois &amp;lt;- glm(y ~ t + t2, data = data, family = &amp;quot;poisson&amp;quot;)
summary(aids2.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ t + t2, family = &amp;quot;poisson&amp;quot;, data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.45903  -0.64491   0.08927   0.67117   1.54596  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  1.901459   0.186877  10.175  &amp;lt; 2e-16 ***
## t            0.556003   0.045780  12.145  &amp;lt; 2e-16 ***
## t2          -0.021346   0.002659  -8.029 9.82e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 872.2058  on 12  degrees of freedom
## Residual deviance:   9.2402  on 10  degrees of freedom
## AIC: 96.924
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coefficients
exp(coef(aids2.pois)) # coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           t          t2 
##   6.6956535   1.7436895   0.9788799&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(confint(aids2.pois)) # confidence interval&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 2.5 %    97.5 %
## (Intercept) 4.5976982 9.5678396
## t           1.5965138 1.9104525
## t2          0.9737254 0.9839292&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#use code below for residual plots
plot(aids2.pois, which = 1, main = &amp;quot;base R: Residual Vs fitted plot for y ~ t + t2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version
ggplot(aids2.pois, aes(x = .fitted, y = .resid)) + geom_point() + labs(title = &amp;quot;ggplot: Residuals Vs fitted plot for y ~ t + t2&amp;quot;) + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2d&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2d&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(aids.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 166.3698&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(aids2.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 96.92358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2e&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2e&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(aids.pois, aids2.pois, test = &amp;quot;Chisq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: y ~ t
## Model 2: y ~ t + t2
##   Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    
## 1        11     80.686                          
## 2        10      9.240  1   71.446 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2a. The number of new AIDS cases has an increasing trend over time and seems to be leveling off between 1981-1991 then it remains somewhat unchanged until 1993. The maximum number of new AIDS cases occurs in 1991.&lt;/p&gt;
&lt;p&gt;2b. Both (b0) and (b1) are statistically significant from zero.&lt;/p&gt;
&lt;p&gt;Interpretation of the coefficients calculated by exponentiating the estimates:&lt;/p&gt;
&lt;p&gt;exp(b1) =1.22 : A one year increase will result in a 22% increase in the mean number of new AIDs cases.&lt;/p&gt;
&lt;p&gt;exp(b0) =23.1 : When t=0, the average number of AID cases is 23.1.&lt;/p&gt;
&lt;p&gt;Likewise, comparing the residual deviance of the model, we can tell that the model is over-spread by 7.80 times on 11 degrees of freedom. Based on the residual plot, we can tell that at time 1, 2, and 13 the residual values are further away from zero indicating they are outliers. Additionally, there is a clear pattern to the residual plot which indicates that mean does not increase as the variance increase because there is not a constant spread in the residuals.&lt;/p&gt;
&lt;p&gt;Additionally, we can see a curved pattern in the Residual vs. Fitted plot. This could tell us that a transformation or adding a quadratic term to the model would be suitable.&lt;/p&gt;
&lt;p&gt;2c. All the model parameters are statistically significant from zero.&lt;br /&gt;
Interpretation of the coefficients calculated by exponentiating the estimates:&lt;/p&gt;
&lt;p&gt;exp(b1) =1.74: Taking all other parameters constant, a one year increase will result in a 74% increase in the mean number of new AID cases.&lt;/p&gt;
&lt;p&gt;exp(b2) =0.98 : Taking all other parameters constant, a one year increase will result in a 2% decrease in the mean number of new AID cases.&lt;/p&gt;
&lt;p&gt;exp(b0) =6.7 : When t=0 and t^2=0, the average number of AID cases is 6.7.&lt;/p&gt;
&lt;p&gt;Additionally, the residuals vs. fitted values plot looks much better than model one. The residuals seems randomly distributed around 0.&lt;/p&gt;
&lt;p&gt;2d. Based on the AIC values and the residual plots, model 2 is a better fit for this data.&lt;/p&gt;
&lt;p&gt;2e.Based on the chi-square test statistic and p-value—in this case we reject the null hypothesis at the  = 0.05 level that model 1 is true. We can tell that the larger model is better, which in this case, adding the quadratic term did improve the model.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the  dataset from  library. The dataset contains information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt. It is a 4 dimensional dataset with 10000 observations. You had developed a logistic regression model on HW #2. Now consider the following two models
&lt;p&gt;For the two competing models do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;With the whole data compare the two models (Use AIC and/or error rate)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use validation set approach and choose the best model. Be aware that we have few people who defaulted in the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use LOOCV approach and choose the best model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use 10-fold cross-validation approach and choose the best model&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Report validation misclassification (error) rate for both models in each of the three assessment methods. Discuss your results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;Default&amp;quot;, package = &amp;quot;ISLR&amp;quot;)
Default$default&amp;lt;-as.numeric(Default$default==&amp;quot;Yes&amp;quot;)

mod.log1&amp;lt;-glm(default ~ student + balance , data = Default, family = binomial())
# summary(mod.log1)


cat (&amp;quot;#3a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.log2&amp;lt;-glm(default ~ balance , data = Default, family = binomial())
cat(&amp;quot;AIC for mod.log1:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC for mod.log1:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cat(&amp;quot;AIC(mod.log1) =&amp;quot;, AIC(mod.log1))
AIC(mod.log1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1577.682&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC for mod.log2:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC for mod.log2:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod.log2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1600.452&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cat(&amp;quot;AIC(mod.log2) =&amp;quot;, AIC(mod.log2))
anova(mod.log1, mod.log2, test=&amp;quot;Chisq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: default ~ student + balance
## Model 2: default ~ balance
##   Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    
## 1      9997     1571.7                          
## 2      9998     1596.5 -1   -24.77 6.459e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#3b Validation approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3b Validation approach&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index&amp;lt;-sample(1:nrow(Default), size=0.6*nrow(Default))
train&amp;lt;- Default[index, ]
val&amp;lt;- Default[-index, ]


mod.train1&amp;lt;-glm(default ~ student + balance , data = train, family = binomial())
# summary(mod.train1)
mod.train2&amp;lt;-glm(default ~ balance , data = train, family = binomial())
# summary(mod.train2)
pred1&amp;lt;-predict(mod.train1, val, type = &amp;quot;response&amp;quot;)
pred2&amp;lt;-predict(mod.train2, val, type = &amp;quot;response&amp;quot;)

cat(&amp;quot;Error rate: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate1&amp;lt;- mean((pred1&amp;gt;0.5 &amp;amp; val$default==0) | (pred1&amp;lt;0.5 &amp;amp; val$default==1))
err.rate2&amp;lt;- mean((pred2&amp;gt;0.5 &amp;amp; val$default==0) | (pred2&amp;lt;0.5 &amp;amp; val$default==1))

cat(&amp;quot;Error rate of model1 =&amp;quot;, err.rate1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate of model1 = 0.027&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Error rate of model2 =&amp;quot;, err.rate2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate of model2 = 0.0265&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#3c LOOCV&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3c LOOCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)

cost &amp;lt;- function(r, pi = 0) mean(abs(r-pi) &amp;gt; 0.5)
cv.err &amp;lt;- cv.glm(Default,mod.log1, cost)$delta
cv.err2 &amp;lt;- cv.glm(Default, mod.log2, cost)$delta
cat(&amp;quot;LOOCV of model1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOOCV of model1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0267 0.0267&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;LOOCV of model2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOOCV of model2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02750000 0.02749994&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#3d 10-fold cross validation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3d 10-fold cross validation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err1.10 &amp;lt;- cv.glm(Default, mod.log1, cost ,K=10)$delta
cv.err2.10 &amp;lt;- cv.glm(Default, mod.log2, cost ,K=10)$delta

cat(&amp;quot;10-fold cross validation of Model1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10-fold cross validation of Model1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err1.10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02670 0.02675&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;10-fold cross validation of Model2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10-fold cross validation of Model2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err2.10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02740 0.02735&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3a. The first model with both student and balance has the smaller AIC. The anova-function was also used to perform a chi-square test for model selection and again concluded the first model was better.&lt;/p&gt;
&lt;p&gt;3b. Splitted the data into 60/40 between the training and validation data sets and made sure the default rate was similar between the two dataset, then fitted the models to the training data and then used the validation set to calculate the error rate using 0.5 as out threshold.&lt;/p&gt;
&lt;p&gt;For model 1, the MSE is 0.023.&lt;/p&gt;
&lt;p&gt;For model 2, the MSE is 0.024.&lt;/p&gt;
&lt;p&gt;Based on these values we would chose model 1 as our best model. We will also examine other validation techniques below.&lt;/p&gt;
&lt;p&gt;3c. LOOCV prediction error is adjusted for bias and we still want the smallest prediction errors.
For model 1, the adjusted prediction error is 0.0267.&lt;/p&gt;
&lt;p&gt;For model 2, the adjusted prediction error is 0.02749994.&lt;/p&gt;
&lt;p&gt;Therefore, we choose model 1 as the best model because it has the smaller adjusted prediction rate using the LOOCV approach.&lt;/p&gt;
&lt;p&gt;3d. Using K=10 for the 10-fold cross-validation approach, we obtain the following error rates:&lt;/p&gt;
&lt;p&gt;For model 1, the CV error rate is 0.02667
For model 2, the CV error rate is 0.0278&lt;/p&gt;
&lt;p&gt;Again, we can choose model 1 as our best model. Though it was little easier to calculate the 10-fold cross validation error rate than the LOOCV error rate but our conclusion is the same.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the  library load the  dataset. This contains Daily percentage returns for the S&amp;amp;P 500 stock index between 2001 and 2005. There are 1250 observations and 9 variables. The variable of interest is Direction which is a factor with levels Down and Up indicating whether the market had a positive or negative return on a given day. Since the goal is to predict the direction of the stock market in the future, here it would make sense to use the data from years 2001 - 2004 as training and 2005 as validation. According to this, create a training set and testing set. Perform logistic regression and assess the error rate.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;Smarket&amp;quot;, package = &amp;quot;ISLR&amp;quot;)
Smarket$Direction &amp;lt;- as.numeric(Smarket$Direction == &amp;quot;Up&amp;quot;)

train.mark &amp;lt;- subset(Smarket, Year &amp;lt;= 2004)
val.mark &amp;lt;- subset(Smarket, Year &amp;gt; 2004)


#Model 1
mod.train.mark &amp;lt;-
  glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 ,
      data = train.mark,
      family = binomial())
summary(mod.train.mark)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, family = binomial(), 
##     data = train.mark)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.339  -1.189   1.070   1.163   1.326  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  0.032269   0.063379   0.509    0.611
## Lag1        -0.055510   0.051706  -1.074    0.283
## Lag2        -0.044218   0.051681  -0.856    0.392
## Lag3         0.008918   0.051517   0.173    0.863
## Lag4         0.008556   0.051514   0.166    0.868
## Lag5        -0.003243   0.051089  -0.063    0.949
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1383.3  on 997  degrees of freedom
## Residual deviance: 1381.3  on 992  degrees of freedom
## AIC: 1393.3
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate1 &amp;lt;-
  mean((
    predict(mod.train.mark, val.mark, type = &amp;quot;response&amp;quot;) - val.mark$Direction
  ) ^ 2)
cat(&amp;quot;Error rate model1:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate model1:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2483559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Model 2
mod.train.mark2 &amp;lt;-
  glm(Direction ~ Lag1 + Lag2 + Lag3,
      data = train.mark,
      family = binomial())
summary(mod.train.mark2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3, family = binomial(), 
##     data = train.mark)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.338  -1.189   1.072   1.163   1.335  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  0.032230   0.063377   0.509    0.611
## Lag1        -0.055523   0.051709  -1.074    0.283
## Lag2        -0.044300   0.051674  -0.857    0.391
## Lag3         0.008815   0.051495   0.171    0.864
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1383.3  on 997  degrees of freedom
## Residual deviance: 1381.4  on 994  degrees of freedom
## AIC: 1389.4
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate2 &amp;lt;-
  mean((
    predict(mod.train.mark2, val.mark, type = &amp;quot;response&amp;quot;) - val.mark$Direction
  ) ^ 2)
cat(&amp;quot;Error rate model2:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate model2:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2483144&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The error rate for model 1 which includes predictor variables lag 1-5 is: 0.4126984.&lt;/p&gt;
&lt;p&gt;The error rate for model 2 which includes predictor variables lag 1-3 is: 0.4087302.&lt;/p&gt;
&lt;p&gt;We can choose the simpler model 2 based on the error rate. This error rate suggests that we are able to predict the direction of the stock market. We can predict the right outcome at around 60% of the time, which is still better than predicting randomly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression&amp;GLM-I</title>
      <link>/achalneupane.github.io/post/logistic_regression_glm_i/</link>
      <pubDate>Wed, 18 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/logistic_regression_glm_i/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how
your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word
Document saved as a PDF). If you are having trouble with .rmd, let us know and
we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please
follow the instructions found under Content for Formatting and Guidelines. No
code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well
as do the same but using the respective commands in the GGPLOT2 library. (So if
the question asks for one plot, your results should have two plots. One produced
using the given R-function and one produced from the GGPLOT2 equivalent). This
doesn’t apply to questions that don’t specifically ask for a plot, however I
still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Collett (2003) argues that two outliers need to be removed from the
 data. Try to identify those two unusual observations by means of
a scatterplot. (7.2 on Handbook)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;calibrate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;HSAUR3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;knitr&amp;quot;)
data(&amp;quot;plasma&amp;quot;)

plasma$rownumber &amp;lt;- 1:nrow(plasma)
plot.new()
# square plotting region, independent of device size; 1 x 1 pictures on one plot
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(
  plasma$fibrinogen,
  plasma$globulin,
  col = plasma$ESR,
  data = plasma,
  pch = 18,
  main = &amp;quot;base R: Scatterplot of plasma data&amp;quot;,
  xlab = &amp;quot;Fibrinogen&amp;quot;,
  ylab = &amp;quot;Globulin&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in plot.window(...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in plot.xy(xy, type, ...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in axis(side = side, at = at, labels = labels, ...): &amp;quot;data&amp;quot; is not a
## graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &amp;quot;data&amp;quot; is not a
## graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in box(...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in title(...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text(globulin ~fibrinogen, labels = rownumber,data=plasma, cex=0.8, font=0.5)
abline(v = 3.5)
abline(h = 45)
# points(plasma[c(27, 30, 32), 1:2], pch = 5)
legend(
  &amp;quot;bottomright&amp;quot;,
  c(&amp;quot;ESR&amp;lt;20&amp;quot;, &amp;quot;ESR&amp;gt;20&amp;quot;),
  # title = &amp;quot;ESR&amp;quot;,
  inset = c(0, 1),
  xpd=TRUE, 
  horiz=TRUE,
  col = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;),
  # lty = c(1, 1),
  pch = c(18, 18),
  bty = &amp;quot;n&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = plasma, aes(x = fibrinogen, y = globulin, colour = ESR)) +
  geom_point() +
  ggtitle(&amp;quot;ggplot: Scatterplot of Plasma data&amp;quot;) +
  xlab(&amp;quot;Fibrinogen&amp;quot;) +
  ylab(&amp;quot;Globulin&amp;quot;) +
  geom_text(aes(label=plasma$rownumber),hjust=0, vjust=0) +
  geom_hline(yintercept=45, linetype=&amp;quot;dashed&amp;quot;, 
             color = &amp;quot;black&amp;quot;, size=0.5) +
  geom_vline(xintercept=3.5, linetype=&amp;quot;dashed&amp;quot;, 
             color = &amp;quot;black&amp;quot;, size=0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion: Based on the scatter plot (and if we consider all data points, both ESR &amp;gt; and &amp;lt; 20, together), we could say that rows 27, 30 and 32 as the potential outliers. If we separate the data by ESR &amp;gt;20 and ESR &amp;lt; 20, there may be other outliers for each group. I think we could make box plots by group to determine the outliers more effectively.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;(Multiple Regression) Continuing from the lecture on the  data
from  library;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Fit a quadratic regression model, i.e.,a model of the form
&lt;span class=&#34;math display&#34;&gt;\[\text{Model 2:   } velocity = \beta_1 \times distance + \beta_2 \times distance^2 +\epsilon\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamair)
data(hubble)

# A. Fit a quadratic regression model
model2 &amp;lt;- lm(y~x + I(x^2) -1, data = hubble)
# summary(model2)
kable(summary(model2)$coefficients, caption = &amp;quot;Summary of the model2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Summary of the model2&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.9046424&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.5725817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.4852433&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000164&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;I(x^2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8837375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9925378&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8903817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3828949&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;b) Plot the fitted curve from Model 2 on the scatterplot of the data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitted curve
# index  &amp;lt;- seq(0, 22, 0.1)
index &amp;lt;- seq(min(hubble$x),max(hubble$x),0.1)
index2 &amp;lt;- index^2
# predicted &amp;lt;- predict(model2,list(x = index, x2=index2))
predicted &amp;lt;- model2$fitted.values
#create a data frame of x nd y values for plotting for ggplot
data &amp;lt;- as.data.frame(cbind(x = hubble$x,predicted))
# Scatter Plot
plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(y~x, data = hubble, main = &amp;quot;base R: Scatter plot with fitted curve from Model2&amp;quot;, xlab = &amp;quot;Distance&amp;quot;, ylab = &amp;quot;Velocity&amp;quot;)
lines(data$x[order(data$x)], data$predicted[order(data$predicted)], col = &amp;quot;green&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = model2, aes(x = model2$model$x, y = model2$model$y)) +
  geom_point() +
  geom_line(aes(x = model2$model$x, y = model2$fitted.values), colour = &amp;quot;green&amp;quot;) +
  labs(title = &amp;quot;ggplot: Scatter plot with fitted curve from Model2&amp;quot;, x = &amp;quot;Distance&amp;quot;, y = &amp;quot;velocity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;c) Add the simple linear regression fit (fitted in class) on this plot - use
different color and line type to differentiate the two and add a legend to
your plot.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Simple lm fitted in class
hmod &amp;lt;- lm(y~x - 1 , data = hubble)
plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(y~x, data = hubble, main = &amp;quot;base R: scatter plot for hubble data&amp;quot;, xlab = &amp;quot;Distance&amp;quot;, ylab = &amp;quot;Velocity&amp;quot;)
lines(data$x[order(data$x)], data$predicted[order(data$predicted)], col = &amp;quot;green&amp;quot;)
abline(hmod, lty=2, col=2)

# Legend
legend(&amp;quot;bottomright&amp;quot;, c(&amp;quot;Quadratic&amp;quot;, &amp;quot;Linear&amp;quot;), lty = 1:2, col = 2:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## ggplot version

ggplot(data = model2, aes(x = model2$model$x, y = model2$model$y)) +
  geom_point() +
  geom_line(aes(x = model2$model$x, y = model2$fitted.values, colour = &amp;quot;Quadratic&amp;quot;)) +
  geom_line(data = hmod, aes(x = hmod$model$x, y = hmod$fitted.values, colour = &amp;quot;Linear&amp;quot;)) +
  labs(title = &amp;quot;ggplot: Scatter plot with fitted curve from Model2&amp;quot;, x = &amp;quot;Distance&amp;quot;, y = &amp;quot;velocity&amp;quot;, colour = &amp;quot;Models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;d) Which model do you consider most sensible considering the nature of the data - looking at the plot? 

Answer: The simple model seems more sensible to me.  The data points seem to
follow a line from lower left to upper right of the plot without a clear
curvature. However, strictly saying, there isn&amp;#39;t much difference between the two models.

e) Which model is better? - provide a statistic to support you claim.

Note: The quadratic model here is still regarded as a ``linear regression&amp;quot;
model since the term ``linear&amp;quot; relates to the parameters of the model and
not to the powers of the explanatory variables.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model2) # # Quadratic regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + I(x^2) - 1, data = hubble)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -713.15 -152.76  -54.85  163.92  557.01 
## 
## Coefficients:
##        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## x       90.9046    16.5726   5.485 1.64e-05 ***
## I(x^2)  -0.8837     0.9925  -0.890    0.383    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 260.1 on 22 degrees of freedom
## Multiple R-squared:  0.944,  Adjusted R-squared:  0.9389 
## F-statistic: 185.3 on 2 and 22 DF,  p-value: 1.715e-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- summary(model2)
summary(hmod)  # Simple linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x - 1, data = hubble)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -736.5 -132.5  -19.0  172.2  558.0 
## 
## Coefficients:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## x   76.581      3.965   19.32 1.03e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 258.9 on 23 degrees of freedom
## Multiple R-squared:  0.9419, Adjusted R-squared:  0.9394 
## F-statistic: 373.1 on 1 and 23 DF,  p-value: 1.032e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hmod.1 &amp;lt;- summary(hmod) 
cat (&amp;quot;Adjusted R-square&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted R-square&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(cbind(Quadratic = mod.2$adj.r.squared, Linear = hmod.1$adj.r.squared), caption = &amp;quot;Adjusted R-square&amp;quot;, row.names = FALSE )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 2: &lt;/span&gt;Adjusted R-square&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Quadratic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Linear&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.9388554&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9394063&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;Answer to 2e. The statistics appear to support the simple model as the
better one. Since the Adjusted r-squared statistic is higher for the simple model
(0.9394) Vs. Quadratic (0.9388554) which indicates that the simple model explains more of the variability in the response data than does the quadratic model.&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The  data from package  shows the survival times
from diagnosis of patients suffering from leukemia and the values of two
explanatory variables, the white blood cell count (wbc) and the presence or
absence of a morphological characteristic of the white blood cells (ag).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Define a binary outcome variable according to whether or not patients
lived for at least 24 weeks after diagnosis. Call it .&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add a binary column named surv24 for time greater than or less than 24. 
library(MASS)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q3_subset &amp;lt;- leuk %&amp;gt;%
  mutate(surv24 = ifelse(time &amp;gt;= 24, 1,0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b) Fit a logistic regression model to the data with \textit{surv24} as
response. It is advisable to transform the very large white blood counts to
avoid regression coefficients very close to 0 (and odds ratio close to 1).
You may use log transformation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surv24.model &amp;lt;- glm(surv24 ~ log(wbc) + ag, data=q3_subset,family = &amp;#39;binomial&amp;#39;)
kable(summary(surv24.model)$coefficient, caption = &amp;quot;Summary coefficients of the glm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 3: &lt;/span&gt;Summary coefficients of the glm&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;z value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|z|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.4555870&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.9821469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.158758&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2465548&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;log(wbc)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4821891&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3149136&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.531179&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1257252&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;agpresent&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.7621259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8093190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.177295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0294586&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;c) Construct some graphics useful in the interpretation of the final model you fit. &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Create a scatter plot of the data fitting the two curves of test results to the fitted output of the model prediciton&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Create a scatter plot of the data fitting the two curves of test results to the fitted output of the model prediciton&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.extension &amp;lt;- seq(0, max(log(q3_subset$wbc)+4.5), by = 0.5)
espframe &amp;lt;- data.frame(&amp;quot;x.extension&amp;quot; = x.extension, &amp;quot;agpress&amp;quot; = (exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3]))), &amp;quot;agabs&amp;quot; = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension)/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension)))


plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(x = log(leuk$wbc), y = surv24.model$fitted.values, col = leuk$ag, xlim = c(0,15), ylim = c(0,1), ylab = &amp;quot;Survive (Time, surv24wks)&amp;quot;, xlab = &amp;quot;log (wbc counts)&amp;quot;, main = &amp;quot;base R: plot of logistic model of Leuk data&amp;quot;)
lines(x = x.extension, y = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension)/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension)))
lines(x = x.extension, y = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])))
legend(&amp;quot;bottomleft&amp;quot;, legend = c(&amp;quot;Ag Absent&amp;quot;, &amp;quot;Ag Present&amp;quot;), col = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;), lty = c(1,1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leuk.gg &amp;lt;- data.frame(&amp;quot;logwbc&amp;quot; = log(leuk$wbc), surv24 = q3_subset$surv24, &amp;quot;fv&amp;quot; = surv24.model$fitted.values, &amp;quot;ag&amp;quot; = leuk$ag)

leuk.gg &amp;lt;- cbind(leuk.gg, espframe)
ggplot(leuk.gg, aes(x = logwbc, y = fv, colour = ag)) + 
  geom_point() +
  # scale_colour_discrete(guide = FALSE) +
  # guides(colour=FALSE) +
  geom_line(aes(x = x.extension, y = agpress, colour = &amp;quot;present&amp;quot;)) +
  geom_line(aes(x = x.extension, y = agabs, colour = &amp;quot;absent&amp;quot;)) +
  labs ( title = &amp;quot;ggplot: plot of logistic model of Leuk data&amp;quot;, x = &amp;quot;log of WBC count&amp;quot;, y = &amp;quot;Survive (Time, surv24wks)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Survival Vs WBC count with logistic model on actual data points&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Survival Vs WBC count with logistic model on actual data points&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # base plot version
line.1.dat &amp;lt;- leuk.gg[leuk.gg$ag == &amp;#39;absent&amp;#39;, ]
line.2.dat &amp;lt;- leuk.gg[leuk.gg$ag == &amp;#39;present&amp;#39;, ]
plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(
  x = leuk.gg$logwbc,
  y = leuk.gg$surv24,
  xlim=c(0,15),
  ylim = c(0,1),
  col = leuk.gg$ag,
  xlab = &amp;quot;WBC counts&amp;quot;,
  ylab = &amp;quot;Probability of Death prior to 24 Weeks&amp;quot;,
  main = &amp;quot;base R: Survival Vs WBC Counts in Leukaemia Patients&amp;quot;
)
lines(x.extension, leuk.gg$agpress, col = &amp;quot;green&amp;quot;)
lines(x.extension, leuk.gg$agabs, col = &amp;quot;black&amp;quot;)
legend(
  &amp;quot;topleft&amp;quot;,
  title = &amp;quot;AG test&amp;quot;,
  legend = c(&amp;quot;absent&amp;quot;, &amp;quot;present&amp;quot;),
  inset = c(1, 0),
  xpd = TRUE,
  horiz = FALSE,
  col = c(&amp;quot;black&amp;quot;, &amp;quot;green&amp;quot;),
  lty = c(1,1),
  pch = c(1, 2),
  bty = &amp;quot;n&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(leuk.gg, aes(x = logwbc, y = surv24, color = ag)) +
  geom_point() +
  scale_colour_manual(name = &amp;quot;AG test&amp;quot;, values = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;)) +
  geom_line(aes(x = x.extension, y = agpress, colour = &amp;quot;present&amp;quot;)) +
  geom_line(aes(x = x.extension, y = agabs, colour = &amp;quot;absent&amp;quot;)) +
  labs(title = &amp;#39;ggplot: Survival Vs WBC Counts in Leukaemia Patients&amp;#39;,
       x = &amp;#39;log WBC Count&amp;#39;,
       y = &amp;#39;Probability of Death prior to 24 Weeks&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;d) Fit a model with an interaction term between the two predictors. Which
model fits the data better? Justify your answer.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#fitting the model with the interaction term ag * log(wbc)
surv24.model2 &amp;lt;- lm(surv24 ~ ag * log(wbc), data=q3_subset,family=&amp;#39;binomial&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
##  extra argument &amp;#39;family&amp;#39; will be disregarded&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(summary(surv24.model2)$coefficients, caption = &amp;quot;Summary of the linear model with an interaction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 4: &lt;/span&gt;Summary of the linear model with an interaction&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0258017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8719219&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0295918&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9765953&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;agpresent&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.4360783&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1398202&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.1372479&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0411387&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;log(wbc)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0286636&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0898818&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3189031&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7520857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;agpresent:log(wbc)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2156187&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1183543&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.8218074&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0788139&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 = summary(surv24.model2)
mod = summary(surv24.model)
# we can also calculate adjusted r-square value for glm using 
library(rsq)
mod.rsq.adj = rsq(surv24.model,adj=TRUE,type=c(&amp;#39;v&amp;#39;,&amp;#39;kl&amp;#39;,&amp;#39;sse&amp;#39;,&amp;#39;lr&amp;#39;,&amp;#39;n&amp;#39;),data=NULL)
mod2.rsq.adj = rsq(surv24.model2,adj=TRUE,type=c(&amp;#39;v&amp;#39;,&amp;#39;kl&amp;#39;,&amp;#39;sse&amp;#39;,&amp;#39;lr&amp;#39;,&amp;#39;n&amp;#39;),data=NULL)
# if not using package rsq
# adj.rsq = rbind(mod2$adj.r.squared, (1 -(mod$deviance/mod$null.deviance)) * 32/(32-2-2))
adj.rsq = rbind(mod2.rsq.adj, mod.rsq.adj)

row.names(adj.rsq) &amp;lt;- c(&amp;quot;Linear model with interation&amp;quot;, &amp;quot;Linear model&amp;quot;)
kable(adj.rsq, col.names = &amp;quot;Adjusted R-square values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Adjusted R-square values&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Linear model with interation&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2308546&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Linear model&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1890705&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Since the adjusted R-square value for Linear model with the interacion is higher, I would say the model with an interaction fits the data better.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load the  dataset from  library. The dataset
contains information on ten thousand customers. The aim here is to predict which
customers will default on their credit card debt. It is a four-dimensional
dataset with 10000 observations. The question of interest is to predict
individuals who will default . We want to examine how each predictor variable is
related to the response (default). Do the following on this dataset&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Perform descriptive analysis on the dataset to have an insight. Use
summaries and appropriate exploratory graphics to answer the question of
interest.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use R to build a logistic regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discuss your result. Which predictor variables were important? Are there
interactions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How good is your model? Assess the performance of the logistic regression
classifier. What is the error rate?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up data
data(&amp;quot;Default&amp;quot;, package = &amp;quot;ISLR&amp;quot;)

kable(summary(Default[,1:2]), caption = &amp;quot;Summary of default and student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-10&#34;&gt;Table 5: &lt;/span&gt;Summary of default and student status&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;default&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;student&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;No :9667&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;No :7056&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yes: 333&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yes:2944&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(summary(Default[,3:4]), caption = &amp;quot;Summary of Income and Balance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-10&#34;&gt;Table 5: &lt;/span&gt;Summary of Income and Balance&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;balance&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;income&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Min. : 0.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Min. : 772&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1st Qu.: 481.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1st Qu.:21340&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Median : 823.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Median :34553&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mean : 835.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mean :33517&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3rd Qu.:1166.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3rd Qu.:43808&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Max. :2654.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Max. :73554&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create default binary
default_binary     &amp;lt;-
  ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$default) == -1, 0, 1)
dflt_str &amp;lt;-
  ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$default) == -1,
         &amp;quot;Not Defaulted&amp;quot;,
         &amp;quot;Defaulted&amp;quot;)

stdn     &amp;lt;- ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$student) == -1, 0, 1)
stdn_str &amp;lt;-
  ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$student) == -1, &amp;quot;Not-Student&amp;quot;, &amp;quot;Student&amp;quot;)

blnc &amp;lt;- Default$balance
incm &amp;lt;- Default$income

df &amp;lt;-  data.frame(default_binary, dflt_str, stdn, stdn_str, blnc, incm)

# par(mfrow = c(1, 1))

cat(&amp;quot;Balance appears roughly normal&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance appears roughly normal&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(blnc, main = &amp;quot;Balance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot() + geom_histogram(aes(blnc), bins = 13, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;)


cat(&amp;quot;Income appears roughly normal with two means&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Income appears roughly normal with two means&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(incm, main = &amp;quot;Income&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Dual means in income appears explained by student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Dual means in income appears explained by student status&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:2, ncol = 2))
hist(
  subset(df$incm, df$stdn == 1),
  main = &amp;quot;Income by Student Status&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Student: Yes&amp;quot;
)
hist(
  subset(df$incm, df$stdn == 0),
  main = &amp;quot;&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Student: No&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;**And** the dual means in income appears NOT to be explained by default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## **And** the dual means in income appears NOT to be explained by default status&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:2, ncol = 2))
hist(
  subset(df$incm, df$default_binary == 1),
  main = &amp;quot;Income by Default Status&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Default: Yes&amp;quot;
)
hist(
  subset(df$incm, df$default_binary == 0),
  main = &amp;quot;&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Default: No&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Clustering of income v. balance explained by student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Clustering of income v. balance explained by student status&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(
  Default$income ~ Default$balance,
  col = Default$student,
  main = &amp;quot;base R: Income by Balance&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Balance&amp;quot;,
  pch = 18
)
legend(
  &amp;quot;topright&amp;quot;,
  c(&amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;),
  title = &amp;quot;Student?&amp;quot;,
  # bty = &amp;quot;n&amp;quot;,
  fill = c(&amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;),
  pch = c(18,18)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = balance, y = income, colour = student)) + 
  geom_point() +
  labs(title = &amp;quot;ggplot: Income by Balance&amp;quot;) + 
  guides(colour=guide_legend(title=&amp;quot;Student?&amp;quot;)) +
  scale_color_manual(values = c(&amp;quot;No&amp;quot; = &amp;quot;black&amp;quot;, &amp;quot;Yes&amp;quot; = &amp;quot;red&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(balance~student, data = Default, main = &amp;quot;base R: Balance grouped by Student status&amp;quot;, xlab = &amp;quot;student&amp;quot;, ylab = &amp;quot;balance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = student, y = balance)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Balance grouped by Student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(balance~default, data = Default, main = &amp;quot;base R: Balance grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = default, y = balance)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Balance grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(income~student, data = Default, main = &amp;quot;base R: Income grouped by Student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-12.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = student, y = income)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Income grouped by Student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-13.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(income~default, data = Default, main = &amp;quot;base R: Income grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-14.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = default, y = income)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Income grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-15.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Median and Max income are lower for defaulted than not defaulted loans&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Median and Max income are lower for defaulted than not defaulted loans&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(df$incm, df$dflt_str, FUN = summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Defaulted
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    9664   19028   31515   32089   43067   66466 
## 
## $`Not Defaulted`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     772   21405   34589   33566   43824   73554&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Median and max balance are higher for defaulted rather than not defaulted loans&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Median and max balance are higher for defaulted rather than not defaulted loans&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(df$blnc, df$dflt_str, FUN = summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Defaulted
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   652.4  1511.6  1789.1  1747.8  1988.9  2654.3 
## 
## $`Not Defaulted`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     0.0   465.7   802.9   803.9  1128.2  2391.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#B. Use R to build a logistic regression model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #B. Use R to build a logistic regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # https://stackoverflow.com/questions/13366755/what-does-the-r-formula-y1-mean
regression_model0 &amp;lt;- glm(default_binary ~ stdn + blnc + incm, family = binomial())
summary(regression_model0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default_binary ~ stdn + blnc + incm, family = binomial())
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4691  -0.1418  -0.0557  -0.0203   3.7383  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.087e+01  4.923e-01 -22.080  &amp;lt; 2e-16 ***
## stdn        -6.468e-01  2.363e-01  -2.738  0.00619 ** 
## blnc         5.737e-03  2.319e-04  24.738  &amp;lt; 2e-16 ***
## incm         3.033e-06  8.203e-06   0.370  0.71152    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.5  on 9996  degrees of freedom
## AIC: 1579.5
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # we could also do to select all predictors in the data
# mod &amp;lt;- glm(default~., data = Default, family = binomial)
# summary(mod)

cat(&amp;quot;Then with interactions:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Then with interactions:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regression_model1 &amp;lt;- glm(default_binary ~ stdn + blnc + incm + stdn * blnc + stdn * incm + blnc * incm, family = binomial())
summary(regression_model1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default_binary ~ stdn + blnc + incm + stdn * blnc + 
##     stdn * incm + blnc * incm, family = binomial())
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4848  -0.1417  -0.0554  -0.0202   3.7579  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.104e+01  1.866e+00  -5.914 3.33e-09 ***
## stdn        -5.201e-01  1.344e+00  -0.387    0.699    
## blnc         5.882e-03  1.180e-03   4.983 6.27e-07 ***
## incm         4.050e-06  4.459e-05   0.091    0.928    
## stdn:blnc   -2.551e-04  7.905e-04  -0.323    0.747    
## stdn:incm    1.447e-05  2.779e-05   0.521    0.602    
## blnc:incm   -1.579e-09  2.815e-08  -0.056    0.955    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.1  on 9993  degrees of freedom
## AIC: 1585.1
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# D. Error Rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # D. Error Rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dflt.fitted0 &amp;lt;- predict(regression_model0, type = &amp;quot;response&amp;quot;)
dflt.fitted1 &amp;lt;- predict(regression_model1, type = &amp;quot;response&amp;quot;)

levs &amp;lt;- c(&amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;)
Tr &amp;lt;- default_binary

Predicted0 &amp;lt;-
  factor(ifelse(dflt.fitted0 &amp;gt;= 0.50, &amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;),
         levels = levs)
Predicted1 &amp;lt;-
  factor(ifelse(dflt.fitted1 &amp;gt;= 0.50, &amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;),
         levels = levs)
Tr1 &amp;lt;-
  factor(ifelse(Tr &amp;gt;= 0.50, &amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;), levels = levs)
rate0 &amp;lt;- table(Predicted0, True = Tr1)
rate1 &amp;lt;- table(Predicted1, True = Tr1)
rate0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                True
## Predicted0      Defaulted Not Defaulted
##   Defaulted           105            40
##   Not Defaulted       228          9627&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_rate0 &amp;lt;- 1 - (rate0[1, 1] + rate0[2, 2]) / sum(rate0)
error_rate0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0268&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                True
## Predicted1      Defaulted Not Defaulted
##   Defaulted           104            40
##   Not Defaulted       229          9627&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_rate1 &amp;lt;- 1 - (rate1[1, 1] + rate1[2, 2]) / sum(rate1)
error_rate1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0269&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;analysis of variance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## analysis of variance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(regression_model0, regression_model1, test = &amp;#39;Chisq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: default_binary ~ stdn + blnc + incm
## Model 2: default_binary ~ stdn + blnc + incm + stdn * blnc + stdn * incm + 
##     blnc * incm
##   Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)
## 1      9996     1571.5                     
## 2      9993     1571.1  3  0.47911   0.9235&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;4a. Based on the outputs for 4a. we can tell that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Fewer people default than don’t default.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Defaulters and non-defaulters appear to have the same income range, given student status.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Defaulters appear to have higher balances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If students default, they likely do it with over $1,000 balance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If non-students default, they are likely do it with over $500 balance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4c.
Without taking interactions into account, it appears that two predictors-
student and balance are significant. With interactions involved, it appears that
only balance predictor is important.&lt;/p&gt;
&lt;p&gt;4d.
The model without interactions has an AICof 1579.5 and the interaction model has
an AIC of 1585.1 (slightly higher). But, both have almost similar error rates
~2.7 %. Also, since analysis of deviance also shows that the chi-square test has
no significance at 5% level, we can conclude that both models are almost the
same as a working model.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go through Section 7.3.1 of the Handbook. Run all the codes (additional exploration of data is allowed) and write your own version of explanation and interpretation.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# density plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # density plot&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plasma &amp;lt;- plasma

layout(matrix(1:2,ncol=2))
cdplot(ESR ~ fibrinogen, data=plasma)
cdplot(ESR ~ globulin,data=plasma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that above a certain level of fibrogen, ESR drops sucessively. This is not the case for globulin.&lt;/p&gt;
&lt;p&gt;ESR Logistic Regression an Confidence Interval Estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plasma_glm_1 &amp;lt;- glm(ESR ~ fibrinogen, data = plasma, family=binomial())
confint(plasma_glm_1,parm=&amp;#39;fibrinogen&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5 %    97.5 % 
## 0.3387619 3.9984921&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, fibrinogen might have value as a predictor of ESR. We can look at the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(plasma_glm_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = ESR ~ fibrinogen, family = binomial(), data = plasma)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9298  -0.5399  -0.4382  -0.3356   2.4794  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)  -6.8451     2.7703  -2.471   0.0135 *
## fibrinogen    1.8271     0.9009   2.028   0.0425 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 30.885  on 31  degrees of freedom
## Residual deviance: 24.840  on 30  degrees of freedom
## AIC: 28.84
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary output indicates a 5% significance of fibrinogenand and increase of the log-odds of ESR &amp;gt; 20 by about 1.83 with confidence interval (CI) of 0.33 to 3.99.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(coef(plasma_glm_1)[&amp;#39;fibrinogen&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## fibrinogen 
##   6.215715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fibrinogen might have value as a predictor of ESR.
To make the results more readable, it is useful to apply an exponent function. This exponenetiates the log-odds of fibriogen and CI to correspond with the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(confint(plasma_glm_1, parm=&amp;#39;fibrinogen&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5 %    97.5 % 
##  1.403209 54.515884&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also perform logistic regression of both explanatory variables (fibrinogen and globulin) and text for the deviance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plasma_glm_2 &amp;lt;- glm(ESR ~ fibrinogen + globulin, data = plasma, family = binomial())
summary(plasma_glm_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = ESR ~ fibrinogen + globulin, family = binomial(), 
##     data = plasma)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9683  -0.6122  -0.3458  -0.2116   2.2636  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept) -12.7921     5.7963  -2.207   0.0273 *
## fibrinogen    1.9104     0.9710   1.967   0.0491 *
## globulin      0.1558     0.1195   1.303   0.1925  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 30.885  on 31  degrees of freedom
## Residual deviance: 22.971  on 29  degrees of freedom
## AIC: 28.971
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# comparison of models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # comparison of models&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(plasma_glm_1, plasma_glm_2, test= &amp;#39;Chisq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: ESR ~ fibrinogen
## Model 2: ESR ~ fibrinogen + globulin
##   Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)
## 1        30     24.840                     
## 2        29     22.971  1   1.8692   0.1716&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can make the bubble plot of the predicted values of model II (plasma_glm_2). The plot shows that the probablity of ‘good’ ESR reading increases as fibrinogen increases. This is true of globulin only up to a point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob &amp;lt;- predict(plasma_glm_2, type=&amp;#39;response&amp;#39;)

plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(globulin ~ fibrinogen,data=plasma,xlim=c(2,6),ylim=c(25,55),pch=&amp;#39;.&amp;#39;, main = &amp;quot;Bubble plot of the predicted values of model II&amp;quot;)
symbols(plasma$fibrinogen,plasma$globulin,circles=prob,add=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graphical Summary</title>
      <link>/achalneupane.github.io/post/graphical_summary/</link>
      <pubDate>Tue, 10 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/graphical_summary/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content titled Format+STAT-701+HW. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.1, pg. 23 in Handbook &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, for this question, we assume that we have data available for the given countries and we will remove any NAs from the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;HSAUR3&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#subset the data for wanted contries only first
median.subset &amp;lt;- subset(Forbes2000,country %in% c(&amp;quot;United States&amp;quot;,&amp;quot;United Kingdom&amp;quot;,&amp;quot;France&amp;quot;,&amp;quot;Germany&amp;quot;))
# library(&amp;quot;plyr&amp;quot;)
# ddply(median.subset,c(&amp;quot;country&amp;quot;),function(x){median(x$profits,na.rm=T)})
by.country &amp;lt;- setNames(aggregate(median.subset$profits, by = list(median.subset$country), function(x){median(x,na.rm=T)}), c(&amp;quot;country&amp;quot;, &amp;quot;median&amp;quot;))
by.country&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          country median
## 1         France  0.190
## 2        Germany  0.230
## 3 United Kingdom  0.205
## 4  United States  0.240&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.2, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Forbes2000[Forbes2000$country == &amp;quot;Germany&amp;quot; &amp;amp; Forbes2000$profits &amp;lt; 0,&amp;quot;name&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Allianz Worldwide&amp;quot;       &amp;quot;Deutsche Telekom&amp;quot;       
##  [3] &amp;quot;E.ON&amp;quot;                    &amp;quot;HVB-HypoVereinsbank&amp;quot;    
##  [5] &amp;quot;Commerzbank&amp;quot;             &amp;quot;Infineon Technologies&amp;quot;  
##  [7] &amp;quot;BHW Holding&amp;quot;             &amp;quot;Bankgesellschaft Berlin&amp;quot;
##  [9] &amp;quot;W&amp;amp;W-Wustenrot&amp;quot;           &amp;quot;mg technologies&amp;quot;        
## [11] &amp;quot;Nurnberger Beteiligungs&amp;quot; &amp;quot;SPAR Handels&amp;quot;           
## [13] &amp;quot;Mobilcom&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.3, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(table(Forbes2000[Forbes2000$country==&amp;quot;Bermuda&amp;quot;,&amp;quot;category&amp;quot;]),decreasing=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##                        Insurance                    Conglomerates 
##                               10                                2 
##             Oil &amp;amp; gas operations                          Banking 
##                                2                                1 
##                    Capital goods             Food drink &amp;amp; tobacco 
##                                1                                1 
##                     Food markets                            Media 
##                                1                                1 
##              Software &amp;amp; services              Aerospace &amp;amp; defense 
##                                1                                0 
##     Business services &amp;amp; supplies                        Chemicals 
##                                0                                0 
##                     Construction                Consumer durables 
##                                0                                0 
##           Diversified financials            Drugs &amp;amp; biotechnology 
##                                0                                0 
## Health care equipment &amp;amp; services     Hotels restaurants &amp;amp; leisure 
##                                0                                0 
##    Household &amp;amp; personal products                        Materials 
##                                0                                0 
##                        Retailing                   Semiconductors 
##                                0                                0 
##  Technology hardware &amp;amp; equipment      Telecommunications services 
##                                0                                0 
##                Trading companies                   Transportation 
##                                0                                0 
##                        Utilities 
##                                0&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.4, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Forbes2000.data &amp;lt;- Forbes2000[order(Forbes2000$profits,decreasing=T),]
par(mfrow = c(1,1))
# Using base plot
plot(
    log(Forbes2000.data[1:50, &amp;quot;assets&amp;quot;]),
    log(Forbes2000.data[1:50, &amp;quot;sales&amp;quot;])
    ,
    ylab = &amp;quot;sales (log scale)&amp;quot;,
    xlab = &amp;quot;assets (log scale)&amp;quot;,
    main = &amp;quot;Sales vs Assets: Log transformed&amp;quot;
)
# we can add texts to data points
text(
    x = log(Forbes2000.data[1:50, &amp;quot;assets&amp;quot;]),
    y = log(Forbes2000.data[1:50, &amp;quot;sales&amp;quot;]),
    labels = abbreviate(Forbes2000.data$country[1:50]),
    col = &amp;quot;black&amp;quot;,
    font = 1,
    pos = 1
)

#Using ggplot
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Forbes2000.data[1:50, ], aes(x = log(assets), y = log(sales))) +
    geom_point(shape = 1, size = 2) +
    geom_text(aes(label = abbreviate(country)), hjust = 0, vjust = 0) +
    labs(title = &amp;quot;Sales vs Assets: Log transformed&amp;quot;, color = &amp;quot;country&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.5, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
# selecting companies with profit more than 5 billion
Forbes2000.5billions.dat &amp;lt;- Forbes2000[Forbes2000$profits &amp;gt; 5 ,]
Forbes2000.5billions.dat &amp;lt;- Forbes2000.5billions.dat[complete.cases(Forbes2000.5billions.dat), ]

setNames(ddply(Forbes2000.5billions.dat,c(&amp;quot;country&amp;quot;),function(x){c(nrow(x),mean(x$sales))}), c(&amp;quot;Country&amp;quot;, &amp;quot;Company_counts&amp;quot;, &amp;quot;Avg_Sales&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       Country Company_counts Avg_Sales
## 1                       China              1   29.5300
## 2                      France              1  131.6400
## 3                     Germany              1  157.1300
## 4                       Japan              1  135.8200
## 5 Netherlands/ United Kingdom              1  133.5000
## 6                 South Korea              1   50.2200
## 7                 Switzerland              3   46.7600
## 8              United Kingdom              3  103.6867
## 9               United States             20   77.2835&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# #or we could also do:
# aggregate(Forbes2000.5billions.dat$sales, by = list(Forbes2000.5billions.dat$country), function(x){c(Company_counts = length(x), Avg_Sales = mean(x))})&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 2.1, pg. 39 in Handbook (see Chapter 6 of R Graphcis Cookbook for GGPlot)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this, we will first calculate total household expenses as below. Then calculate the itemized expenses (proportion).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;household&amp;quot;, package = &amp;quot;HSAUR3&amp;quot;)

# total expenses
household$total&amp;lt;-household$housing+household$food+household$goods+household$service
library(ggplot2)

ggplot(household, aes(x=gender, y=total))+geom_boxplot() +
  labs(title = &amp;quot;Total expenses per gender&amp;quot;) + 
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# base R version
boxplot(household$total ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;total&amp;quot;, main = &amp;quot;Total household expenses by gender&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This indicates that males tend to have more expenses than females in total. 
#calculate proportions
household$housing&amp;lt;-household$housing/household$total
household$service&amp;lt;-household$service/household$total
household$goods&amp;lt;-household$goods/household$total
household$food&amp;lt;-household$food/household$total

#plot boxplots of proportions
a&amp;lt;-ggplot(household, aes(x=gender, y=housing))+geom_boxplot() + labs(title = &amp;quot;Housing expenses (in %) per gender&amp;quot;) 
b&amp;lt;-ggplot(household, aes(x=gender, y=service))+geom_boxplot() + labs(title = &amp;quot;Service expenses (in %) per gender&amp;quot;)
c&amp;lt;-ggplot(household, aes(x=gender, y=goods))+geom_boxplot() + labs(title = &amp;quot;Goods expenses (in %) per gender&amp;quot;)
d&amp;lt;-ggplot(household, aes(x=gender, y=food))+geom_boxplot() + labs(title = &amp;quot;Food expenses (in %) per gender&amp;quot;)

# combine all 4 gg objects
library(gridExtra)
grid.arrange(a,b,c,d, nrow=2, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# base R version
par(mfrow=c(2,2))
boxplot(household$housing ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;Housing expenses&amp;quot;, main = &amp;quot;Housing expenses (in %) by gender&amp;quot;)
boxplot(household$service ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;Service expenses&amp;quot;, main = &amp;quot;Service expenses (in %) by gender&amp;quot;)
boxplot(household$goods ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;Goods expenses&amp;quot;, main = &amp;quot;Goods expenses (in %) by gender&amp;quot;)
boxplot(household$food ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;food expenses&amp;quot;, main = &amp;quot;Food expenses (in %) by gender&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# histogram of total expenditures by gender
ggplot(household, aes(x=total, fill=gender)) +
  geom_histogram(position=&amp;quot;identity&amp;quot;, alpha=0.4, binwidth = 1000) + 
  labs(title = &amp;quot;Histogram of total expenditures by gender&amp;quot; ) + 
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on these results, males tend to spend more money than females on Foods
and Services, where as females take the lead on Housing and Goods.&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 2.3, pg. 41 in Handbook (see Chapter 6 of R Graphcis Cookbook for GGPlot)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;suicides2&amp;quot;)
#Boxplot of mortality
boxplot(suicides2,
        xlab = &amp;quot;Age group&amp;quot;, ylab = &amp;quot;Mortality rates per 100,000&amp;quot;, main = &amp;quot;Mortality by suicide&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
suicides2.melted &amp;lt;- melt(suicides2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No id variables; using all as measure variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version
ggplot(suicides2.melted, aes(x=factor(variable), y=value)) + 
  geom_boxplot() +
  labs(x = &amp;quot;Age group&amp;quot;, y = &amp;quot;Mortality rates per 100,000&amp;quot;, title = &amp;quot;Mortality by suicide&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using a single R expression, calculate the median absolute deviation, &lt;span class=&#34;math inline&#34;&gt;\(1.4826\cdot median|x-\mu|\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the sample median. Use the dataset . Use the R function mad() to verify your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(abs(chickwts$weight - median(chickwts$weight))) * 1.4826&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.9212&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#check using mad
mad(chickwts$weight, median (chickwts$weight), constant = 1.4826)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.9212&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using the data matrix , find the state with the minimum per capita income in the New England region as defined by the factor . Use the vector  to get the state name.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(state.x77)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in data(state.x77): data set &amp;#39;state.x77&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create df with state name, income, and division
state_df &amp;lt;- data.frame(income=state.x77[,&amp;quot;Income&amp;quot;],
                    name=state.name,
                    div=state.division)
#Subset for New England
New_Eng &amp;lt;- subset(state_df, div ==&amp;quot;New England&amp;quot;)

#State with min income per capita
New_Eng[which(New_Eng$income == min(New_Eng$income)), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       income  name         div
## Maine   3694 Maine New England&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;11&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use subscripting operations on the dataset  to find the vehicles with highway mileage of less than 25 miles per gallon (variable ) and weight (variable ) over 3500lbs. Print the model name, the price range (low, high), highway mileage, and the weight of the cars that satisfy these conditions.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Cars93, package= &amp;quot;MASS&amp;quot;)
df.cars93 &amp;lt;- Cars93[Cars93$MPG.highway &amp;lt; 25 &amp;amp; Cars93$Weight &amp;gt; 3500, c(&amp;quot;Model&amp;quot;, &amp;quot;Price&amp;quot;, &amp;quot;MPG.highway&amp;quot;, &amp;quot;Weight&amp;quot;)]
df.cars93[with(df.cars93, order(Price)), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Model Price MPG.highway Weight
## 16 Lumina_APV  16.3          23   3715
## 17      Astro  16.6          20   4025
## 26    Caravan  19.0          21   3705
## 56        MPV  19.1          24   3735
## 66      Quest  19.1          23   4100
## 70 Silhouette  19.5          23   3715
## 89    Eurovan  19.7          21   3960
## 36   Aerostar  19.9          20   3735
## 87     Previa  22.7          22   3785
## 28    Stealth  25.8          24   3805
## 63   Diamante  26.1          24   3730
## 49      ES300  28.0          24   3510
## 50      SC300  35.2          23   3515
## 48        Q45  47.9          22   4000&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;12&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Form a matrix object named  from the variables  from the  dataframe from the  package. Use it to create a list object named  containing named components as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;A vector of means, named &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A vector of standard errors of the means, named &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Cars93, package= &amp;quot;MASS&amp;quot;)
mycars &amp;lt;- Cars93[,c(&amp;quot;Min.Price&amp;quot;, &amp;quot;Max.Price&amp;quot;, &amp;quot;MPG.city&amp;quot;, &amp;quot;MPG.highway&amp;quot;, &amp;quot;EngineSize&amp;quot;, &amp;quot;Length&amp;quot;, &amp;quot;Weight&amp;quot;)]
Cars.Means &amp;lt;- sapply(mycars, mean)       
Cars.Means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Min.Price   Max.Price    MPG.city MPG.highway  EngineSize      Length 
##   17.125806   21.898925   22.365591   29.086022    2.667742  183.204301 
##      Weight 
## 3072.903226&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to calculate standard error &amp;#39;standard.err&amp;#39;
standard.err &amp;lt;- function(x) sqrt(var(x)/length(x))
Cars.Std.Errors &amp;lt;- lapply(mycars, standard.err)  
Cars.Std.Errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Min.Price
## [1] 0.906921
## 
## $Max.Price
## [1] 1.143805
## 
## $MPG.city
## [1] 0.5827473
## 
## $MPG.highway
## [1] 0.5528742
## 
## $EngineSize
## [1] 0.1075695
## 
## $Length
## [1] 1.514196
## 
## $Weight
## [1] 61.16942&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the  function on the three-dimensional array  to compute:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Sample means of the variables , for each of the three species &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(iris3, c(2,3), mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Setosa Versicolor Virginica
## Sepal L.  5.006      5.936     6.588
## Sepal W.  3.428      2.770     2.974
## Petal L.  1.462      4.260     5.552
## Petal W.  0.246      1.326     2.026&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b) Sample means of the variables \textit{Sepal Length, Sepal Width, Petal Width} for the entire data set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(iris3, 2, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sepal L. Sepal W. Petal L. Petal W. 
## 5.843333 3.057333 3.758000 1.199333&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;14&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the data matrix  and the  function to obtain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The mean per capita income of the states in each of the four regions defined by the factor &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mydf &amp;lt;- data.frame(state.x77, state.region = state.region, stringsAsFactors = FALSE)
tapply(mydf$Income, mydf$state.region, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Northeast         South North Central          West 
##      4570.222      4011.938      4611.083      4702.615&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b) The maximum illiteracy rates for states in each of the nine divisions defined by the factor \textit{state.division}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mydf &amp;lt;- data.frame(state.x77, state.division = state.division, stringsAsFactors = FALSE)
tapply(mydf$Illiteracy, mydf$state.division, max)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        New England    Middle Atlantic     South Atlantic East South Central 
##                1.3                1.4                2.3                2.4 
## West South Central East North Central West North Central           Mountain 
##                2.8                0.9                0.8                2.2 
##            Pacific 
##                1.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;c) The number of states in each region&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mydf &amp;lt;- data.frame(state.x77, state.region = state.region, state.name = state.name, stringsAsFactors = FALSE)
tapply(mydf$state.name, mydf$state.region, length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Northeast         South North Central          West 
##             9            16            12            13&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;15&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Using the dataframe , produce a scatter plot matrix of the variables . Use different colors to identify cars belonging to each of the categories defined by the  variable in different colors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carsize = cut(mtcars[,&amp;quot;wt&amp;quot;], breaks=c(0, 2.5, 3.5, 5.5), 
labels = c(&amp;quot;Compact&amp;quot;,&amp;quot;Midsize&amp;quot;,&amp;quot;Large&amp;quot;))
carsize = cut(mtcars[,&amp;quot;wt&amp;quot;], breaks=c(0, 2.5, 3.5,
                                  5.5), labels = c(&amp;quot;Compact&amp;quot;,&amp;quot;Midsize&amp;quot;,&amp;quot;Large&amp;quot;))

mydf &amp;lt;- data.frame(mtcars, carsize = carsize)

# Using base R
par(mfrow = c(1,1))
pairs(~mpg + disp + hp + drat + qsec, data=mydf, 
  main=&amp;quot;mtcars Scatterplot Matrix&amp;quot;)

# Using ggplot
library(ggplot2)
library(&amp;#39;GGally&amp;#39;)
ggpairs(mydf[,c(&amp;quot;mpg&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;qsec&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the function  to perform a one-way analysis of variance on the  data with  as the treatment factor. Assign the result to an object named  and use it to print an ANOVA table.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chick.aov &amp;lt;- aov( weight ~ feed, chickwts)

# summary aov
summary.aov(chick.aov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## feed         5 231129   46226   15.37 5.94e-10 ***
## Residuals   65 195556    3009                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# post-hoc test (Tukey HSD)
TukeyHSD(chick.aov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = weight ~ feed, data = chickwts)
## 
## $feed
##                            diff         lwr       upr     p adj
## horsebean-casein    -163.383333 -232.346876 -94.41979 0.0000000
## linseed-casein      -104.833333 -170.587491 -39.07918 0.0002100
## meatmeal-casein      -46.674242 -113.906207  20.55772 0.3324584
## soybean-casein       -77.154762 -140.517054 -13.79247 0.0083653
## sunflower-casein       5.333333  -60.420825  71.08749 0.9998902
## linseed-horsebean     58.550000  -10.413543 127.51354 0.1413329
## meatmeal-horsebean   116.709091   46.335105 187.08308 0.0001062
## soybean-horsebean     86.228571   19.541684 152.91546 0.0042167
## sunflower-horsebean  168.716667   99.753124 237.68021 0.0000000
## meatmeal-linseed      58.159091   -9.072873 125.39106 0.1276965
## soybean-linseed       27.678571  -35.683721  91.04086 0.7932853
## sunflower-linseed    110.166667   44.412509 175.92082 0.0000884
## soybean-meatmeal     -30.480519  -95.375109  34.41407 0.7391356
## sunflower-meatmeal    52.007576  -15.224388 119.23954 0.2206962
## sunflower-soybean     82.488095   19.125803 145.85039 0.0038845&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;17&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write an R function named  for conducting a one-sample t-test. Return a list object containing the two components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the t-statistic named T;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the two-sided p-value named P.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Annotated R Code
  library(HSAUR3)
  library(stats)
  attach(chickwts)
  head(chickwts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   weight      feed
## 1    179 horsebean
## 2    160 horsebean
## 3    136 horsebean
## 4    227 horsebean
## 5    217 horsebean
## 6    168 horsebean&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  t.test(chickwts$weight,mu=240)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  chickwts$weight
## t = 2.2999, df = 70, p-value = 0.02444
## alternative hypothesis: true mean is not equal to 240
## 95 percent confidence interval:
##  242.8301 279.7896
## sample estimates:
## mean of x 
##  261.3099&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  ttest=function(x,mu,alpha){
    # x = data
    # mu = sample mean, 
    # alpha = alpha 
    # level = (1-confidence level)
    me=mean(x$weight)
    p1=qt(alpha/2,(nrow(x)-1))
    p2=qt(1-alpha/2,(nrow(x)-1))
    s=sqrt(var(x$weight))
    n=nrow(x)
    
    T=(me-mu)/(s/sqrt(nrow(x))) 
    P=seq(1,1,1)
    P[1]=2*(1-pt(T,n))
    P=data.frame(P)
    return (cbind(P,T))
  }
  
  t_test_values &amp;lt;- ttest(chickwts,240,0.05)
  print (&amp;quot;T value and two sided P values returned by the funtion: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;T value and two sided P values returned by the funtion: &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  print(t_test_values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            P        T
## 1 0.02439824 2.299879&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use this function to test the hypothesis that the mean of the  variable (in the  dataset) is equal to 240 against the two-sided alternative. &lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  message(&amp;quot;Hypothesis Result:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hypothesis Result:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  if (2*abs(t_test_values[1,2])&amp;gt;2*abs(t_test_values[1,1])){
    print(&amp;quot;Rejected! The true mean is NOT 240 !!&amp;quot;)
  } else if (2*abs(t_test_values[1,2])&amp;lt;2*abs(t_test_values[1,1])){
    print(&amp;quot;Null. The mean is 240 !!&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Rejected! The true mean is NOT 240 !!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Density Estimation</title>
      <link>/achalneupane.github.io/post/density_estimation/</link>
      <pubDate>Sun, 01 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/density_estimation/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The  data from  contains the velocities of 82 galaxies from six well-separated conic sections of space (Postman et al., 1986, Roeder, 1990). The data are intended to shed light on whether or not the observable universe contains superclusters of galaxies surrounded by large voids. The evidence for the existence of superclusters would be the multimodality of the distribution of velocities.(8.1 Handbook)&lt;/p&gt;
&lt;p&gt;a.) Construct histograms using the following functions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; -hist() and ggplot()+geom_histogram()

 -truehist() and ggplot+geom_histogram() (pay attention to the y-axis!)

 -qplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comment on the shape and distribution of the variable based on the three plots. (Hint: Also play around with binning)&lt;/p&gt;
&lt;p&gt;b.) Create a new variable  = &lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt;(galaxies). Construct histograms using the functions in part a.) and comment on the shape and differences.&lt;/p&gt;
&lt;p&gt;c.) Construct kernel density estimates using two different choices of kernel functions and three choices of bandwidth (one that is too large and “oversmooths,” one that is too small and “undersmooths,” and one that appears appropriate.) Therefore you should have six different kernel density estimates plots. Discuss your results. You can use the log scale or original scale for the variable.&lt;/p&gt;
&lt;p&gt;d.) What is your conclusion about the possible existence of superclusterd of galaxies? How many superclusters (1,2, 3, … )?&lt;/p&gt;
&lt;p&gt;e.) How many clusters did it find? Did it match with your answer from (d) above? Report parameter estimates and BIC of the best model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
library(ggplot2)
# Load the data
data(galaxies)
# #set the vector of numeric velocities to dataframe
# galaxies &amp;lt;- as.data.frame(galaxies)
# #rename the single column in the galaxies dataframe to Velocity
# names(galaxies) &amp;lt;- &amp;#39;Velocity&amp;#39;
# #replace the typo with the correct numeric value
# # galaxies[78,1] &amp;lt;- 26960
# #add the log velocity column
# galaxies &amp;lt;- galaxies %&amp;gt;%
#   mutate(loggalaxies = log(Velocity))

message(&amp;quot;# 1a.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1a.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# histogram
message(&amp;quot;Using hist and geom_histogram:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using hist and geom_histogram:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set the figure position
# par(fig=c(0,1,0,1),new=T)
#draw the histogram
hist(galaxies,
     xlab = &amp;#39;velocity&amp;#39;,
     main = &amp;#39;base R: Histogram showing galaxies&amp;#39;,
     ylab = &amp;#39;Frequency&amp;#39;, freq = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(galaxies) +
  geom_histogram(binwidth = 5000, breaks = c(seq(5000, 35000, 5000)), boundary = NULL, fill = &amp;#39;white&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(title = &amp;#39;ggplot: Histogram showing galaxies&amp;#39;, x= &amp;quot;velocity&amp;quot;, y = &amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# truehist
message(&amp;quot;Using truehist and geom_histogram():&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using truehist and geom_histogram():&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# par(fig=c(0,1,0,1),new=T)
#draw the histogram
truehist(galaxies,
         xlab = &amp;#39;velocity&amp;#39;,
         main = &amp;#39;base R: True Histogram showing galaxies&amp;#39;,
         ylab = &amp;#39;density&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######### *****
# ggplot() + aes(galaxies) +
#   geom_histogram(binwidth = 5000, bins = 10, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;) +
#   labs(x = &amp;quot;velocity&amp;quot;, title = &amp;#39;ggplot: True Histogram showing galaxies&amp;#39;) 

BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
# Frequency
# ggplot() + aes(galaxies) + geom_histogram(binwidth = BINWIDTH, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) 
# Density
ggplot() + aes(galaxies) + geom_histogram(aes(y = ..density..), binwidth = BINWIDTH, breaks = c(seq(5000, 35000, 5000)), boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) + 
labs(title = &amp;quot;ggplot: True Histogram showing galaxies&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######




# qplot
message(&amp;quot;Using qplot:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using qplot:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(galaxies) +
  labs(title=&amp;#39;base R: Histogram showing galaxies (qplot)&amp;#39;,
       x=&amp;#39;Velocity&amp;#39;,
       y=&amp;#39;Frequency&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 30
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() + aes(galaxies) +
  geom_histogram(bins = BINS, breaks = BREAKS, binwidth = BINWIDTH, boundary = NULL, fill = &amp;#39;grey&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;velocity&amp;quot;, y= &amp;quot;Frequency&amp;quot;, title = &amp;#39;ggplot: Histogram showing galaxies (qplot)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;# 1b.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1b.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loggalaxies &amp;lt;- log(galaxies)
# histogram
message(&amp;quot;Using hist and geom_histogram:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using hist and geom_histogram:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set the figure position
# par(fig=c(0,1,0,1),new=T)
#draw the histogram
hist(loggalaxies,
     xlab = &amp;#39;log velocity&amp;#39;,
     main = &amp;#39;base R: Histogram showing log velocity of galaxies&amp;#39;,
     ylab = &amp;#39;Frequency&amp;#39;, freq = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 7
BREAKS &amp;lt;- seq(9, 10.5, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() + aes(loggalaxies) +
  geom_histogram(binwidth = .15, bins = 8, boundary = NULL, fill = &amp;#39;white&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;log velocity&amp;quot;, y = &amp;quot;Frequency&amp;quot;, title = &amp;#39;ggplot: Histogram showing log velocity of galaxies&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# truehist
message(&amp;quot;Using truehist and geom_histogram():&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using truehist and geom_histogram():&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# par(fig=c(0,1,0,1),new=T)
#draw the histogram
truehist(loggalaxies,
         xlab = &amp;#39;log velocity of galaxies&amp;#39;,
         main = &amp;#39;base R: True Histogram of log velocity of galaxies&amp;#39;,
         ylab = &amp;#39;density&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######### *****
# ggplot() + aes(loggalaxies) +
#   geom_histogram(binwidth = 0.1, bins = 0.1, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;) +
#   labs(x = &amp;quot;log velocity&amp;quot;, title = &amp;#39;ggplot: True Histogram showing log velocity of galaxies&amp;#39;) 

BINS &amp;lt;- 7
BREAKS &amp;lt;- seq(9, 10.5, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
# ggplot() + aes(loggalaxies) + geom_histogram(binwidth = BINWIDTH, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) # Frequency
ggplot() + aes(loggalaxies) + geom_histogram(aes(y = ..density..), binwidth = BINWIDTH, bins = BINS, breaks = BREAKS, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
labs(title = &amp;quot;ggplot: True Histogram of log velocity of galaxies&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######


# qplot
message(&amp;quot;Using qplot:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using qplot:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(loggalaxies) +
  labs(title=&amp;#39;base R: Histogram of log velocity of galaxies (qplot)&amp;#39;,
       x=&amp;#39;log velocity of Galaxies&amp;#39;,
       y=&amp;#39;Frequency&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(loggalaxies) +
  geom_histogram(bins = 30, boundary = NULL, fill = &amp;#39;grey&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;log velocity&amp;quot;, title = &amp;#39;ggplot: Histogram showing log velocity of Galaxies (qplot)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-12.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message (&amp;quot;# 1c.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1c.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;blue&amp;quot;, main=&amp;quot;base R: Gaussian Over Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;gaussian&amp;quot;,bw=5000),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-13.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;blue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;gaussian&amp;quot;, bw = 5000, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Gaussian Over Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-14.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;blue&amp;quot;, main=&amp;quot;base R: Gaussian Under Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;gaussian&amp;quot;,bw=500),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-15.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
# BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
# BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;blue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;gaussian&amp;quot;, bw = 500, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Gaussian Under Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-16.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;blue&amp;quot;, main=&amp;quot;base R: Gaussian Best Appx&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;gaussian&amp;quot;,bw=1100),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-17.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;blue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;gaussian&amp;quot;, bw = 1100, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Gaussian Best Appx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-18.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;green&amp;quot;, ylab = &amp;quot;density&amp;quot;, main=&amp;quot;base R: Triangular Over Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;triangular&amp;quot;,bw=5000),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-19.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;green&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;triangular&amp;quot;, bw = 5000, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Triangular Over Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-20.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;green&amp;quot;, ylab= &amp;quot;density&amp;quot;, main=&amp;quot;base R: Triangular Under Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;triangular&amp;quot;,bw=500),col=&amp;quot;red&amp;quot;,main=&amp;quot;Triangular_Under&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-21.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;green&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;triangular&amp;quot;, bw = 500, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Triangular Under Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-22.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;green&amp;quot;, ylab = &amp;quot;density&amp;quot;, main=&amp;quot;base R: Triangular Best Appx&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;triangular&amp;quot;,bw=1100), col=&amp;quot;red&amp;quot;,main=&amp;quot;Triangular_Appx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-23.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;green&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;triangular&amp;quot;, bw = 1100, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Triangular Best Appx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-24.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # hist(galaxies, xlab = &amp;quot;Waiting times (in min.)&amp;quot;, ylab = &amp;quot;Frequency&amp;quot;,
# # probability = TRUE, main = &amp;quot;Rectangular kernel&amp;quot;, border = &amp;quot;gray&amp;quot;)
# # lines(density(galaxies, bw = 5000, window = &amp;quot;gaussian&amp;quot;))
# # rug(galaxies)
# 
# #construct a stat density plot with ggplot2, adjust = 1 for less smoothing
# p1_g &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=1,aes(x=galaxies)) +
#   labs(title = &amp;#39;Gaussian Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a stat density plot with ggplot2, adjust = 2 for moderate smoothing
# p2_g &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=2,aes(x=galaxies)) +
#   labs(title = &amp;#39;Gaussian Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a stat density plot with ggplot2, adjust = 3 for over-smoothing
# p3_g &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=3,aes(x=galaxies$Velocity)) +
#   labs(title = &amp;#39;Gaussian Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a triangular stat density plot, adjust for less smoothing
# p1_t &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;triangular&amp;#39;,adjust=1,aes(x=galaxies$Velocity)) +
#   labs(title = &amp;#39;Triangular Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a triangular stat density plot, adjust for moderate smoothing
# p2_t &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;triangular&amp;#39;,adjust=2,aes(x=galaxies$Velocity)) +
#   labs(title = &amp;#39;Triangular Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a triangular stat density plot, adjust for over-smoothing
# p3_t &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;triangular&amp;#39;,adjust=3,aes(x=galaxies)) +
#   labs(title = &amp;#39;Triangular Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)


message(&amp;quot;# 1e.&amp;quot;,&amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1e.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can use:
library(mclust)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package &amp;#39;mclust&amp;#39; version 5.4.5
## Type &amp;#39;citation(&amp;quot;mclust&amp;quot;)&amp;#39; for citing this R package in publications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod=Mclust(galaxies)
print(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (V,4) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(mod, parameters = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust V (univariate, unequal variance) model with 4 components: 
## 
##  log-likelihood  n df       BIC       ICL
##        -765.694 82 11 -1579.862 -1598.907
## 
## Clustering table:
##  1  2  3  4 
##  7 35 32  8 
## 
## Mixing probabilities:
##          1          2          3          4 
## 0.08440635 0.38660329 0.37116156 0.15782880 
## 
## Means:
##         1         2         3         4 
##  9707.492 19804.259 22879.486 24459.536 
## 
## Variances:
##          1          2          3          4 
##   177296.7   436160.9  1261611.3 34437115.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#pot the density plot of the model
# par(fig=c(0,1,0,1),new=T)
plot(mod,what=&amp;quot;density&amp;quot;)
title (&amp;quot;Density plot of the finite mixture model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-25.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BIC
mclustBIC(galaxies)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Bayesian Information Criterion (BIC): 
##           E         V
## 1 -1622.361 -1622.361
## 2 -1631.243 -1595.403
## 3 -1584.016 -1592.299
## 4 -1592.828 -1579.862
## 5 -1592.299 -1593.277
## 6 -1601.228 -1604.069
## 7 -1588.610 -1611.538
## 8 -1597.427 -1625.804
## 9 -1600.709 -1633.494
## 
## Top 3 models based on the BIC criterion: 
##       V,4       E,3       E,7 
## -1579.862 -1584.016 -1588.610&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;1.a. All graphs have similar shapes for hist() and truehist(), except for a scaling factor. This is expected as hist() shows the frequency along the y axis, whereas truehist() produces the probability, which is a scaled version of the frequency. We can make some estimation about the distribution of the data, but cannot comment in a parametric way. Just based on the plots, we can tell that the data are highly congested in the middle of the data range. We can best think that there are three clusters of data, but qplot(), shows an extra cluster in the middle-congested cluster of data. By observing the qplot(),we can assume that there are four clusters in the data set.&lt;/p&gt;
&lt;p&gt;1.b. Here a scaled version of the data in part (a) is used to construct the same plots. Similar comments like in part (a) is valid here as well to describe the similarity of hist() and truehist(). From this scaled data hist() and truehist(), still it is reasonable to guess that there are three clusters.&lt;/p&gt;
&lt;p&gt;1.c. Here over smoothing and under smoothing bin width choice were easy and straight forward. Startting with the extreme bw values, 5000, 500 and 1100 were used to generate the three Kernels for Gaussian and Triangular. Searching for the best fit just by visualizing was not easy. By changing the the binwidth, we can actually set the Kernel to better fit for the best approximate of clusters.&lt;/p&gt;
&lt;p&gt;1.d. From the figures found in part (c), we can assume that there are four clusters in the data set with unequal variances. It was hard to determine manually, but my guess was that there are about three or four clusters with almost equal possiblity.&lt;/p&gt;
&lt;p&gt;1.e. The mclust() found four clusters of unequal variance for the best fit. However, the density plot of the model indicates there are three clusters. Based on this information, we can say that there are at least 3 or 4 clusters that can be determined from this analysis.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The  data from  gives the birth and death rates for 69 countries (from Hartigan, 1975). (8.2 Handbook)&lt;/p&gt;
&lt;p&gt;a.) Produce a scatterplot of the data and overlay a contour plot of the estimated bivariate density.&lt;/p&gt;
&lt;p&gt;b.) Does the plot give you any interesting insights into the possible structure of the data?&lt;/p&gt;
&lt;p&gt;c.) Construct the perspective plot (persp() in R, GGplot is not required for this question).&lt;/p&gt;
&lt;p&gt;d.) Model-based clustering (Mclust). Provide plot of the summary of your fit (BIC, classification, uncertainty, and density).&lt;/p&gt;
&lt;p&gt;e.) Discuss the results (structure of data, outliers, etc.). Write a discussion in the context of the problem.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(KernSmooth)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## KernSmooth 2.23 loaded
## Copyright M. P. Wand 1997-2009&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(birthdeathrates)
head(birthdeathrates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     birth death
## alg  36.4  14.6
## con  37.3   8.0
## egy  42.1  15.3
## gha  55.8  25.6
## ict  56.1  33.1
## mag  41.8  15.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(birthdeathrates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(birthdeathrates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 69&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message (&amp;quot;# 2a&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 2a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BDRd &amp;lt;- bkde2D(birthdeathrates, bandwidth = sapply(birthdeathrates, dpik))
contour(x=BDRd$x1, y=BDRd$x2, z=BDRd$fhat,
        main = &amp;quot;base R: Countour Scatterplot of Birth_Death_Rates&amp;quot;,
        xlab=&amp;quot;Birth Rates&amp;quot;, 
        ylab=&amp;quot;Death Rates&amp;quot;,
        xlim =c(0,60), ylim = c(0,35))
points(birthdeathrates, pch=16, col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=birthdeathrates,aes(birth,death)) + 
  geom_density2d(aes(colour=..level..)) + 
  scale_colour_gradient(low=&amp;quot;green&amp;quot;,high=&amp;quot;red&amp;quot;) + 
  theme_bw() +
  geom_point() +
  # geom_text(aes(label=ifelse(death &amp;gt;= 15 | birth &amp;gt;= 35,row.names(birthdeathrates),&amp;#39;&amp;#39;),hjust=0, vjust=0)) +
  labs(title=&amp;#39;ggplot: Countour Scatterplot of Birth_Death_Rates&amp;#39;,
       x=&amp;#39;Birth Rate&amp;#39;,
       y=&amp;#39;Death Rate&amp;#39;) +
  scale_x_continuous(limits = c(0,60)) +
  scale_y_continuous(limits = c(0,35))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;# 2c.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 2c.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persp (x=BDRd$x1, y=BDRd$x2, z=BDRd$fhat,
       xlab=&amp;quot;Birth Rates&amp;quot;, 
       ylab=&amp;quot;Death Rates&amp;quot;,
       zlab=&amp;quot;Estimated Density&amp;quot;,
       theta=-35, axes=TRUE, box=TRUE, main = &amp;quot;Perspective plot for birthdeathrates data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;# 2d.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 2d.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mclust)

mod &amp;lt;- Mclust(birthdeathrates)
mod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (EII,4) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod, parameters = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust EII (spherical, equal volume) model with 4 components: 
## 
##  log-likelihood  n df       BIC       ICL
##       -424.4194 69 12 -899.6481 -906.4841
## 
## Clustering table:
##  1  2  3  4 
##  2 17 38 12 
## 
## Mixing probabilities:
##          1          2          3          4 
## 0.02898652 0.24555002 0.55023375 0.17522972 
## 
## Means:
##           [,1]     [,2]      [,3]      [,4]
## birth 55.94967 43.80396 19.922913 33.730672
## death 29.34960 12.09411  9.081348  8.535812
## 
## Variances:
## [,,1]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108
## [,,2]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108
## [,,3]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108
## [,,4]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;Value for mod$parameters$mean&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Value for mod$parameters$mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(mod$parameters$variance$sigmasq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.195434&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BIC.data&amp;lt;- as.data.frame(mod$BIC[,])
BIC.data$NumComp&amp;lt;-rownames(BIC.data)
melted.BIC&amp;lt;- reshape2::melt(BIC.data, var.ids= &amp;quot;NumComp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using NumComp as id variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# length(levels(melted.BIC$variable))

par(mfrow=c(1,1), ask=FALSE)
# BIC
plot(mod, what=&amp;quot;BIC&amp;quot;, main = &amp;quot;base R: Plot of BIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(melted.BIC, aes(x=as.numeric(NumComp), y=value, colour=variable, group=variable))+
  scale_x_continuous(&amp;quot;Number of Components&amp;quot;)+
  scale_y_continuous(&amp;quot;BIC&amp;quot;)+
  scale_colour_hue(&amp;quot;&amp;quot;)+
  geom_point()+
  geom_line()+
  theme_bw() + 
  labs(title = &amp;quot;ggplot: BIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 19 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 19 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# uncertainty
par(mfrow=c(1,1), ask=FALSE)
plot(mod, what=&amp;quot;uncertainty&amp;quot;)
title(main = &amp;quot;base R: Plot of uncertainty&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birthdeathrates %&amp;gt;% mutate(uncertainty = mod$uncertainty,
                classification = factor(mod$classification)) %&amp;gt;% 
  ggplot(aes(birth, death, size = uncertainty, color = classification)) +
  geom_point() + 
  guides(size = guide_legend(), colour = &amp;quot;legend&amp;quot;) + theme_classic() +
  stat_ellipse(level = 0.5, type = &amp;quot;t&amp;quot;) + 
  labs(x = &amp;quot;birth&amp;quot;, y = &amp;quot;death&amp;quot;, title = &amp;quot;ggplot: Uncertainty&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Too few points to calculate an ellipse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# classification
par(mfrow=c(1,1), ask=FALSE)
plot(mod, what=&amp;quot;classification&amp;quot;)
title(main = &amp;quot;base R: Plot of classification&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birthdeathrates %&amp;gt;% mutate(uncertainty = mod$uncertainty,
                classification = factor(mod$classification)) %&amp;gt;% 
  ggplot(aes(birth, death, shape = classification, color = classification)) +
  geom_point() + 
  guides(size = guide_legend(), shape = guide_legend()) + theme_classic() +
  stat_ellipse(level = 0.5, type = &amp;quot;t&amp;quot;) + 
  labs(x = &amp;quot;birth&amp;quot;, y = &amp;quot;death&amp;quot;, title = &amp;quot;ggplot: Plot of classification&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Too few points to calculate an ellipse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# density
par(mfrow=c(1,1), ask=FALSE)
plot(mod, what=&amp;quot;density&amp;quot;)
title(main = &amp;quot;base R: Plot of density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(birthdeathrates, aes(x = birth, y = death)) +
  geom_point() +
  geom_density_2d() + 
  labs ( title = &amp;quot;ggplot: Plot of density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;2.b. Comparaing data for birth rates from 10 to about 50 with the death rates, we can tell that the death rate is relatively slow. I would say twice as many people are being born than they are dying (i.e., 2:1 birth to death ratio). The countour appears to show the majority of countries grouping around a birthrate of 20 and a death rate of 10.&lt;/p&gt;
&lt;p&gt;2.e. The BIC plot indicates that there are 4 clusters. The classification plot seems to indicate the groupings of the data. The table of means indicate that the birth rates group along ~ 20, 34, 44, and 56. The death rates group around 9, 8, 12 and 30.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A sex difference in the age of onset of schizophrenia was noted by Kraepelin (1919). Subsequent epidemiological studies of the disorder have consistently shown an earlier onset in men than in women. One model that has been suggested to explain this observed difference is known as the subtype model which postulates two types of schizophrenia, one characterized by early onset, typical symptoms and poor premorbid competence; and the other by late onset, atypical symptoms and good premorbid competence. The early onset type is assumed to be largely a disorder of men and the late onset largely a disorder of women. Fit finite mixutres of normal densities separately to the onset data for men and women given in the  data from . See if you can produce some evidence for or against the subtype model. (8.3 Handbook)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Answer:&lt;/p&gt;
&lt;p&gt;Based on this density plot faceted by gender, we can tell that the distribution in diagnosis of disease is centered towards 20 years (age) in males whereas, for females, its more uniform thoughout the life.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(schizophrenia)
#plot the schizophrenia data using stat_density within ggplot2, facet by gender
# par(fig=c(0,1,0,1),new=T)
ggplot(data=schizophrenia)+
  stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=1,aes(age,fill=gender)) +
  facet_grid(gender~.) +
       labs(title = &amp;#39;Density plot (gaussian) of Schizophrenia diagnosis data&amp;#39;,
       x = &amp;quot;Diagnosis Age&amp;quot;,
       y=&amp;#39;Density Estimate&amp;#39;) +
  scale_fill_manual( values = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;)) +
  theme(
        strip.background = element_blank(),
        strip.text.y = element_blank(),
        panel.background = element_blank()
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also check the distribution of the data using histogram plots. These plots also show that male diagnosis age numbers are centered more around mid 20’s, where as females patients can also be diagnosed in 40s and fewer (compared to males) in mid 20s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# par(fig=c(0,1,0,1),new=T)
ggplot(data=schizophrenia)+
  geom_histogram(aes(age,fill=gender)) +
  facet_grid(gender~.) +
  labs(title = &amp;#39;Histogram of Schizophrenia Diagnosis by Gender&amp;#39;,
       x = &amp;#39;Age of diagnosis&amp;#39;,
       y=&amp;#39;Frequency&amp;#39;) +
scale_fill_manual( values = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;)) +
# scale_color_brewer(palette = &amp;quot;Set2&amp;quot;) +
  theme(
        strip.background = element_blank(),
        strip.text.y = element_blank(),
        panel.background = element_blank()
        )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or we can visualize both together and see the same results discussed above by making this plot below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setMen &amp;lt;- subset(schizophrenia, gender==&amp;quot;male&amp;quot;)$age
setWmn &amp;lt;- subset(schizophrenia, gender!=&amp;quot;male&amp;quot;)$age

par(mfrow=c(1,1))
hist (schizophrenia$age, xlab=&amp;quot;Age&amp;quot;, ylab=&amp;quot;Density&amp;quot;, main=&amp;quot;Distribution of Schizophrenia Onset by Age&amp;quot;, freq=FALSE, ylim=c(0,.075), border=4)
lines(density(setMen), col=1, )
lines(density(setWmn), col=2)
legend(40, 0.05, legend=c(&amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;),
       col=c(1,2), lty=c(1,1), cex=0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can subset the schizophrenia data by male and female for fit of model analysis by gender:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(schizophrenia)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   age gender
## 1  20 female
## 2  30 female
## 3  21 female
## 4  23 female
## 5  30 female
## 6  25 female&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;onset=schizophrenia
male=subset(onset,gender==&amp;quot;male&amp;quot;)
female=subset(onset,gender==&amp;quot;female&amp;quot;)
message(&amp;quot;
Mclust Data
        &amp;quot;,&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mclust Data
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_male=Mclust(male$age)
mod_female=Mclust(female$age)
# par(mfrow=c(2,2))
plot(mod_male, what = &amp;quot;BIC&amp;quot;)
title(main=&amp;#39;BIC Of schizophrenia for male&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mod_female, what = &amp;quot;BIC&amp;quot;)
title(main=&amp;#39;BIC Of schizophrenia for female&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;Male
        &amp;quot;,&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Male
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(mod_male)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (V,2) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(mod_male, parameters = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust V (univariate, unequal variance) model with 2 components: 
## 
##  log-likelihood   n df       BIC       ICL
##       -520.9747 152  5 -1067.069 -1134.392
## 
## Clustering table:
##  1  2 
## 99 53 
## 
## Mixing probabilities:
##         1         2 
## 0.5104189 0.4895811 
## 
## Means:
##        1        2 
## 20.23922 27.74615 
## 
## Variances:
##          1          2 
##   9.395305 111.997525&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# print(mod_male$parameters)
message(&amp;quot;FeMale
        &amp;quot;,&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## FeMale
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(mod_female)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (E,2) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(mod_female, parameters = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust E (univariate, equal variance) model with 2 components: 
## 
##  log-likelihood  n df       BIC       ICL
##       -373.6992 99  4 -765.7788 -774.8935
## 
## Clustering table:
##  1  2 
## 74 25 
## 
## Mixing probabilities:
##         1         2 
## 0.7472883 0.2527117 
## 
## Means:
##        1        2 
## 24.93517 46.85570 
## 
## Variances:
##        1        2 
## 44.55641 44.55641&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# print(mod_female$parameters)

# male group mean
message(paste0(&amp;quot;Male Group mean is 0.51*20.23+(1-0.51)*27.75 = &amp;quot;, 0.51*20.23+(1-0.51)*27.75))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Male Group mean is 0.51*20.23+(1-0.51)*27.75 = 23.9148&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Female group mean
message(paste0(&amp;quot;Female Group mean is 0.746*24.93+(1-0.747)*46.85 = &amp;quot;, 0.746*24.93+(1-0.747)*46.85))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Female Group mean is 0.746*24.93+(1-0.747)*46.85 = 30.45083&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model summary above, we can see that the female model showing data points centered at about 25 and age 47 of age marks, whereas for males it was at around 20 and 27 years of age (i.e., within 20s). For males, the mean was calculated to be 23.91. Similarly, the mean of whole female group was calculated to be 30.45. So female group has a larger age mean which tells us that the males have onset of disorder earlier than females. Additionally, the BIC plot shows that the optimal number of cluster for both males and females is 2.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
