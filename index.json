[{"authors":["admin"],"categories":null,"content":"I have been working on bioinformatics and statistical genetics focused projects. I have extensive experience in academic, clinical and industry collaborations with researchers in the US and Australia. I have gained key expertise in the analysis of large clinically focused datasets (mostly Cancer genomes). The goals of these research were to characterize and understand the genetic origin of disease and to translate these findings into clinical practice. In the past, I had analyzed a cohort of 150 clinically characterized acute myeloid leukemia (AML) sequenced with whole exome and whole genome sequencing, and a cohort of 900 control exomes and 600 whole genomes to validate and compare genotyping algorithms and sequencing technologies, and to perform gene discovery for both somatic and germline risk variants. For this, I used sophisticated statistical genetics algorithms that identify both protective and deleterious variants. I also used several variant calling methods to characterize germline, copy number and structural variants in AML. I also developed several algorithms (in R) for these studies, including to detect sample contamination, identity and ethnicity for quality control of Next Generation Sequencing (NGS) data. I also compared pathology, cytogenetic and Sequenom genotyping data to refine and calibrate these algorithms.\nRecently, I have studied the effects of hypovirulence-associated DNA virus 1(SsHADV-1) and Sclerotinia sclerotiorum hypovirus 2 Lactuca (SsHV2L) on white mold fungus pathogen Sclerotinia sclerotiorum using small RNAseq and RNAseq data. Over the years, I have worked on human (Cancer), plants, insects, fungal and viral genome datasets.\nDuring my graduate studies, I had taken several \u0026ldquo;data-heavy\u0026rdquo; courses in Statistics and Computer science (besides Biological science) which fueled my interest in Data Science. I am interested in big data analysis, algorithm development, scientific computing, including analysis of biological (NGS)/non-biological datasets.\nGraduate level courses in Statistics and Computer science: View\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/achalneupane.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/achalneupane.github.io/authors/admin/","section":"authors","summary":"I have been working on bioinformatics and statistical genetics focused projects. I have extensive experience in academic, clinical and industry collaborations with researchers in the US and Australia. I have gained key expertise in the analysis of large clinically focused datasets (mostly Cancer genomes). The goals of these research were to characterize and understand the genetic origin of disease and to translate these findings into clinical practice. In the past, I had analyzed a cohort of 150 clinically characterized acute myeloid leukemia (AML) sequenced with whole exome and whole genome sequencing, and a cohort of 900 control exomes and 600 whole genomes to validate and compare genotyping algorithms and sequencing technologies, and to perform gene discovery for both somatic and germline risk variants.","tags":null,"title":"Achal Neupane","type":"authors"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  The algorithm to do this analysis is divided into several steps as described below:\nFirst you have to define the path to all csv files for final under (path.final) and all .csv files from midterm under path.midterm in the begining of the code below. I have used some external packages like ggplot2 to make the plots. I have written several functions to analyze each individual field as well as merged field statistics that calculates cohens effect size, required replicates, and does ANOVA analysis on individual field as well as on merged data from all fields. This function get_question_1_and_2_answers calculates the time spent each day on field, start and end time start and end date. observations each day, and also calculate one hour gap (more than an hour gap in consecutive rows) if present in the data. It spits out a table (range_table) with all the date data required for this project. I then used this range table to plot time, and dates. I have also plotted the Moisture content in each harvest field and how it gets affected during the day, and across the range of dates. Function calculate_field_mean_SD_and_get_RR_EffSize also uses mean, sd, and counts from each field to calculate cohens d and Required replicates. It also uses pooled standard deviations and Mean (mean of Means) calculated from all four fields for different control Rates to calculate cohens d and required replicates of combined field data. I have tested this function on all four fields at ControlRate level of 500, 1000, 2000 and 3000 intervals for mid-term, but this time, I have modified this function to calculate cohends d and required replicates for 1000 control rates also taking dates into consideration. This function also calculates these values for date only (neglecting control Rates). I have also shown ANOVA analysis followed by TUKEY HSD test to show which ControlRates of seeding have significant effect on Yield for each field and then for all fields (combined all four fields). Lastly, I have plotted EffectSize Vs RequiredReplicates for all combined field.  First of all we, will set two directory paths where we have all our files we want to anlalyze. first path is path.final for final data and path.midterm for midterm data\nall.files contains the file name of all csv files in our directory. I have decided to read all files from the system rather than reading one file at a time.\n# # Path to all .csv data files for final. # path.final \u0026lt;- # \u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/final/\u0026quot; # # # Path to all .csv data files for midterm. # path.midterm \u0026lt;- # \u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/midterm/\u0026quot; # instead for local path, we can get files from git repos library(rvest) ## Loading required package: xml2 path.final \u0026lt;- read_html(\u0026quot;https://github.com//achalneupane/data\u0026quot;) %\u0026gt;% html_nodes(\u0026quot;.js-navigation-open\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) path.midterm \u0026lt;- read_html(\u0026quot;https://github.com//achalneupane/data\u0026quot;) %\u0026gt;% html_nodes(\u0026quot;.js-navigation-open\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) Now, we first install and load some of the packages (“multcompView”, “ggplot2”, “scales”, “data.table”, etc.) we will be using for this exercise.\n# First, install missing packages and load them myPackages \u0026lt;- c( \u0026quot;multcompView\u0026quot;, \u0026quot;ggplot2\u0026quot;, \u0026quot;scales\u0026quot;, \u0026quot;data.table\u0026quot;, \u0026quot;reshape2\u0026quot;, \u0026quot;RColorBrewer\u0026quot;, \u0026quot;plyr\u0026quot;, \u0026quot;ggpmisc\u0026quot; ) my.installed.packages \u0026lt;- installed.packages() available.packages \u0026lt;- myPackages %in% my.installed.packages if (sum(!available.packages) \u0026gt; 0) { install.packages(myPackages[!available.packages]) } # Load all required packages lapply(myPackages, require, character.only = TRUE) ## Loading required package: multcompView ## Loading required package: ggplot2 ## Loading required package: scales ## Loading required package: data.table ## Loading required package: reshape2 ## ## Attaching package: \u0026#39;reshape2\u0026#39; ## The following objects are masked from \u0026#39;package:data.table\u0026#39;: ## ## dcast, melt ## Loading required package: RColorBrewer ## Loading required package: plyr ## Loading required package: ggpmisc ## For news about \u0026#39;ggpmisc\u0026#39;, please, see https://www.r4photobiology.info/ ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] TRUE ## ## [[5]] ## [1] TRUE ## ## [[6]] ## [1] TRUE ## ## [[7]] ## [1] TRUE ## ## [[8]] ## [1] TRUE Normally, I would write all functions in a separate file and use them as source(‘function_file.r’) However, I have my functions for this project written here.\n# field_or_seed \u0026lt;- harvestB # functions # function for question 1 and 2 get_question_1_and_2_answers \u0026lt;- function(field_or_seed) { # range of planting dates field_or_seed$Date \u0026lt;- gsub(\u0026quot;T.*\u0026quot;, \u0026quot;\u0026quot;, field_or_seed$Timestamp) field_or_seed$Time \u0026lt;- gsub(\u0026quot;.*T|Z\u0026quot;, \u0026quot;\u0026quot;, field_or_seed$Timestamp) # Range in days field_or_seed$date_time \u0026lt;- as.POSIXct(paste(field_or_seed$Date, field_or_seed$Time), tz = \u0026quot;UTC\u0026quot;) # field_or_seed$date_time \u0026lt;- paste(field_or_seed$Date, field_or_seed$Time) # num_days \u0026lt;- round(difftime(max(as.POSIXct(field_or_seed$date_time)), min(as.POSIXct(field_or_seed$date_time))), 3) num_days \u0026lt;- round(difftime(max(as.POSIXct( field_or_seed$date_time )), min(as.POSIXct( field_or_seed$date_time )), units = \u0026quot;days\u0026quot;), 3) range_from_start_to_end_date_in_days \u0026lt;- ifelse(num_days \u0026lt;= 1, paste0(num_days, \u0026quot; day\u0026quot;), paste0(num_days, \u0026quot; days\u0026quot;)) # Range in span of date for the field range_of_start_end_date \u0026lt;- paste0(range(as.Date(field_or_seed$Date))[1], \u0026quot; to \u0026quot;, range(as.Date(field_or_seed$Date))[2]) field_or_seed$Date \u0026lt;- as.factor(field_or_seed$Date) # time range each day tt \u0026lt;- field_or_seed[, c(\u0026quot;Date\u0026quot;, \u0026quot;Time\u0026quot;)] time.range.each.day \u0026lt;- aggregate(data.frame(Time = strptime(do.call(paste, tt), \u0026#39;%F %R:%OS\u0026#39;, tz = \u0026#39;UTC\u0026#39;)), by = list(Date = tt$Date), function(Time) { paste(format(min(Time), \u0026#39;%T\u0026#39;), format(max(Time), \u0026#39;%T\u0026#39;), sep = \u0026#39; to \u0026#39;) }) rm(tt) # Function to find more than an hour gap in consecutive rows about_an_hour_gap \u0026lt;- function(field_or_seed) { one_hour_thing \u0026lt;- { } for (i in 1:nrow(field_or_seed)) { # i=3 if (i == nrow(field_or_seed)) { break } else time.lapse \u0026lt;- abs(as.numeric( difftime(field_or_seed$date_time[i + 1], field_or_seed$date_time[i]), units = \u0026quot;mins\u0026quot; )) # find consecutive rows where the time is more than or equal 60 minutes and # dates are the same if (time.lapse \u0026gt;= 60 \u0026amp; field_or_seed$Date[i + 1] == field_or_seed$Date[i]) { one_hour_thing.tmp \u0026lt;- field_or_seed[i:(i + 1), ] one_hour_thing \u0026lt;- rbind(one_hour_thing, one_hour_thing.tmp) } } return(one_hour_thing) } about_an_hour_gap(field_or_seed) one_hour_gap_time_stamp \u0026lt;- about_an_hour_gap(field_or_seed) # function to calculate total time spent each day time.elapsed.each.day.function \u0026lt;- function(x) { difftime(max(as.POSIXct(x)), min(as.POSIXct(x)), units = \u0026quot;mins\u0026quot;) } # time.range.each.day(tt) time.elapsed.each.day \u0026lt;- aggregate(date_time ~ Date, data = field_or_seed, FUN = time.elapsed.each.day.function) colnames(time.elapsed.each.day) \u0026lt;- c(\u0026quot;Date\u0026quot;, \u0026quot;Time_elapsed (minutes)\u0026quot;) observations.each.day \u0026lt;- table(field_or_seed$Date) # Elapsed Dates dates.elapsed.each.field.function \u0026lt;- function(x){ difftime(max(as.POSIXct(x)), min(as.POSIXct(x)), units = \u0026quot;days\u0026quot;) } elapsed.dates.each.field \u0026lt;- as.integer(dates.elapsed.each.field.function(field_or_seed$Date)) return( list( observations.each.day = observations.each.day, range_from_start_to_end_date_in_days = range_from_start_to_end_date_in_days, range_of_start_end_date = range_of_start_end_date, time.elapsed.each.day = time.elapsed.each.day, elapsed.dates.each.field = elapsed.dates.each.field, time.range.each.day = time.range.each.day, one_hour_gap_time_stamp = one_hour_gap_time_stamp ) ) } # Functions to calculate cohen\u0026#39;s d and Required Replicates cohen.d \u0026lt;- function(m1, s1, m2, s2) { cohens_d \u0026lt;- (abs(m1 - m2) / sqrt((s1 ^ 2 + s2 ^ 2) / 2)) return(cohens_d) } required.replicates \u0026lt;- function (m1, s1, m2, s2, alpha = 0.05, beta = 0.2) { n \u0026lt;- 2 * ((((sqrt((s1 ^ 2 + s2 ^ 2) / 2 )) / (m1 - m2)) ^ 2) * (qnorm((1 - alpha / 2)) + qnorm((1 - beta))) ^ 2) return(round(n, 0)) } # Additional functions: They do anova and also calculate required replicates for fields # We willl also perform ANOVA analysis with Tukey Test for paired comparision of # mean for each field data as well as merged data at different ControlRate # intervals. This function does Tukey HSD test and generates label for # significant outcomes. # Create function to get the labels for Tukey HSD: generate_label_df \u0026lt;- function(TUKEY, variable) { # Extract labels and factor levels from Tukey post-hoc Tukey.levels \u0026lt;- TUKEY[[variable]][, 4] Tukey.labels \u0026lt;- data.frame(multcompLetters(Tukey.levels)[\u0026#39;Letters\u0026#39;]) #I need to put the labels in the same order as in the boxplot : Tukey.labels$treatment = rownames(Tukey.labels) Tukey.labels = Tukey.labels[order(Tukey.labels$treatment) ,] return(Tukey.labels) } # This function does ANOVA and makes boxplots with Tukey statistics for # comparing Mean yield. get_my_box_plot \u0026lt;- function (field, plot_name = \u0026quot;the Field\u0026quot;) { field$CR.Date.Levels \u0026lt;- gsub(\u0026quot;-\u0026quot;, \u0026quot;_\u0026quot;, field$CR.Date.Levels) field$CR.Date.Levels \u0026lt;- as.factor(field$CR.Date.Levels) model = lm(field$Yield ~ field$CR.Date.Levels) ANOVA = aov(model) # If residual degrees of freedom is less than or equal 1, don\u0026#39;t do Tukey test; # simply return ANOVA summary. if (ANOVA$df.residual \u0026lt;= 1) { return(summary.aov(ANOVA)) } # Tukey test to study each pair of treatment : TUKEY \u0026lt;- TukeyHSD(x = ANOVA, \u0026#39;field$CR.Date.Levels\u0026#39;, conf.level = 0.95) # generate labels using function labels \u0026lt;- generate_label_df(TUKEY , \u0026quot;field$CR.Date.Levels\u0026quot;) # rename columns for merging names(labels) \u0026lt;- c(\u0026#39;Letters\u0026#39;, \u0026#39;CR.Date.Levels\u0026#39;) # Obtain letter positions for y axis using means yvalue \u0026lt;- aggregate(Yield ~ CR.Date.Levels, data = field, mean) final \u0026lt;- merge(labels, yvalue) #merge dataframes p \u0026lt;- ggplot(field, aes(x = CR.Date.Levels, y = Yield)) + geom_blank() + theme_bw() + # theme(panel.grid.major = element_blank(), # panel.grid.minor = element_blank()) + labs(x = \u0026#39;CR.Date.Levels\u0026#39;, y = \u0026#39;Mean Yield\u0026#39;) + ggtitle(paste0(\u0026quot;CR.Date.Levels Vs Mean yield for \u0026quot;, plot_name), expression(atop(italic( \u0026quot;(Anova:TukeyHSD)\u0026quot; ), \u0026quot;\u0026quot;))) + # ggtitle(paste0(\u0026quot;CR.Date.Levels Vs Mean yield for \u0026quot;, plot_name)) + theme(plot.title = element_text(hjust = 0.5, face = \u0026#39;bold\u0026#39;)) + geom_boxplot(fill = \u0026#39;grey\u0026#39;, stat = \u0026quot;boxplot\u0026quot;) + # coord_cartesian(clip = \u0026#39;off\u0026#39;) + geom_text( data = final, aes(x = CR.Date.Levels, y = Yield, label = Letters), vjust = -3.5, hjust = -.5 ) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # geom_vline(aes(xintercept = 4.5), linetype = \u0026quot;dashed\u0026quot;) + theme(plot.title = element_text(vjust = -0.6)) + coord_cartesian(ylim = c(min(field$Yield), max(field$Yield) + 10)) print(p) } Function to calculate mean and sd for each field for a given interval of ControlRate and then calculate Required Replicates and EffectSize. I have made one function to do all that so I can just use this function to analyze all four fields. To analyze single field set Single.Field.Analysis = TRUE, and to analyze all four field together, set Single.Field.Analysis = FALSE,\ncalculate_field_mean_SD_and_get_RR_EffSize \u0026lt;- function( field, intervals = 1000, Single.Field.Analysis = TRUE, Date.Only = FALSE, printplot = FALSE){ options(warn=1) plot_NAME \u0026lt;- (deparse(substitute(field))) if(Single.Field.Analysis \u0026gt; 0){ # removing rows with NAs in Yield sum(is.na(field$Yield)) # field \u0026lt;- field[!complete.cases(field$Yield), ] field \u0026lt;- field[!is.na(field$Yield),] # as.factor(fieldA$ControlRate) # work on timestamp columns field$Date \u0026lt;- gsub(\u0026quot;T.*\u0026quot;, \u0026quot;\u0026quot;, field$Timestamp) field$Time \u0026lt;- gsub(\u0026quot;.*T|Z\u0026quot;, \u0026quot;\u0026quot;, field$Timestamp) # Range in days field$date_time \u0026lt;- as.POSIXct(paste(field$Date, field$Time), tz = \u0026quot;UTC\u0026quot;) # table.field \u0026lt;- table(as.factor(field$Date)) field$ControlRate.Levels \u0026lt;- as.factor(intervals * ceiling(field$ControlRate/intervals)) if(Date.Only \u0026lt; 1){ field$CR.Date.Levels \u0026lt;- as.factor(paste(field$ControlRate.Levels, field$Date, sep = \u0026quot;*\u0026quot;)) } else { field$CR.Date.Levels \u0026lt;- as.factor(field$Date) } table.field \u0026lt;- table(as.factor(field$CR.Date.Levels)) field.Count \u0026lt;- setNames(aggregate(field$CR.Date.Levels, by = list(field$CR.Date.Levels), FUN = length), c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;Count\u0026quot;)) # # Degree of freedome = n * k - k # field.Count$degree.Freedom \u0026lt;- # (field.Count$Count * length(field.Count$ControlRate)) -length(field.Count$ControlRate) field.mean \u0026lt;- setNames(aggregate( field$Yield, by = list(field$CR.Date.Levels), FUN = mean ), c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;Mean\u0026quot;)) field.SD \u0026lt;- setNames(aggregate( field$Yield, by = list(field$CR.Date.Levels), FUN = sd ), c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;SD\u0026quot;)) if(printplot == 1 ){ get_my_box_plot(field, plot_name = plot_NAME) } # plot individual fields with tukey test We will print box plot only if we # want for certain ControlRates intervals. Otherwise we will have too many # plots } else { temp.Field \u0026lt;- field colnames(temp.Field)[colnames(temp.Field) == \u0026quot;CR.Date.Levels\u0026quot;] \u0026lt;- \u0026quot;ControlRate.Levels\u0026quot; colnames(temp.Field)[colnames(temp.Field) == \u0026quot;Mean\u0026quot;] \u0026lt;- \u0026quot;Yield\u0026quot; # get_my_box_plot(temp.Field) field.SD \u0026lt;- as.data.frame(cbind(ControlRate = field[\u0026quot;CR.Date.Levels\u0026quot;], SD = field[\u0026quot;SD_pooled\u0026quot;])) field.mean \u0026lt;- as.data.frame(cbind(ControlRate = field[\u0026quot;CR.Date.Levels\u0026quot;], Mean = field[\u0026quot;Mean\u0026quot;])) } # Calculate Required replicate and Effect Size from each # field for ControlRate i vs i+1 # ReqRep_EffectSize_table \u0026lt;- function (field.mean, field.SD){ Req.Rep.table.field \u0026lt;- {} for (i in 1:nrow(field.SD)){ if(i+1 \u0026gt; nrow(field.SD) ){ break } temp.Effect.size \u0026lt;- cohen.d( m1 = field.mean$Mean[i], s1 = field.SD$SD[i], m2 = field.mean$Mean[i + 1], s2 = field.SD$SD[i + 1] ) tmp.req.reps \u0026lt;- required.replicates( m1 = field.mean$Mean[i], s1 = field.SD$SD[i], m2 = field.mean$Mean[i + 1], s2 = field.SD$SD[i + 1] ) tmp.table \u0026lt;- cbind( Group = paste0(field.SD$CR.Date.Levels[i], \u0026quot; Vs \u0026quot;, field.SD$CR.Date.Levels[i + 1]), EffectSize = temp.Effect.size, RequiredReplicates = tmp.req.reps ) Req.Rep.table.field \u0026lt;- rbind(Req.Rep.table.field, tmp.table) } if (Single.Field.Analysis \u0026gt; 0) { return( list( field.mean = field.mean, fieldSD = field.SD, field.Count = field.Count, Req.Rep.table.field = Req.Rep.table.field ) ) } else{ return(list(Req.Rep.table.field = Req.Rep.table.field)) } } Here, I am reading all files in my directory we set in the begining (path.final). I was being a bit lazy to copy hard coded paths. I then read those files in for loop and assigned them to their respective variable names [eg, gsub(“.csv”,\u0026quot;\u0026quot;,filename.csv)].\n# all.files contains the file name of all csv files in our directory. I have decided to # read all files from the system rather than reading one file at a time. # setwd(path.final) # if using local path # all.files \u0026lt;- list.files(path.final) # using from github all.files \u0026lt;- path.final all.files \u0026lt;- all.files[grepl(\u0026quot;*.csv\u0026quot;, all.files)] # # selecting only required files all.files \u0026lt;- all.files[grepl(\u0026quot;seedA.csv|seedB.csv|seedC.csv|seedD.csv|harvestA.csv|harvestB.csv|harvestC.csv|harvestD.csv\u0026quot;, all.files)] # # Or choose specifically which data files in path to analyze # all.files \u0026lt;- all.files \u0026lt;- c(\u0026quot;seedA.csv\u0026quot;, \u0026quot;seedB.csv\u0026quot;, \u0026quot;seedC.csv\u0026quot;, \u0026quot;seedD.csv\u0026quot;) # Rearranging the vectors, putting seed data first and then harvest data all.files \u0026lt;- c(all.files[grepl(\u0026quot;seed\u0026quot;, all.files)], all.files[grepl(\u0026quot;harvest\u0026quot;, all.files)]) # all.files # [1] \u0026quot;harvestA.csv\u0026quot; \u0026quot;harvestB.csv\u0026quot; \u0026quot;harvestC.csv\u0026quot; \u0026quot;harvestD.csv\u0026quot; \u0026quot;seedA.csv\u0026quot; \u0026quot;seedB.csv\u0026quot; \u0026quot;seedC.csv\u0026quot; \u0026quot;seedD.csv\u0026quot; #https://raw.githubusercontent.com/achalneupane/data/master/ # reading all csv files within the path and saving them to their respective names. for (i in 1:length(all.files)) { assign( gsub(\u0026quot;.*master/|.csv\u0026quot;, \u0026quot;\u0026quot;, all.files)[i], read.table(gsub(\u0026quot;blob/\u0026quot;, \u0026quot;\u0026quot;,paste0(\u0026quot;https://raw.githubusercontent.com\u0026quot;,all.files[i])), header = TRUE, sep = \u0026quot;,\u0026quot;) ) print(paste0(\u0026quot;Read file \u0026quot;, all.files[i], \u0026#39; !\u0026#39;)) } ## [1] \u0026quot;Read file /achalneupane/data/blob/master/seedA.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/seedB.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/seedC.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/seedD.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/harvestA.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/harvestB.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/harvestC.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/harvestD.csv !\u0026quot; # head(harvestA) # head(harvestB) # and so on.. Here, I am using get_question_1_and_2_answers to extract date information by looping over all data files we just read above.\n# make a table of date range range_table \u0026lt;- {} one.hour.time.difference \u0026lt;- data.frame() for (i in 1:length(all.files)) { # file.dat \u0026lt;- eval(sub(\u0026quot;.csv\u0026quot;, \u0026quot;\u0026quot;, all.files, fixed = TRUE)[i]) file.dat \u0026lt;- eval(gsub(\u0026quot;.*master/|.csv\u0026quot;, \u0026quot;\u0026quot;, all.files)[i]) # print(paste0(\u0026quot;Doing time thing for \u0026quot;, file.dat, \u0026quot;...\u0026quot;)) dat \u0026lt;- get_question_1_and_2_answers(eval(parse(text = file.dat))) tmp.range.of.days \u0026lt;- cbind( data = file.dat, range_from_start_to_end_date_in_days = dat$range_from_start_to_end_date_in_days, elapsed.dates.each.field = dat$elapsed.dates.each.field, range_of_start_end_date = dat$range_of_start_end_date, time.range.each.day = dat$time.range.each.day, time.elapsed.each.day = dat$time.elapsed.each.day ) range_table \u0026lt;- rbind(range_table, tmp.range.of.days) tmp.one.hour.time.difference \u0026lt;- dat$one_hour_gap_time_stamp tmp.one.hour.time.difference$field \u0026lt;- file.dat if (dim(as.data.frame(tmp.one.hour.time.difference))[1] \u0026gt;= 2) { one.hour.time.difference \u0026lt;- rbind.fill(one.hour.time.difference, tmp.one.hour.time.difference) } else{ one.hour.time.difference \u0026lt;- one.hour.time.difference } } range_table \u0026lt;- as.data.frame(range_table) range_table$days_number \u0026lt;- as.numeric(gsub( \u0026#39; days|day\u0026#39;, \u0026quot;\u0026quot;, range_table$range_from_start_to_end_date_in_days )) # Range table range_table ## data range_from_start_to_end_date_in_days elapsed.dates.each.field ## 1 seedA 0.117 day 0 ## 2 seedB 0.352 day 1 ## 3 seedB 0.352 day 1 ## 4 seedC 0.17 day 1 ## 5 seedC 0.17 day 1 ## 6 seedD 0.22 day 0 ## 7 harvestA 18.824 days 19 ## 8 harvestA 18.824 days 19 ## 9 harvestA 18.824 days 19 ## 10 harvestB 0.363 day 0 ## 11 harvestC 1.085 days 1 ## 12 harvestC 1.085 days 1 ## 13 harvestD 43.214 days 43 ## 14 harvestD 43.214 days 43 ## 15 harvestD 43.214 days 43 ## range_of_start_end_date time.range.each.day.Date ## 1 2018-05-17 to 2018-05-17 2018-05-17 ## 2 2018-05-20 to 2018-05-21 2018-05-20 ## 3 2018-05-20 to 2018-05-21 2018-05-21 ## 4 2018-05-18 to 2018-05-19 2018-05-18 ## 5 2018-05-18 to 2018-05-19 2018-05-19 ## 6 2018-05-20 to 2018-05-20 2018-05-20 ## 7 2018-10-27 to 2018-11-15 2018-10-27 ## 8 2018-10-27 to 2018-11-15 2018-10-28 ## 9 2018-10-27 to 2018-11-15 2018-11-15 ## 10 2018-11-02 to 2018-11-02 2018-11-02 ## 11 2018-11-11 to 2018-11-12 2018-11-11 ## 12 2018-11-11 to 2018-11-12 2018-11-12 ## 13 2018-10-01 to 2018-11-13 2018-10-01 ## 14 2018-10-01 to 2018-11-13 2018-11-12 ## 15 2018-10-01 to 2018-11-13 2018-11-13 ## time.range.each.day.Time time.elapsed.each.day.Date ## 1 15:24:19 to 18:13:06 2018-05-17 ## 2 18:50:11 to 21:53:11 2018-05-20 ## 3 02:41:49 to 03:16:59 2018-05-21 ## 4 22:24:15 to 23:59:59 2018-05-18 ## 5 00:00:00 to 02:29:27 2018-05-19 ## 6 13:20:08 to 18:36:20 2018-05-20 ## 7 21:07:30 to 23:06:33 2018-10-27 ## 8 16:22:46 to 22:13:25 2018-10-28 ## 9 16:31:55 to 16:53:40 2018-11-15 ## 10 15:01:21 to 23:43:43 2018-11-02 ## 11 15:34:34 to 23:59:59 2018-11-11 ## 12 00:00:00 to 17:37:01 2018-11-12 ## 13 17:04:25 to 17:32:04 2018-10-01 ## 14 17:42:25 to 23:55:58 2018-11-12 ## 15 15:10:12 to 22:11:56 2018-11-13 ## time.elapsed.each.day.Time_elapsed (minutes) days_number ## 1 168.78665 0.117 ## 2 183.00582 0.352 ## 3 35.17333 0.352 ## 4 95.73995 0.170 ## 5 149.45087 0.170 ## 6 316.21000 0.220 ## 7 119.06642 18.824 ## 8 350.63592 18.824 ## 9 21.75020 18.824 ## 10 522.35150 0.363 ## 11 505.40003 1.085 ## 12 1057.01712 1.085 ## 13 27.65013 43.214 ## 14 373.55237 43.214 ## 15 421.71987 43.214 This is the range table for all fields. I have calculated the range for total elapsed time (in terms of days) in range_from_start_to_end_date_in_days column, total dates elapsed in elapsed.dates.each.field column, Range in dates in range_of_start_end_date column, time range each day (from start to end time) in time.range.each.day.Time column, total time spent in the field each date in time.elapsed.each.day.Time_elapsed (minutes) column.\n# This is the list of consecutive rows with more than one hour time gap from all files: one.hour.time.difference ## X Timestamp LonM LatM Moisture DISTANCE ## 1 27840 2018-11-12T01:51:27.071Z 208.29634 396.43056 15.33 1.075304 ## 2 27849 2018-11-12T16:42:23.002Z 82.56309 62.87237 15.60 5.872164 ## VRYIELDVOL Date Time date_time field ## 1 133.3961 2018-11-12 01:51:27.071 2018-11-12 01:51:27 harvestC ## 2 119.6537 2018-11-12 16:42:23.002 2018-11-12 16:42:23 harvestC I was able to find more than an hour gap in consecutive rows in harvestC data only\n# Plotting the total number of days for each field type ggplot(range_table, aes(x = data, y = days_number)) + geom_point(size = 4) + ggtitle(\u0026quot;Plot in terms of elapsed time- \\n start to end (in days)\u0026quot;) + theme_bw() + theme( axis.line = element_line(colour = \u0026quot;black\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank() ) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(text = element_text(size = 14)) + scale_y_continuous(breaks = round(c( 1, max(range_table$days_number) * 1 / 3, max(range_table$days_number) * 2 / 3, max(range_table$days_number) ), 2)) + # to check if any of the data were planted/harvested within one day range geom_hline(yintercept = 1) + geom_text((aes(3, 1, label = \u0026quot;Within 1 day (24 hours margin)\u0026quot;, vjust = -1))) Based on this plot, we can see that SeedA, seedB, seedC and seedD were all planted within 24 hours span. We can also check the range_table above to get the exact timeframe for this. However, if want to see if they were planted or harvested on the same date, then we can plot this figure below:\n# If we want to see whether they were planted or harvested the same day (i.e # Date) and (not necessarily 24 hours margin), we choose range_table$elapsed.dates.each.field # Now plot them ggplot(range_table, aes(x = data, y = elapsed.dates.each.field)) + geom_point(size = 2) + theme_bw() + ggtitle(\u0026quot;Plot in terms of elapsed dates\u0026quot;) + theme(axis.line = element_line(colour = \u0026quot;black\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank()) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(text = element_text(size=14)) + # to check if any of the data were planted/harvested the same \u0026quot;Date\u0026quot; geom_hline(yintercept = 1) + geom_hline(yintercept = 0, color=\u0026quot;blue\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_text((aes(3, 1, label = \u0026quot;Margin outside the \\\u0026quot;same\\\u0026quot; date\u0026quot;, vjust = -1))) This plot above shows that seedA and seedD were planted on the same date, and harvestB was harvested also on the same date. Rest of the field data were harvested/planted for multiple dates. Bubbles within the black solid line indicates that the fields were, harvested/planted the same date. Only seedA, seedD and harvestB are within the solid line. Here, Y axis margin of 1 means next date. Data within the black and dashed blue lines indicate the same date.\nAdditionally, if we want to see how many hours were spent on each field in total, we can plot this below:\nplot.new \u0026lt;- range_table[, c(\u0026quot;data\u0026quot;, \u0026quot;time.elapsed.each.day.Time_elapsed (minutes)\u0026quot;, \u0026quot;time.elapsed.each.day.Date\u0026quot;)] plot.new$time.elapsed.each.day.Time_elapsed \u0026lt;- as.numeric(plot.new$`time.elapsed.each.day.Time_elapsed (minutes)`) plot.new$time.elapsed.each.day.Time_elapsed.in.hour \u0026lt;- plot.new$time.elapsed.each.day.Time_elapsed/60 plot.new.data \u0026lt;- setNames(aggregate(plot.new$time.elapsed.each.day.Time_elapsed.in.hour, list(range_table$data), sum), c(\u0026quot;group\u0026quot;, \u0026quot;time.spent.in.hours\u0026quot;)) ggplot(plot.new.data, aes(x = group, y = time.spent.in.hours)) + geom_point(size = 4) + theme_bw() + ggtitle(\u0026quot;Plot in terms of total time spent in hours\u0026quot;) + theme(axis.line = element_line(colour = \u0026quot;black\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank()) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(text = element_text(size=14))  Based on this above plot, it looks like only harvestC and harvestD had more than 10 hours spent on them.\nNow, we can also plot start and end time for everyday record:\nrange_table$start_date_time \u0026lt;- as.POSIXct(paste( range_table$time.range.each.day.Date , gsub(\u0026quot;to.*| \u0026quot;, \u0026quot;\u0026quot;, range_table$time.range.each.day.Time) ), tz = \u0026#39;UTC\u0026#39;) range_table$end_date_time \u0026lt;- as.POSIXct(paste( range_table$time.range.each.day.Date , gsub(\u0026quot;.*to| \u0026quot;, \u0026quot;\u0026quot;, range_table$time.range.each.day.Time) ), tz = \u0026#39;UTC\u0026#39;) tt \u0026lt;- range_table[, c(\u0026quot;data\u0026quot;, \u0026quot;start_date_time\u0026quot;, \u0026quot;end_date_time\u0026quot;)] tt \u0026lt;- melt(tt, id = \u0026quot;data\u0026quot;) # Plot for date and time ggplot(tt, aes( x = data, y = as.character.Date(value), colour = variable )) + geom_point(size = 4) + ylab(\u0026quot;Dates and Time\u0026quot;) + theme_bw() + ggtitle(\u0026quot;Start and end time for everyday record\u0026quot;) + theme( axis.line = element_line(colour = \u0026quot;black\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank() ) + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 14)) + theme(axis.text.y = element_text(angle = 0, hjust = 1, size = 8)) + # theme(text = element_text(size = 14)) + geom_hline(yintercept = 1:length(tt$value), linetype = \u0026quot;dotted\u0026quot;) + labs(color = \u0026#39;Each Day\u0026#39;) + scale_color_manual(labels = c(\u0026quot;START_time\u0026quot;, \u0026quot;END_time\u0026quot;), values = c(\u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;))  This plot also shows similar result as with previous plot, but with both start and end time.\nWe can also check the moisture level differences in the fields by date and times.\n# Now, let\u0026#39;s work on the effect of time/dates on moiture level tt1 \u0026lt;- harvestA tt2 \u0026lt;- harvestB tt3 \u0026lt;- harvestC tt4 \u0026lt;- harvestD tt1$type \u0026lt;- \u0026quot;harvestA\u0026quot; tt2$type \u0026lt;- \u0026quot;harvestB\u0026quot; tt3$type \u0026lt;- \u0026quot;harvestC\u0026quot; tt4$type \u0026lt;- \u0026quot;harvestD\u0026quot; make_posixct \u0026lt;- function(x) { x$Date \u0026lt;- gsub(\u0026quot;T.*\u0026quot;, \u0026quot;\u0026quot;, x$Timestamp) x$Date \u0026lt;- as.factor(x$Date) x$Time \u0026lt;- gsub(\u0026quot;.*T|Z\u0026quot;, \u0026quot;\u0026quot;, x$Timestamp) n \u0026lt;- 2 pat \u0026lt;- paste0(\u0026#39;^([^:]+(?::[^:]+){\u0026#39;, n - 1, \u0026#39;}).*\u0026#39;) x$Time \u0026lt;- as.factor(sub(pat, \u0026#39;\\\\1\u0026#39;, x$Time)) return(x) } tt1 \u0026lt;- make_posixct(tt1) tt2 \u0026lt;- make_posixct(tt2) tt3 \u0026lt;- make_posixct(tt3) tt4 \u0026lt;- make_posixct(tt4) colnames(tt2) \u0026lt;- colnames(tt1) df \u0026lt;- rbind(tt1, tt2, tt3, tt4) df$Time \u0026lt;- as.POSIXct(strptime(df$Time, format = \u0026quot;%H:%M\u0026quot;, tz = \u0026quot;UTC\u0026quot;)) lims \u0026lt;- as.POSIXct(strptime(c(\u0026quot;0:00\u0026quot;, \u0026quot;23:59\u0026quot;), format = \u0026quot;%H:%M\u0026quot;, tz = \u0026quot;UTC\u0026quot;)) p \u0026lt;- ggplot(data = df, aes( x = Time, y = Moisture, colour = as.factor(Date) )) + geom_point() + scale_x_datetime( limits = lims, breaks = date_breaks(\u0026quot;2 hour\u0026quot;), labels = date_format(\u0026quot;%H:%M\u0026quot;, tz = \u0026quot;UTC\u0026quot;) ) + labs(color = \u0026#39;Dates\u0026#39;) + facet_grid(. ~ type) p + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) Based on this figure above, we can see that the moisture content is very low from midnight to 2:00pm, but then after, it increases substantially. I don’t think there is so much difference in moisture in terms of harvest dates though.\nI tried to pull weather data for each observation, however the api I used only allows 1000 requests per day, so I did not include that data.\n# We can use darksky like api\u0026#39;s to pull weather data, however it\u0026#39;s not # possible as we have so many observations. Only allows 1000 downloads per day. # install.packages(\u0026quot;darksky\u0026quot;) library(darksky) darksky_api_key(force = TRUE) #API: 60b8ca6aaece606b21c0dfec1cb7ec9f head(seedA) # field_or_seed \u0026lt;- seedA tmp \u0026lt;- get_forecast_for(10.25209, 478.4000, \u0026quot;2013-05-06T12:00:00\u0026quot;) # Create empty columns and cbind to the field data so you can add new weather data field_or_seed \u0026lt;- cbind(field_or_seed, setNames(lapply(get.weather.cols, function(x) x = NA), get.weather.cols)) field_or_seed$Date \u0026lt;- gsub(\u0026quot;T.*\u0026quot;, \u0026quot;\u0026quot;, field_or_seed$Timestamp) field_or_seed$Time \u0026lt;- gsub(\u0026quot;.*T|Z\u0026quot;, \u0026quot;\u0026quot;, field_or_seed$Timestamp) # Range in days field_or_seed$date_time \u0026lt;- as.POSIXct(paste(field_or_seed$Date, field_or_seed$Time), tz = \u0026quot;UTC\u0026quot;) field_or_seed_with_weather \u0026lt;- {} for (i in 1:nrow(field_or_seed)) { print(paste0(\u0026quot;My iteration is: \u0026quot;, i)) tmp.min \u0026lt;- get_forecast_for(44.311356, -96.798386, min(field_or_seed$date_time[i])) tmp.weather.data \u0026lt;- cbind(field_or_seed[i,], tmp.min$daily) field_or_seed_with_weather \u0026lt;- rbind(field_or_seed_with_weather, tmp.weather.data) } Additional work:\nLet’s read our csv files from midterm\n# # If using local path # setwd(path.midterm) # all.files \u0026lt;- list.files(path.midterm) # here, instead, I am just using the URL all.files \u0026lt;- path.midterm all.files \u0026lt;- all.files[grepl(\u0026quot;*.csv\u0026quot;, all.files)] # \u0026gt; all.files # [1] \u0026quot;fieldA.csv\u0026quot; \u0026quot;fieldB.csv\u0026quot; \u0026quot;fieldC.csv\u0026quot; \u0026quot;fieldD.csv\u0026quot; all.files \u0026lt;- all.files[grepl(\u0026quot;fieldA.csv|fieldB.csv|fieldC.csv|fieldD.csv\u0026quot;, all.files)] # reading all csv files within the path and saving them with their respective # names # https://raw.githubusercontent.com/achalneupane/data/master/ # reading all csv files within the path and saving them to their respective names. for (i in 1:length(all.files)) { assign( gsub(\u0026quot;.*master/|.csv\u0026quot;, \u0026quot;\u0026quot;, all.files)[i], read.table(gsub(\u0026quot;blob/\u0026quot;, \u0026quot;\u0026quot;,paste0(\u0026quot;https://raw.githubusercontent.com\u0026quot;,all.files[i])), header = TRUE, sep = \u0026quot;,\u0026quot;) ) print(paste0(\u0026quot;Read file \u0026quot;, all.files[i], \u0026#39; !\u0026#39;)) } ## [1] \u0026quot;Read file /achalneupane/data/blob/master/fieldA.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/fieldB.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/fieldC.csv !\u0026quot; ## [1] \u0026quot;Read file /achalneupane/data/blob/master/fieldD.csv !\u0026quot;  # Now, let\u0026#39;s merge field and seed data # head(fieldA) # head(seedA) dataA \u0026lt;- merge( fieldA, seedA, by.x = c(\u0026quot;Easting\u0026quot;, \u0026quot;Northing\u0026quot;), by.y = c(\u0026quot;LonM\u0026quot;, \u0026quot;LatM\u0026quot;) ) colnames(dataA) \u0026lt;- gsub(\u0026quot;.x\u0026quot;, \u0026quot;\u0026quot;, colnames(dataA)) dataB \u0026lt;- merge( fieldB, seedB, by.x = c(\u0026quot;Easting\u0026quot;, \u0026quot;Northing\u0026quot;), by.y = c(\u0026quot;LonM\u0026quot;, \u0026quot;LatM\u0026quot;) ) colnames(dataB) \u0026lt;- gsub(\u0026quot;.x\u0026quot;, \u0026quot;\u0026quot;, colnames(dataB)) dataC \u0026lt;- merge( fieldC, seedC, by.x = c(\u0026quot;Easting\u0026quot;, \u0026quot;Northing\u0026quot;), by.y = c(\u0026quot;LonM\u0026quot;, \u0026quot;LatM\u0026quot;) ) colnames(dataC) \u0026lt;- gsub(\u0026quot;.x\u0026quot;, \u0026quot;\u0026quot;, colnames(dataC)) dataD \u0026lt;- merge( fieldD, seedD, by.x = c(\u0026quot;Easting\u0026quot;, \u0026quot;Northing\u0026quot;), by.y = c(\u0026quot;LonM\u0026quot;, \u0026quot;LatM\u0026quot;) ) colnames(dataD) \u0026lt;- gsub(\u0026quot;.x\u0026quot;, \u0026quot;\u0026quot;, colnames(dataD)) # Taking 1000 ControlRates interval, we will also compare different dates (i.e, # ControlRate and date pairs) for each merged seed and field data. ControlRateInterval \u0026lt;- 1000 # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataA, intervals = ControlRateInterval) # fieldA.data # field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataB, intervals = ControlRateInterval) # fieldB.data # From midterm for fieldB, we saw significant difference between all Control Rates of # Seeding except between 28000 Vs 29000. The seeding rates of 28000 has the # highest Mean yield here. # field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataC, intervals = ControlRateInterval) # fieldC.data # From midterm for fieldC, we saw all Control rates of seeding are significantly diffferent with the highest Mean yeild for 28000. # field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataD, intervals = ControlRateInterval) # fieldD.data Based on my analysis from midterm, we saw that it is better to compare all four fields together. Also, I do not want to clutter this report with too many figures and tables, I am analyzing 1000 Control Rate and all four fields merged together. This way we can compare the skewed data points if we analyze with date and ControlRate pairs. I have done this analysis below\nWe can now merge all four fields for SD, means and ControlRate level counts.\n merged.SD.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$fieldSD, fieldB.data$fieldSD, fieldC.data$fieldSD, fieldD.data$fieldSD ) ) merged.Mean.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$field.mean, fieldB.data$field.mean, fieldC.data$field.mean, fieldD.data$field.mean ) ) merged.Count.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$field.Count, fieldB.data$field.Count, fieldC.data$field.Count, fieldD.data$field.Count ) ) We need SD pooled for these levels so we can calculate Cohen’s d and Required Replicates.\n levels(merged.SD.4.plots$CR.Date.Levels) ## [1] \u0026quot;23000*2018-05-17\u0026quot; \u0026quot;24000*2018-05-17\u0026quot; \u0026quot;25000*2018-05-17\u0026quot; ## [4] \u0026quot;26000*2018-05-17\u0026quot; \u0026quot;27000*2018-05-17\u0026quot; \u0026quot;28000*2018-05-17\u0026quot; ## [7] \u0026quot;29000*2018-05-17\u0026quot; \u0026quot;24000*2018-05-20\u0026quot; \u0026quot;25000*2018-05-20\u0026quot; ## [10] \u0026quot;25000*2018-05-21\u0026quot; \u0026quot;26000*2018-05-20\u0026quot; \u0026quot;26000*2018-05-21\u0026quot; ## [13] \u0026quot;27000*2018-05-20\u0026quot; \u0026quot;27000*2018-05-21\u0026quot; \u0026quot;28000*2018-05-20\u0026quot; ## [16] \u0026quot;28000*2018-05-21\u0026quot; \u0026quot;29000*2018-05-20\u0026quot; \u0026quot;29000*2018-05-21\u0026quot; ## [19] \u0026quot;23000*2018-05-18\u0026quot; \u0026quot;24000*2018-05-18\u0026quot; \u0026quot;25000*2018-05-18\u0026quot; ## [22] \u0026quot;26000*2018-05-18\u0026quot; \u0026quot;26000*2018-05-19\u0026quot; \u0026quot;27000*2018-05-18\u0026quot; ## [25] \u0026quot;27000*2018-05-19\u0026quot; \u0026quot;28000*2018-05-18\u0026quot; \u0026quot;28000*2018-05-19\u0026quot; ## [28] \u0026quot;23000*2018-05-20\u0026quot;  # First, we merged all merged.SD.4.plots, merged.Mean.4.plots and # merged.Count.4.plots from all fields. # Therefore, now merging all three dataframes from all four plots for SD, Mean # and counts by ControlRate column Mean_SD_Count.dat \u0026lt;- Reduce(function(...) merge(..., by = c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;grp\u0026quot;), all.x = TRUE), lapply( list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots), transform, grp = ave(seq_along(CR.Date.Levels), CR.Date.Levels, FUN = seq_along) )) # head(Mean_SD_Count.dat) # Now, we can also do Anova on the merged data `Mean_SD_Count.dat` Mean_SD_Count_merged_for_all_four_plots \u0026lt;- Mean_SD_Count.dat Mean_SD_Count_merged_for_all_four_plots$Yield \u0026lt;- Mean_SD_Count_merged_for_all_four_plots$Mean Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels \u0026lt;- Mean_SD_Count_merged_for_all_four_plots$ControlRate get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = \u0026quot;all four fields\u0026quot;) Based on this plot above, from all fields, we can tell that, Control Rates of seeding of 23000-25000 has significantly less Mean yield as compared to 26000-29000. It looks like Control Rates of Seeding between 27000 and 28000 has the highest Mean yield. I also think that there is no effect on seeding dates, I have done some analysis to show the seeding effect only in the last part of this report and based on that as well, I do not think there is significant effect of Dates on Yield.\nWe now calculate pooled SD for merged 4 plots Mean_SD_Count.dat to calculate Effect Size and Required Replicates for all fields (combined analysis) using pooled SD.\npooled.dat \u0026lt;- Mean_SD_Count.dat # # Pooled sd can be calculated as: pooled.dat$df \u0026lt;- pooled.dat$Count - 1 # pooled SD is : # pooledSD \u0026lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) ) # We can calculate our SD pooled using this formula: # s_{pooled} = \\sqrt{\\frac{\\sum_i (n_i-1)s_i^2}{N-k}} # We will derrive this in steps as below: pooled.dat$df \u0026lt;- pooled.dat$Count - 1 pooled.dat$sd.square \u0026lt;- pooled.dat$SD ^ 2 pooled.dat$ss \u0026lt;- pooled.dat$sd.square * pooled.dat$df # We can use convenience function (aggregate) for splitting and calculating # the necessary sums. ds \u0026lt;- aggregate(ss ~ CR.Date.Levels, data = pooled.dat, sum) # Two different built-in methods for split apply, we could use aggregate for # both if we wanted. This calculates our degrees of freedom. ds$df \u0026lt;- tapply(pooled.dat$df, pooled.dat$CR.Date.Levels, sum) # divide ss by df and then we get sd square ds$sd.square \u0026lt;- ds$ss / ds$df # Finally, we can get our sd pooled ds$SD_pooled \u0026lt;- sqrt(ds$sd.square) # ds # However, we could also calculate our sd_pooled as below and get the same results : # sd_pooled \u0026lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$CR.Date.Levels), # function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) ) # Now, we calculate Mean (Mean of Means) from the merged table # `Mean_SD_Count.dat so we can calculate Cohens d and RequiredReplicates for all # four field combined. ds.Mean \u0026lt;- setNames(aggregate( Mean_SD_Count.dat$Mean, by = list(Mean_SD_Count.dat$CR.Date.Levels), FUN = mean ), c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;Mean\u0026quot;)) ds \u0026lt;- merge(ds, ds.Mean, by.x = \u0026quot;CR.Date.Levels\u0026quot;) # ds # Now we calculate the Effect Size and Cohen\u0026#39;s D for the combined 4 plots using # mean yield and sd pooled for different ControlRate # Now we calculate the Effect Size and Cohen\u0026#39;s D for the combined 4 plots using # mean yield and sd pooled for different ControlRate usinf our function RequiredReplicates_for_all_fields \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = ds, intervals = ControlRateInterval, Single.Field.Analysis = FALSE ) This table gives us the required replicates and effectSize for all ControlRates-Date pairs\nRequiredReplicates_for_all_fields ## $Req.Rep.table.field ## Group EffectSize ## [1,] \u0026quot;23000*2018-05-17 Vs 23000*2018-05-18\u0026quot; \u0026quot;5.76937088213814\u0026quot; ## [2,] \u0026quot;23000*2018-05-18 Vs 23000*2018-05-20\u0026quot; \u0026quot;1.77467806243082\u0026quot; ## [3,] \u0026quot;23000*2018-05-20 Vs 24000*2018-05-17\u0026quot; \u0026quot;0.319085268435178\u0026quot; ## [4,] \u0026quot;24000*2018-05-17 Vs 24000*2018-05-18\u0026quot; \u0026quot;2.23695386227101\u0026quot; ## [5,] \u0026quot;24000*2018-05-18 Vs 24000*2018-05-20\u0026quot; \u0026quot;1.4793489792455\u0026quot; ## [6,] \u0026quot;24000*2018-05-20 Vs 25000*2018-05-17\u0026quot; \u0026quot;0.764820441835373\u0026quot; ## [7,] \u0026quot;25000*2018-05-17 Vs 25000*2018-05-18\u0026quot; \u0026quot;0.749715759825526\u0026quot; ## [8,] \u0026quot;25000*2018-05-18 Vs 25000*2018-05-20\u0026quot; \u0026quot;0.631404233619908\u0026quot; ## [9,] \u0026quot;25000*2018-05-20 Vs 25000*2018-05-21\u0026quot; \u0026quot;0.0122749797874684\u0026quot; ## [10,] \u0026quot;25000*2018-05-21 Vs 26000*2018-05-17\u0026quot; \u0026quot;1.86649999718639\u0026quot; ## [11,] \u0026quot;26000*2018-05-17 Vs 26000*2018-05-18\u0026quot; \u0026quot;0.604237639607718\u0026quot; ## [12,] \u0026quot;26000*2018-05-18 Vs 26000*2018-05-19\u0026quot; \u0026quot;0.335012013191533\u0026quot; ## [13,] \u0026quot;26000*2018-05-19 Vs 26000*2018-05-20\u0026quot; \u0026quot;0.294694932900609\u0026quot; ## [14,] \u0026quot;26000*2018-05-20 Vs 26000*2018-05-21\u0026quot; \u0026quot;0.0568859046090692\u0026quot; ## [15,] \u0026quot;26000*2018-05-21 Vs 27000*2018-05-17\u0026quot; \u0026quot;0.450821573333859\u0026quot; ## [16,] \u0026quot;27000*2018-05-17 Vs 27000*2018-05-18\u0026quot; \u0026quot;0.227990733240137\u0026quot; ## [17,] \u0026quot;27000*2018-05-18 Vs 27000*2018-05-19\u0026quot; \u0026quot;0.212920132008121\u0026quot; ## [18,] \u0026quot;27000*2018-05-19 Vs 27000*2018-05-20\u0026quot; \u0026quot;0.186994200485925\u0026quot; ## [19,] \u0026quot;27000*2018-05-20 Vs 27000*2018-05-21\u0026quot; \u0026quot;0.312513439276243\u0026quot; ## [20,] \u0026quot;27000*2018-05-21 Vs 28000*2018-05-17\u0026quot; \u0026quot;0.296577835810804\u0026quot; ## [21,] \u0026quot;28000*2018-05-17 Vs 28000*2018-05-18\u0026quot; \u0026quot;0.114094426452159\u0026quot; ## [22,] \u0026quot;28000*2018-05-18 Vs 28000*2018-05-19\u0026quot; \u0026quot;2.35237654983648\u0026quot; ## [23,] \u0026quot;28000*2018-05-19 Vs 28000*2018-05-20\u0026quot; \u0026quot;0.182678706582559\u0026quot; ## [24,] \u0026quot;28000*2018-05-20 Vs 28000*2018-05-21\u0026quot; \u0026quot;0.703554320633904\u0026quot; ## [25,] \u0026quot;28000*2018-05-21 Vs 29000*2018-05-17\u0026quot; \u0026quot;0.983655196454225\u0026quot; ## [26,] \u0026quot;29000*2018-05-17 Vs 29000*2018-05-20\u0026quot; \u0026quot;0.927565354976892\u0026quot; ## [27,] \u0026quot;29000*2018-05-20 Vs 29000*2018-05-21\u0026quot; \u0026quot;0.341504648673164\u0026quot; ## RequiredReplicates ## [1,] \u0026quot;0\u0026quot; ## [2,] \u0026quot;5\u0026quot; ## [3,] \u0026quot;154\u0026quot; ## [4,] \u0026quot;3\u0026quot; ## [5,] \u0026quot;7\u0026quot; ## [6,] \u0026quot;27\u0026quot; ## [7,] \u0026quot;28\u0026quot; ## [8,] \u0026quot;39\u0026quot; ## [9,] \u0026quot;104183\u0026quot; ## [10,] \u0026quot;5\u0026quot; ## [11,] \u0026quot;43\u0026quot; ## [12,] \u0026quot;140\u0026quot; ## [13,] \u0026quot;181\u0026quot; ## [14,] \u0026quot;4851\u0026quot; ## [15,] \u0026quot;77\u0026quot; ## [16,] \u0026quot;302\u0026quot; ## [17,] \u0026quot;346\u0026quot; ## [18,] \u0026quot;449\u0026quot; ## [19,] \u0026quot;161\u0026quot; ## [20,] \u0026quot;178\u0026quot; ## [21,] \u0026quot;1206\u0026quot; ## [22,] \u0026quot;3\u0026quot; ## [23,] \u0026quot;470\u0026quot; ## [24,] \u0026quot;32\u0026quot; ## [25,] \u0026quot;16\u0026quot; ## [26,] \u0026quot;18\u0026quot; ## [27,] \u0026quot;135\u0026quot; We can plot this results\nRequiredReplicates_for_all_fields \u0026lt;- as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field) RequiredReplicates_for_all_fields$Group \u0026lt;- factor(RequiredReplicates_for_all_fields$Group) RequiredReplicates_for_all_fields$RequiredReplicates \u0026lt;- as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates)) RequiredReplicates_for_all_fields$EffectSize \u0026lt;- as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize)) # There are one or two extreme Required replicates and extreme Effect size. We can # remove them to plot them, so we can properly visualize the datapoints RequiredReplicates_for_all_fields \u0026lt;- RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$RequiredReplicates \u0026lt; 4000, ] RequiredReplicates_for_all_fields \u0026lt;- RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$EffectSize \u0026lt; 1.5, ] my_levels \u0026lt;- length(levels(RequiredReplicates_for_all_fields$Group)) RequiredReplicates_for_all_fields$EffectSize \u0026lt;- round(RequiredReplicates_for_all_fields$EffectSize, 2) # scaleFUN \u0026lt;- function(x) sprintf(\u0026quot;%.2f\u0026quot;, x) # Plot RequiredReplicates_for_all_fields n \u0026lt;- my_levels qual_col_pals = brewer.pal.info[brewer.pal.info$category == \u0026#39;qual\u0026#39;, ] col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals))) ggplot( RequiredReplicates_for_all_fields, aes( x = EffectSize, y = RequiredReplicates, shape = as.factor(Group), group = 1 ) ) + ggtitle(\u0026quot;Effect size Vs Required replicates \\n for all fields data \\n(Combined Effect)\u0026quot;) + geom_point(alpha = 0.9, size = 3, stroke = 1) + scale_shape_manual(values = rep(c(0:2, 5:6, 9:10, 11:12, 14), times = 4)) + scale_color_manual(values = col_vector) + geom_line(color = \u0026quot;black\u0026quot;) + scale_x_continuous(\u0026quot;EffectSize\u0026quot;, breaks = c(0, 0.2, 0.5, 1, max( as.numeric(RequiredReplicates_for_all_fields$EffectSize) ))) + theme_bw() + theme( axis.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 10), plot.title = element_text(hjust = 0.5, face = \u0026quot;bold\u0026quot;), axis.line = element_line(colour = \u0026quot;black\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank() ) + labs(shape = \u0026quot;Comparison groups\u0026quot;) Based on this plot above, we can tell that there is strong negative correlation between EffectSize and RequiredReplicates.\nSo far, we analyzed our data for controlRates-Date pair, now we can also do the same analysis for Date effect only.\n######################## Date Only Analysis !! ############################## # If we want to do Date effect only without taking ControlRates into consideration, then we do the following: # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataA, intervals = ControlRateInterval, Date.Only = TRUE) # fieldA.data # field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataB, intervals = ControlRateInterval, Date.Only = TRUE) # fieldB.data # field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataC, intervals = ControlRateInterval, Date.Only = TRUE) # fieldC.data # field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize(field = dataD, intervals = ControlRateInterval, Date.Only = TRUE) # fieldD.data # We can now merge all four fields for SD, means and ControlRate level counts. merged.SD.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$fieldSD, fieldB.data$fieldSD, fieldC.data$fieldSD, fieldD.data$fieldSD ) ) merged.Mean.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$field.mean, fieldB.data$field.mean, fieldC.data$field.mean, fieldD.data$field.mean ) ) merged.Count.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$field.Count, fieldB.data$field.Count, fieldC.data$field.Count, fieldD.data$field.Count ) ) # We need SD pooled for these levels so we can calculate Cohen\u0026#39;s d and # Required Replicates. levels(merged.SD.4.plots$CR.Date.Levels) ## [1] \u0026quot;2018-05-17\u0026quot; \u0026quot;2018-05-20\u0026quot; \u0026quot;2018-05-21\u0026quot; \u0026quot;2018-05-18\u0026quot; \u0026quot;2018-05-19\u0026quot; # First, we merged all merged.SD.4.plots, merged.Mean.4.plots and # merged.Count.4.plots from all fields. # Therefore, now merging all three dataframes from all four plots for SD, Mean # and counts by ControlRate column Mean_SD_Count.dat \u0026lt;- Reduce(function(...) merge(..., by = c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;grp\u0026quot;), all.x = TRUE), lapply( list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots), transform, grp = ave(seq_along(CR.Date.Levels), CR.Date.Levels, FUN = seq_along) )) head(Mean_SD_Count.dat) ## CR.Date.Levels grp Mean SD Count ## 1 2018-05-17 1 234.0993 40.74205 6710 ## 2 2018-05-18 1 222.4764 20.50457 3880 ## 3 2018-05-19 1 229.3088 10.26760 6524 ## 4 2018-05-20 1 229.8301 26.90213 7747 ## 5 2018-05-20 2 230.8569 30.22076 12654 ## 6 2018-05-21 1 226.4394 19.87301 1574 # Now, we can also do Anova on the merged data `Mean_SD_Count.dat` Mean_SD_Count_merged_for_all_four_plots \u0026lt;- Mean_SD_Count.dat Mean_SD_Count_merged_for_all_four_plots$Yield \u0026lt;- Mean_SD_Count_merged_for_all_four_plots$Mean Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels \u0026lt;- Mean_SD_Count_merged_for_all_four_plots$ControlRate Mean_SD_Count_merged_for_all_four_plots$CR.Date.Levels \u0026lt;- as.factor(gsub( \u0026quot;-\u0026quot;, \u0026quot;_\u0026quot;, Mean_SD_Count_merged_for_all_four_plots$CR.Date.Levels )) get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = \u0026quot;all four fields\u0026quot;) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## field$CR.Date.Levels 4 78.66 19.665 37.3 0.122 ## Residuals 1 0.53 0.527 From this result above, since the residuals from ANOVA model has only 1 degree of freedom, and the global p-value is not significant, we don’t have to plot the TUKEY pairwise test. Instead, we only show the Summary anova. This shows that the date alone doesn’t affect our mean Yield.\nWe now calculate pooled SD for merged 4 plots Mean_SD_Count.dat to calculate Effect Size and Required Replicates for all fields (combined analysis) using pooled SD.\npooled.dat \u0026lt;- Mean_SD_Count.dat # # Pooled sd can be calculated as: pooled.dat$df \u0026lt;- pooled.dat$Count - 1 # pooled SD is : # pooledSD \u0026lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) ) # We can calculate our SD pooled using this formula: # s_{pooled} = \\sqrt{\\frac{\\sum_i (n_i-1)s_i^2}{N-k}} # We will derrive this in steps as below: pooled.dat$df \u0026lt;- pooled.dat$Count - 1 pooled.dat$sd.square \u0026lt;- pooled.dat$SD ^ 2 pooled.dat$ss \u0026lt;- pooled.dat$sd.square * pooled.dat$df # We can use convenience function (aggregate) for splitting and calculating # the necessary sums. ds \u0026lt;- aggregate(ss ~ CR.Date.Levels, data = pooled.dat, sum) # Two different built in methods for split apply, we could use aggregate for # both if we wanted. This calculates our degrees of freedom. ds$df \u0026lt;- tapply(pooled.dat$df, pooled.dat$CR.Date.Levels, sum) # divide ss by df and then we get sd square ds$sd.square \u0026lt;- ds$ss / ds$df # Finally, we can get our sd pooled ds$SD_pooled \u0026lt;- sqrt(ds$sd.square) ds ## CR.Date.Levels ss df sd.square SD_pooled ## 1 2018-05-17 11136365.7 6709 1659.9144 40.74205 ## 2 2018-05-20 17161878.7 20399 841.3098 29.00534 ## 3 2018-05-21 621235.0 1573 394.9364 19.87301 ## 4 2018-05-18 1630876.2 3879 420.4373 20.50457 ## 5 2018-05-19 687678.2 6523 105.4236 10.26760 # However, we could also calculate our sd_pooled as below and get the same results : sd_pooled \u0026lt;- lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$CR.Date.Levels), function(dd) sqrt(sum(dd$SD ^ 2 * (dd$Count - 1)) / (sum(dd$Count - 1) - nrow(dd)))) # Now, we calculate Mean (Mean of Means) from the merged table # `Mean_SD_Count.dat`, so we can calculate Cohens d and RequiredReplicates for # all four field combined. ds.Mean \u0026lt;- setNames(aggregate( Mean_SD_Count.dat$Mean, by = list(Mean_SD_Count.dat$CR.Date.Levels), FUN = mean ), c(\u0026quot;CR.Date.Levels\u0026quot;, \u0026quot;Mean\u0026quot;)) ds \u0026lt;- merge(ds, ds.Mean, by.x = \u0026quot;CR.Date.Levels\u0026quot;) ds ## CR.Date.Levels ss df sd.square SD_pooled Mean ## 1 2018-05-17 11136365.7 6709 1659.9144 40.74205 234.0993 ## 2 2018-05-18 1630876.2 3879 420.4373 20.50457 222.4764 ## 3 2018-05-19 687678.2 6523 105.4236 10.26760 229.3088 ## 4 2018-05-20 17161878.7 20399 841.3098 29.00534 230.3435 ## 5 2018-05-21 621235.0 1573 394.9364 19.87301 226.4394 # Now we calculate the Effect Size and Cohen\u0026#39;s D for the combined 4 plots using # mean yield and sd pooled for different ControlRate # Now we calculate the Effect Size and Cohen\u0026#39;s D for the combined 4 plots using # mean yield and sd pooled for different ControlRate usinf our function # `calculate_field_mean_SD_and_get_RR_EffSize`. RequiredReplicates_for_all_fields \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = ds, intervals = ControlRateInterval, Single.Field.Analysis = FALSE ) RequiredReplicates_for_all_fields ## $Req.Rep.table.field ## Group EffectSize RequiredReplicates ## [1,] \u0026quot;2018-05-17 Vs 2018-05-18\u0026quot; \u0026quot;0.360381603784739\u0026quot; \u0026quot;121\u0026quot; ## [2,] \u0026quot;2018-05-18 Vs 2018-05-19\u0026quot; \u0026quot;0.421358834058387\u0026quot; \u0026quot;88\u0026quot; ## [3,] \u0026quot;2018-05-19 Vs 2018-05-20\u0026quot; \u0026quot;0.047558010269331\u0026quot; \u0026quot;6940\u0026quot; ## [4,] \u0026quot;2018-05-20 Vs 2018-05-21\u0026quot; \u0026quot;0.157027441353185\u0026quot; \u0026quot;637\u0026quot; # We can plot this results for better visualization of pattern. RequiredReplicates_for_all_fields \u0026lt;- as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field) RequiredReplicates_for_all_fields$Group \u0026lt;- factor(RequiredReplicates_for_all_fields$Group) RequiredReplicates_for_all_fields$RequiredReplicates \u0026lt;- as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates)) RequiredReplicates_for_all_fields$EffectSize \u0026lt;- as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize)) # There are one or two extreme Required replicates and extreme Effect size. We can # remove them to plot them, so we can properly visualize the datapoints RequiredReplicates_for_all_fields \u0026lt;- RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$RequiredReplicates \u0026lt; 4000, ] RequiredReplicates_for_all_fields \u0026lt;- RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$EffectSize \u0026lt; 1.5, ] my_levels \u0026lt;- length(levels(RequiredReplicates_for_all_fields$Group)) RequiredReplicates_for_all_fields$EffectSize \u0026lt;- round(RequiredReplicates_for_all_fields$EffectSize, 2) # Plot RequiredReplicates_for_all_fields n \u0026lt;- my_levels qual_col_pals = brewer.pal.info[brewer.pal.info$category == \u0026#39;qual\u0026#39;,] col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals))) ggplot( RequiredReplicates_for_all_fields, aes( x = EffectSize, y = RequiredReplicates, shape = as.factor(Group), group = 1 ) ) + ggtitle(\u0026quot;Effect size Vs Required replicates for all fields data \\n(Combined Effect)\u0026quot;) + geom_point(alpha = 0.9, size = 3, stroke = 1) + scale_shape_manual(values = rep(c(0:2, 5:6, 9:10, 11:12, 14), times = 4)) + scale_color_manual(values = col_vector) + geom_line(color = \u0026quot;black\u0026quot;) + scale_x_continuous(\u0026quot;EffectSize\u0026quot;, breaks = c(0, 0.2, 0.5, 1, max( as.numeric(RequiredReplicates_for_all_fields$EffectSize) ))) + theme_bw() + theme( axis.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 10), plot.title = element_text(hjust = 0.5, face = \u0026quot;bold\u0026quot;), axis.line = element_line(colour = \u0026quot;black\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank() ) + labs(shape = \u0026quot;Comparison groups\u0026quot;) ################################ End of codes !! ################################ Discussion: Based on these results, I think that the fields were planted within 24 hours margin. seedB and seedC were planted on the different dates. Similarly, harvestB was harvested within 24 hours and on the same date, but rest of the fields were harvested/planted for multiple dates. Additionally, I also performed ANOVA for ControlRates paired with dates and found no significant difference to what I found for my midterm. I analyzed these data for Date effect also and it looks like there is no Date effect on Mean Yield. I thought the skewned in 28000 interval from my midterm would be addressed by Date effect, but I could not find significant effect of date here. As in the mid-term, I also calculated the required replicated for ControlRate-Date pairs as well as Date only observations, and in both cases, the EffectSize was inversely proportional to RequiredReplicates.\n","date":1568586383,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568586383,"objectID":"3721dd994fdc910d2041e7415a30122f","permalink":"/achalneupane.github.io/post/final_assignment/","publishdate":"2019-09-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/final_assignment/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Final assignment","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  In this exercise, I will be analyzing four fields as a statistical consultant. This data comes from four different farm fields and we try to investigate the effects of variable seeding rates in overall yeild.\nThe table summarizes data from four corn fields. The top row are target seeding rates, in seeds per acre, the bottom row are corn yields in bushels per acre. This table summarizes all four fields at the Control Rate interval of 1000 which was also reproduced below in my anaysis. More importantly, we are interested in exploring the EffectSize and Required Replicates for properly examining any statistical evidence for the yield and variable seeding rates relationships.\n  Rate 23000 24000 25000 26000 27000 28000 29000    Yield 111.4216 155.0326 181.1176 227.5800 233.4623 242.1753 231.3890    The algorithm to do this analysis is divided into several steps as described below:\nI have used some external packages like ggplot2 to make the plots. I have written several functions to analyze each individual field as well as merged field statistics that calculates cohens effect size, required replicates, and does ANOVA analysis on individual field as well as on merged data from all fields. Function calculate_field_mean_SD_and_get_RR_EffSize also uses mean, sd, and counts from each field to calculate cohens d and Required replicates. It also uses pooled standard deviations and Mean (mean of Means) calculated from all four fields for different control Rates to calculate cohens d and required replicates of combined field data. I have tested this function on all four fields at ControlRate level of 500, 1000, 2000 and 3000 intervals, then based on the results (also merged the output of all four intervals to get the effectSize and Required Replicates compared), I have decided to choose 1000 intervals of ControlRates for further analysis for combined data from all four fields. I have also shown ANOVA analysis followed by TUKEY HSD test to show which ControlRates of seeding have significant effect on Yield for each field and then for all fields (combined all four fields). Lastly, I have plotted EffectSize Vs RequiredReplicates for each field and also for combined data from four fields.  Here, we first install ond load some of the packages (“multcompView”, “ggplot2”, “scales”, “data.table”) I will be using for this exercise.\n# First, install missing packages and load them myPackages \u0026lt;- c(\u0026quot;multcompView\u0026quot;, \u0026quot;ggplot2\u0026quot;, \u0026quot;scales\u0026quot;, \u0026quot;data.table\u0026quot;) my.installed.packages\u0026lt;- installed.packages() available.packages \u0026lt;- myPackages %in% my.installed.packages if (sum(!available.packages) \u0026gt; 0){ install.packages(myPackages[!available.packages]) } # Load all required packages lapply(myPackages, require, character.only = TRUE) ## Loading required package: multcompView ## Loading required package: ggplot2 ## Loading required package: scales ## Loading required package: data.table ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] TRUE We will first write and describe our function here. We will be using these function to manipulate our data, calculate Cohen’s d, required replicates and also do ANOVA and TukeyHSD paired test, then finally to plot our data. We can calculate EffectSize and RequiredReplicates for these data using functions from previous homework cohen.d and required.replicates. These two functions are then called within another function calculate_field_mean_SD_and_get_RR_EffSize that analyzes each individual field and also merged data from all four fields to calculate Effect Size, Required Replictaes and then plot the results.\ncohen.d \u0026lt;- function(m1, s1, m2, s2){ cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) return(cohens_d) } required.replicates \u0026lt;- function (m1, s1, m2, s2, alpha=0.05, beta=0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) return(round(n,0)) } We willl also perform ANOVA analysis with Tukey Test for paired comparision of mean for each field data as well as merged data at different ControlRate intervals. This function does Tukey HSD test and generates label for significant outcomes.\n# Create function to get the labels for Tukey HSD: generate_label_df \u0026lt;- function(TUKEY, variable){ # Extract labels and factor levels from Tukey post-hoc Tukey.levels \u0026lt;- TUKEY[[variable]][,4] Tukey.labels \u0026lt;- data.frame(multcompLetters(Tukey.levels)[\u0026#39;Letters\u0026#39;]) #I need to put the labels in the same order as in the boxplot : Tukey.labels$treatment=rownames(Tukey.labels) Tukey.labels=Tukey.labels[order(Tukey.labels$treatment) , ] return(Tukey.labels) } This function does ANOVA and makes boxplots with Tukey statistics for comparing Mean yield.\nget_my_box_plot \u0026lt;- function (field, plot_name = \u0026quot;the Field\u0026quot;) { model = lm(field$Yield ~ field$ControlRate.Levels) ANOVA = aov(model) # Tukey test to study each pair of treatment : TUKEY \u0026lt;- TukeyHSD(x = ANOVA, \u0026#39;field$ControlRate.Levels\u0026#39;, conf.level = 0.95) # generate labels using function labels \u0026lt;- generate_label_df(TUKEY , \u0026quot;field$ControlRate.Levels\u0026quot;) # rename columns for merging names(labels) \u0026lt;- c(\u0026#39;Letters\u0026#39;, \u0026#39;ControlRate.Levels\u0026#39;) # Obtain letter positions for y axis using means yvalue \u0026lt;- aggregate(. ~ ControlRate.Levels, data = field, mean) final \u0026lt;- merge(labels, yvalue) #merge dataframes p \u0026lt;- ggplot(field, aes(x = ControlRate.Levels, y = Yield)) + geom_blank() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + labs(x = \u0026#39;Control Rates\u0026#39;, y = \u0026#39;Mean Yield\u0026#39;) + ggtitle(paste0(\u0026quot;ControlRates Vs Mean yield for \u0026quot;, plot_name), expression(atop(italic(\u0026quot;(Anova:TukeyHSD)\u0026quot;), \u0026quot;\u0026quot;))) + theme(plot.title = element_text(hjust = 0.5, face = \u0026#39;bold\u0026#39;)) + geom_boxplot(fill = \u0026#39;grey\u0026#39;, stat = \u0026quot;boxplot\u0026quot;) + geom_text( data = final, aes(x = ControlRate.Levels, y = Yield, label = Letters), vjust = -3.5, hjust = -.5 ) + geom_vline(aes(xintercept = 4.5), linetype = \u0026quot;dashed\u0026quot;) + theme(plot.title = element_text(vjust = -0.6)) print(p) # print(TUKEY) } Function to calculate mean and sd for each field for a given interval of ControlRate and then calculate Required Replicates and EffectSize. I have made one function to do all that so I can just use this function to analyze all four fields. To analyze single field set Single.Field.Analysis = TRUE, and to analyze all four field together, set Single.Field.Analysis = FALSE,\ncalculate_field_mean_SD_and_get_RR_EffSize \u0026lt;- function( field, intervals, Single.Field.Analysis = TRUE, printplot = FALSE){ plot_NAME \u0026lt;- (deparse(substitute(field))) if(Single.Field.Analysis \u0026gt; 0){ # removing rows with NAs in Yield sum(is.na(field$Yield)) # field \u0026lt;- field[!complete.cases(field$Yield), ] field \u0026lt;- field[!is.na(field$Yield),] # as.factor(fieldA$ControlRate) table.field \u0026lt;- table(as.factor(field$ControlRate)) field$ControlRate.Levels \u0026lt;- as.factor(intervals * ceiling(field$ControlRate/intervals)) field.Count \u0026lt;- setNames(aggregate(field$ControlRate, by = list(field$ControlRate.Levels), FUN = length), c(\u0026quot;ControlRate\u0026quot;, \u0026quot;Count\u0026quot;)) # Degree of freedome = n * k - k field.Count$degree.Freedom \u0026lt;- (field.Count$Count * length(field.Count$ControlRate)) -length(field.Count$ControlRate) field.mean \u0026lt;- setNames(aggregate( field$Yield, by = list(field$ControlRate.Levels), FUN = mean ), c(\u0026quot;ControlRate\u0026quot;, \u0026quot;Mean\u0026quot;)) field.SD \u0026lt;- setNames(aggregate( field$Yield, by = list(field$ControlRate.Levels), FUN = sd ), c(\u0026quot;ControlRate\u0026quot;, \u0026quot;SD\u0026quot;)) # plot individual fields with tukey test We will print box plot only if we # want for certain ControlRates intervals. Otherwise we will have too many # plots if(printplot == 1 ){ get_my_box_plot(field, plot_name = plot_NAME) } # Else we work on the merged data with SD pooled; no need to calculate mean # and SD as we will be doing it below } else { temp.Field \u0026lt;- field colnames(temp.Field)[colnames(temp.Field) == \u0026quot;ControlRate\u0026quot;] \u0026lt;- \u0026quot;ControlRate.Levels\u0026quot; colnames(temp.Field)[colnames(temp.Field) == \u0026quot;Mean\u0026quot;] \u0026lt;- \u0026quot;Yield\u0026quot; # get_my_box_plot(temp.Field) field.SD \u0026lt;- as.data.frame(cbind(ControlRate = field[\u0026quot;ControlRate\u0026quot;], SD = field[\u0026quot;SD_pooled\u0026quot;])) field.mean \u0026lt;- as.data.frame(cbind(ControlRate = field[\u0026quot;ControlRate\u0026quot;], Mean = field[\u0026quot;Mean\u0026quot;])) } # Calculate Required replicate and Effect Size from each # field for ControlRate i vs i+1 # ReqRep_EffectSize_table \u0026lt;- function (field.mean, field.SD){ Req.Rep.table.field \u0026lt;- {} for (i in 1:nrow(field.SD)){ if(i+1 \u0026gt; nrow(field.SD) ){ break } temp.Effect.size \u0026lt;- cohen.d( m1 = field.mean$Mean[i], s1 = field.SD$SD[i], m2 = field.mean$Mean[i + 1], s2 = field.SD$SD[i + 1] ) tmp.req.reps \u0026lt;- required.replicates( m1 = field.mean$Mean[i], s1 = field.SD$SD[i], m2 = field.mean$Mean[i + 1], s2 = field.SD$SD[i + 1] ) tmp.table \u0026lt;- cbind( Group = paste0(field.SD$ControlRate[i], \u0026quot; Vs \u0026quot;, field.SD$ControlRate[i + 1]), EffectSize = temp.Effect.size, RequiredReplicates = tmp.req.reps ) Req.Rep.table.field \u0026lt;- rbind(Req.Rep.table.field, tmp.table) } if (Single.Field.Analysis \u0026gt; 0) { return( list( field.mean = field.mean, fieldSD = field.SD, field.Count = field.Count, Req.Rep.table.field = Req.Rep.table.field ) ) } else{ return(list(Req.Rep.table.field = Req.Rep.table.field)) } } ###############################################End of Functions########## Now, we read four fields data:\nfieldA \u0026lt;- read.table( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldA.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot; ) # head(fieldA) dim(fieldA) ## [1] 6718 6  tmp.fieldA \u0026lt;- fieldA tmp.fieldA$ControlRate \u0026lt;- as.factor(tmp.fieldA$ControlRate) # aggregate(fieldA$Yield, by = list(fieldA$ControlRate), FUN = mean) Note: There are 40 levels of Control rates in fieldA, we can reduce these levels, so what we can do is relevel them separated by ‘intervals’ of(say 1000) as used in ‘calculate_field_mean_SD’ function above. We can do the same for other fields. However, instead of checking all this one by one, we will be using the functions we described above to analyze these four fields data and get the plots.\nfieldB \u0026lt;- read.table( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldB.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot; ) # head(fieldB) dim(fieldB) ## [1] 9321 6  # fieldC fieldC \u0026lt;- read.table( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldC.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot; ) # head(fieldC) dim(fieldC) ## [1] 10404 6  # fieldD fieldD \u0026lt;- read.table( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldD.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot; ) # head(fieldD) dim(fieldD) ## [1] 12654 6 we can calculate EffectSize and RequiredReplicates for these four fields using previous homeworks functions Cohen’s d and required.replicates as described in the begining of this report.\nHere, we can work on each individual field, first for the interval of 500, 1000, 2000 and 3000 ControlRates and decide which ControlRate interval fits the best for our data. Then we we will work on merged data that we merge after calculating individual fields (i.e using Mean and SD pooled for each ControlRate from all fields)\nWe chan check how many replicates we need for each field if we compare at the different ControlRates intervals starting with 500 to 3000. Set eval = TRUE to check this code.\nAt ControlRates interval of 500:\n# Now we chan check how many replicates we need for each field if we compare at # the different ControlRates intervals starting with 500 to 3000: # At ControlRates interval of 500: ControlRateInterval \u0026lt;- 500 # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldA, interval = ControlRateInterval ) # field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldB, interval = ControlRateInterval ) # field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldC, interval = ControlRateInterval ) # field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldD, interval = ControlRateInterval ) # Now, we merge all these so we can plot them together later my_list \u0026lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field) my_list_nms \u0026lt;- setNames(my_list, c(\u0026quot;fieldA\u0026quot;, \u0026quot;fieldB\u0026quot;, \u0026quot;fieldC\u0026quot;, \u0026quot;fieldD\u0026quot;)) Merged.EffectSize.500 \u0026lt;- data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = \u0026quot;id\u0026quot;)) Merged.EffectSize.500$interval \u0026lt;- 500 At ControlRates interval of 1000:\nControlRateInterval \u0026lt;- 1000 # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldA, interval = ControlRateInterval ) # field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldB, interval = ControlRateInterval ) # field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldC, interval = ControlRateInterval ) # field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldD, interval = ControlRateInterval ) # Now, we merge all these so we can plot them together later my_list \u0026lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field) my_list_nms \u0026lt;- setNames(my_list, c(\u0026quot;fieldA\u0026quot;, \u0026quot;fieldB\u0026quot;, \u0026quot;fieldC\u0026quot;, \u0026quot;fieldD\u0026quot;)) Merged.EffectSize.1000 \u0026lt;- data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = \u0026quot;id\u0026quot;)) Merged.EffectSize.1000$interval \u0026lt;- 1000 At ControlRates interval of 2000:\nControlRateInterval \u0026lt;- 2000 # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldA, interval = ControlRateInterval ) # field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldB, interval = ControlRateInterval ) # field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldC, interval = ControlRateInterval ) # field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldD, interval = ControlRateInterval ) # Now, we merge all these so we can plot them together later my_list \u0026lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field) my_list_nms \u0026lt;- setNames(my_list, c(\u0026quot;fieldA\u0026quot;, \u0026quot;fieldB\u0026quot;, \u0026quot;fieldC\u0026quot;, \u0026quot;fieldD\u0026quot;)) Merged.EffectSize.2000 \u0026lt;- data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = \u0026quot;id\u0026quot;)) Merged.EffectSize.2000$interval \u0026lt;- 2000 At ControlRates interval of 3000:\nControlRateInterval \u0026lt;- 3000 # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldA, interval = ControlRateInterval ) # field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldB, interval = ControlRateInterval ) # field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldC, interval = ControlRateInterval ) # field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldD, interval = ControlRateInterval ) # Now, we merge all these so we can plot them together later my_list \u0026lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field) my_list_nms \u0026lt;- setNames(my_list, c(\u0026quot;fieldA\u0026quot;, \u0026quot;fieldB\u0026quot;, \u0026quot;fieldC\u0026quot;, \u0026quot;fieldD\u0026quot;)) Merged.EffectSize.3000 \u0026lt;- data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = \u0026quot;id\u0026quot;)) Merged.EffectSize.3000$interval \u0026lt;- 3000 Now, let’s plot these data for EffectSize Vs RequiredReplicates for each Field and fordifferent ControlRate intervals\nEffectSize.Plot \u0026lt;- rbind( Merged.EffectSize.500, Merged.EffectSize.1000, Merged.EffectSize.2000, Merged.EffectSize.3000 ) EffectSize.Plot$id \u0026lt;- factor(EffectSize.Plot$id) EffectSize.Plot$interval \u0026lt;- factor(EffectSize.Plot$interval) EffectSize.Plot$Group \u0026lt;- factor(EffectSize.Plot$Group) EffectSize.Plot$EffectSize \u0026lt;- as.numeric(EffectSize.Plot$EffectSize) EffectSize.Plot$RequiredReplicates \u0026lt;- as.numeric(EffectSize.Plot$RequiredReplicates) head(EffectSize.Plot) ## id Group EffectSize RequiredReplicates interval ## 1 fieldA 23000 Vs 23500 1.03546848 15 500 ## 2 fieldA 23500 Vs 24000 0.73574477 29 500 ## 3 fieldA 24000 Vs 25000 1.48620683 7 500 ## 4 fieldA 25000 Vs 25500 0.41887785 89 500 ## 5 fieldA 25500 Vs 26000 1.10746983 13 500 ## 6 fieldA 26000 Vs 26500 0.07390218 2874 500 scaleFUN \u0026lt;- function(x) sprintf(\u0026quot;%.2f\u0026quot;, x) # To eliminate any outliers and noise, we can plot effect Size between 0-1 and # Required Replicates upto 5000 EffectSize.Plot \u0026lt;- EffectSize.Plot[EffectSize.Plot$RequiredReplicates\u0026lt;= 5000,] EffectSize.Plot \u0026lt;- EffectSize.Plot[EffectSize.Plot$EffectSize\u0026lt;= 1,] EffectSize.Plot$Group \u0026lt;- factor(EffectSize.Plot$Group) ggplot(EffectSize.Plot, aes(x=EffectSize, y=RequiredReplicates)) + ggtitle(\u0026quot;Effect size Vs Required replicates for each field data\u0026quot;) + geom_point(aes(shape = Group), size = 2) + scale_shape_manual(values=1:nlevels(EffectSize.Plot$Group)) + geom_line() + scale_x_continuous(\u0026quot;EffectSize\u0026quot;, breaks=c(0,0.2,0.5,1), labels = scaleFUN) + theme_bw() + theme(axis.title = element_text(size=14,face=\u0026quot;bold\u0026quot;), axis.text = element_text(size=10), plot.title = element_text(hjust = 0.5, face = \u0026quot;bold\u0026quot;)) + facet_wrap(~id) Based on the plot we just plotted above, RequiredReplicates inccrease significantly with decreasing Effect Size.\nAdditionally, after running all these ControlRateInterval, I noticed that the Required replicates decrease for the higher ControlRate intervals. The reason I did not include this in my report is because it gives lots of output. SO, if we consider taking the ControlRate intervals of only 1000 and get the field mean and field SD for each field as below.\nNow, I will only limit my analysis for all fields (merged) for the Control Rates interval of 1000.\n# If we consider taking the ControlRate intervals of 1000 and get the field mean and field # SD for each field: ControlRateInterval \u0026lt;- 1000 # field A fieldA.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldA, intervals = ControlRateInterval, printplot = TRUE ) fieldA.data ## $field.mean ## ControlRate Mean ## 1 23000 63.01258 ## 2 24000 100.63078 ## 3 25000 172.57527 ## 4 26000 237.45902 ## 5 27000 237.33206 ## 6 28000 238.59221 ## 7 29000 195.73072 ## ## $fieldSD ## ControlRate SD ## 1 23000 15.45740 ## 2 24000 51.36069 ## 3 25000 41.82109 ## 4 26000 31.64908 ## 5 27000 44.09390 ## 6 28000 38.11963 ## 7 29000 53.59605 ## ## $field.Count ## ControlRate Count degree.Freedom ## 1 23000 24 161 ## 2 24000 93 644 ## 3 25000 44 301 ## 4 26000 4381 30660 ## 5 27000 786 5495 ## 6 28000 1283 8974 ## 7 29000 99 686 ## ## $Req.Rep.table.field ## Group EffectSize RequiredReplicates ## [1,] \u0026quot;23000 Vs 24000\u0026quot; \u0026quot;0.991869049115401\u0026quot; \u0026quot;16\u0026quot; ## [2,] \u0026quot;24000 Vs 25000\u0026quot; \u0026quot;1.53614606065372\u0026quot; \u0026quot;7\u0026quot; ## [3,] \u0026quot;25000 Vs 26000\u0026quot; \u0026quot;1.74957194606738\u0026quot; \u0026quot;5\u0026quot; ## [4,] \u0026quot;26000 Vs 27000\u0026quot; \u0026quot;0.00330802194919998\u0026quot; \u0026quot;1434501\u0026quot; ## [5,] \u0026quot;27000 Vs 28000\u0026quot; \u0026quot;0.0305749319500618\u0026quot; \u0026quot;16792\u0026quot; ## [6,] \u0026quot;28000 Vs 29000\u0026quot; \u0026quot;0.921630680790716\u0026quot; \u0026quot;18\u0026quot; Based on the plot we can see that the Mean Yield is significant between 23000 vs all ControlRates , 24000 vs all Control Rates, 25000 Vs all Control rates. Yeild has no significant effect of Seeding from Control Rate between 26000 to 28000. Infact, it starts to decline significantly from 29000 Control Seedings.\n# field B fieldB.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldB, interval = ControlRateInterval, printplot = TRUE ) fieldB.data ## $field.mean ## ControlRate Mean ## 1 24000 125.8102 ## 2 25000 180.9337 ## 3 26000 215.8033 ## 4 27000 229.8309 ## 5 28000 240.5746 ## 6 29000 238.9680 ## ## $fieldSD ## ControlRate SD ## 1 24000 33.51371 ## 2 25000 33.36007 ## 3 26000 28.10560 ## 4 27000 19.11270 ## 5 28000 14.00890 ## 6 29000 14.26832 ## ## $field.Count ## ControlRate Count degree.Freedom ## 1 24000 36 210 ## 2 25000 390 2334 ## 3 26000 2455 14724 ## 4 27000 798 4782 ## 5 28000 5173 31032 ## 6 29000 469 2808 ## ## $Req.Rep.table.field ## Group EffectSize RequiredReplicates ## [1,] \u0026quot;24000 Vs 25000\u0026quot; \u0026quot;1.64857878536612\u0026quot; \u0026quot;6\u0026quot; ## [2,] \u0026quot;25000 Vs 26000\u0026quot; \u0026quot;1.13048091635421\u0026quot; \u0026quot;12\u0026quot; ## [3,] \u0026quot;26000 Vs 27000\u0026quot; \u0026quot;0.583669852812804\u0026quot; \u0026quot;46\u0026quot; ## [4,] \u0026quot;27000 Vs 28000\u0026quot; \u0026quot;0.641176026096802\u0026quot; \u0026quot;38\u0026quot; ## [5,] \u0026quot;28000 Vs 29000\u0026quot; \u0026quot;0.113628916568796\u0026quot; \u0026quot;1216\u0026quot; Here, we see significant difference between all Control Rates of Seeding except between 28000 Vs 29000. The seeding rates of 28000 has the highest Mean yield here.\n# field C fieldC.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldC, interval = ControlRateInterval, printplot = TRUE ) fieldC.data ## $field.mean ## ControlRate Mean ## 1 23000 149.2422 ## 2 24000 185.4175 ## 3 25000 195.8427 ## 4 26000 225.0309 ## 5 27000 231.4881 ## 6 28000 240.9121 ## ## $fieldSD ## ControlRate SD ## 1 23000 14.416699 ## 2 24000 15.340123 ## 3 25000 13.316623 ## 4 26000 14.335465 ## 5 27000 11.247829 ## 6 28000 4.463365 ## ## $field.Count ## ControlRate Count degree.Freedom ## 1 23000 28 162 ## 2 24000 170 1014 ## 3 25000 69 408 ## 4 26000 5697 34176 ## 5 27000 4419 26508 ## 6 28000 21 120 ## ## $Req.Rep.table.field ## Group EffectSize RequiredReplicates ## [1,] \u0026quot;23000 Vs 24000\u0026quot; \u0026quot;2.43022553971697\u0026quot; \u0026quot;3\u0026quot; ## [2,] \u0026quot;24000 Vs 25000\u0026quot; \u0026quot;0.725780226926124\u0026quot; \u0026quot;30\u0026quot; ## [3,] \u0026quot;25000 Vs 26000\u0026quot; \u0026quot;2.1096712954133\u0026quot; \u0026quot;4\u0026quot; ## [4,] \u0026quot;26000 Vs 27000\u0026quot; \u0026quot;0.501162120939759\u0026quot; \u0026quot;63\u0026quot; ## [5,] \u0026quot;27000 Vs 28000\u0026quot; \u0026quot;1.10136177756912\u0026quot; \u0026quot;13\u0026quot; Here, all Control rates of seeding are significantly diffferent with the highest Mean yeild for 28000.\n# field D fieldD.data \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = fieldD, interval = ControlRateInterval, printplot = TRUE ) fieldD.data ## $field.mean ## ControlRate Mean ## 1 23000 113.4783 ## 2 24000 157.5754 ## 3 25000 176.8111 ## 4 26000 227.5933 ## 5 27000 240.8722 ## 6 28000 247.0608 ## 7 29000 230.0359 ## ## $fieldSD ## ControlRate SD ## 1 23000 24.58440 ## 2 24000 39.35167 ## 3 25000 36.11914 ## 4 26000 26.50089 ## 5 27000 17.44482 ## 6 28000 15.63857 ## 7 29000 25.17324 ## ## $field.Count ## ControlRate Count degree.Freedom ## 1 23000 50 343 ## 2 24000 372 2597 ## 3 25000 132 917 ## 4 26000 8284 57981 ## 5 27000 1167 8162 ## 6 28000 2631 18410 ## 7 29000 18 119 ## ## $Req.Rep.table.field ## Group EffectSize RequiredReplicates ## [1,] \u0026quot;23000 Vs 24000\u0026quot; \u0026quot;1.34402785417073\u0026quot; \u0026quot;9\u0026quot; ## [2,] \u0026quot;24000 Vs 25000\u0026quot; \u0026quot;0.50928632312699\u0026quot; \u0026quot;61\u0026quot; ## [3,] \u0026quot;25000 Vs 26000\u0026quot; \u0026quot;1.60311540536477\u0026quot; \u0026quot;6\u0026quot; ## [4,] \u0026quot;26000 Vs 27000\u0026quot; \u0026quot;0.591895252495428\u0026quot; \u0026quot;45\u0026quot; ## [5,] \u0026quot;27000 Vs 28000\u0026quot; \u0026quot;0.373564449867553\u0026quot; \u0026quot;112\u0026quot; ## [6,] \u0026quot;28000 Vs 29000\u0026quot; \u0026quot;0.812438594873596\u0026quot; \u0026quot;24\u0026quot; Here, all Control Rates of seeding are significantly different except for 26000-28000 Vs 29000 with the highest Mean yield for 28000.\nAlso, to check if my calculation of Control rates and Yeild mean matches with the table in instruction (with 1000 intervals), we can check this with the merged field data ( i.e. fieldA, fieldB, fieldC and fieldD) data. We should get the same Mean values in the instruction table above.\nall.merged.raw.fields \u0026lt;- rbind(fieldA, fieldB, fieldC, fieldD) check.the.instruction.table \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = all.merged.raw.fields, intervals = ControlRateInterval ) check.the.instruction.table$field.mean ## ControlRate Mean ## 1 23000 111.4216 ## 2 24000 155.0326 ## 3 25000 181.1176 ## 4 26000 227.5779 ## 5 27000 233.4717 ## 6 28000 242.1698 ## 7 29000 231.3890 We can now merge all four fields for SD, means and ControlRate level counts.\nmerged.SD.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$fieldSD, fieldB.data$fieldSD, fieldC.data$fieldSD, fieldD.data$fieldSD ) ) merged.Mean.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$field.mean, fieldB.data$field.mean, fieldC.data$field.mean, fieldD.data$field.mean ) ) merged.Count.4.plots \u0026lt;- Reduce( function(x, y) merge(x, y, all = TRUE), list( fieldA.data$field.Count, fieldB.data$field.Count, fieldC.data$field.Count, fieldD.data$field.Count ) ) We need SD pooled for these levels so we can calculate Cohen’s d and Required Replicates.\nlevels(merged.SD.4.plots$ControlRate) ## [1] \u0026quot;23000\u0026quot; \u0026quot;24000\u0026quot; \u0026quot;25000\u0026quot; \u0026quot;26000\u0026quot; \u0026quot;27000\u0026quot; \u0026quot;28000\u0026quot; \u0026quot;29000\u0026quot; First, we merged all merged.SD.4.plots, merged.Mean.4.plots and merged.Count.4.plots from all fields.\n# Therefore, now merging all three dataframes from all four plots for SD, Mean # and counts by ControlRate column Mean_SD_Count.dat \u0026lt;- Reduce(function(...) merge(..., by = c(\u0026quot;ControlRate\u0026quot;, \u0026quot;grp\u0026quot;), all.x = TRUE), lapply( list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots), transform, grp = ave(seq_along(ControlRate), ControlRate, FUN = seq_along) )) head(Mean_SD_Count.dat) ## ControlRate grp Mean SD Count degree.Freedom ## 1 23000 1 63.01258 14.41670 24 161 ## 2 23000 2 113.47830 15.45740 28 162 ## 3 23000 3 149.24222 24.58440 50 343 ## 4 24000 1 100.63078 15.34012 36 210 ## 5 24000 2 125.81017 33.51371 93 644 ## 6 24000 3 157.57538 39.35167 170 1014 Now, we can also do Anova on the merged data Mean_SD_Count.dat\nMean_SD_Count_merged_for_all_four_plots \u0026lt;- Mean_SD_Count.dat Mean_SD_Count_merged_for_all_four_plots$Yield \u0026lt;- Mean_SD_Count_merged_for_all_four_plots$Mean Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels \u0026lt;- Mean_SD_Count_merged_for_all_four_plots$ControlRate get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = \u0026quot;all four fields\u0026quot;) Based on this plot, from all fields, we can tell that, Control Rates of seeding of 23000-25000 has significantly less Mean yield as compared to 26000-29000. It looks like Control Rates of Seeding between 27000 and 28000 has the highest Mean yield.\nWe now calculate pooled SD for merged 4 plots Mean_SD_Count.dat to calculate Effect Size and Required Replicates for all fields (combined analysis) using pooled SD.\npooled.dat \u0026lt;- Mean_SD_Count.dat # # Pooled sd can be calculated as: pooled.dat$df \u0026lt;- pooled.dat$Count-1 # pooled SD is : # pooledSD \u0026lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) ) # We can calculate our SD pooled using this formula: # s_{pooled} = \\sqrt{\\frac{\\sum_i (n_i-1)s_i^2}{N-k}} # We will derrive this in steps as below: pooled.dat$df \u0026lt;- pooled.dat$Count-1 pooled.dat$sd.square \u0026lt;- pooled.dat$SD^2 pooled.dat$ss \u0026lt;- pooled.dat$sd.square * pooled.dat$df # We can use convenience function (aggregate) for splitting and calculating the necessary sums. ds \u0026lt;- aggregate(ss ~ ControlRate, data = pooled.dat, sum) # Two different built in methods for split apply, we could use aggregate for # both if we wanted. This calculates our degrees of freedom. ds$df \u0026lt;- tapply(pooled.dat$df, pooled.dat$ControlRate, sum) # divide ss by df and then we get sd square ds$sd.square \u0026lt;- ds$ss / ds$df # Finally, we can get our sd pooled ds$SD_pooled \u0026lt;- sqrt(ds$sd.square) ds ## ControlRate ss df sd.square SD_pooled ## 1 23000 40846.73 99 412.5933 20.31239 ## 2 24000 1351941.68 667 2026.8991 45.02110 ## 3 25000 934566.13 631 1481.0874 38.48490 ## 4 26000 16376572.57 20813 786.8434 28.05073 ## 5 27000 9357584.58 7166 1305.8309 36.13628 ## 6 28000 8410659.15 9104 923.8422 30.39477 ## 7 29000 1409909.69 583 2418.3700 49.17693 # However, we could also calculate our sd_pooled as below and get the same results : sd_pooled \u0026lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$ControlRate), function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) ) Now, we calculate Mean (Mean of Means) from the merged table Mean_SD_Count.dat, so we can calculate Cohen’s d and RequiredReplicates for all four field combined.\nds.Mean \u0026lt;- setNames(aggregate( Mean_SD_Count.dat$Mean, by = list(Mean_SD_Count.dat$ControlRate), FUN = mean ), c(\u0026quot;ControlRate\u0026quot;, \u0026quot;Mean\u0026quot;)) ds \u0026lt;- merge(ds, ds.Mean, by.x = \u0026quot;ControlRate\u0026quot;) ds ## ControlRate ss df sd.square SD_pooled Mean ## 1 23000 40846.73 99 412.5933 20.31239 108.5777 ## 2 24000 1351941.68 667 2026.8991 45.02110 142.3585 ## 3 25000 934566.13 631 1481.0874 38.48490 181.5407 ## 4 26000 16376572.57 20813 786.8434 28.05073 226.4716 ## 5 27000 9357584.58 7166 1305.8309 36.13628 234.8808 ## 6 28000 8410659.15 9104 923.8422 30.39477 241.7849 ## 7 29000 1409909.69 583 2418.3700 49.17693 221.5782 Now we calculate the Effect Size and Cohen’s D for the combined 4 plots using mean yield and sd pooled for different ControlRate\nNow we calculate the Effect Size and Cohen’s D for the combined 4 plots using mean yield and sd pooled for different ControlRate usinf our function calculate_field_mean_SD_and_get_RR_EffSize.\nRequiredReplicates_for_all_fields \u0026lt;- calculate_field_mean_SD_and_get_RR_EffSize( field = ds, intervals = ControlRateInterval, Single.Field.Analysis = FALSE ) RequiredReplicates_for_all_fields ## $Req.Rep.table.field ## Group EffectSize RequiredReplicates ## [1,] \u0026quot;23000 Vs 24000\u0026quot; \u0026quot;0.967241180364881\u0026quot; \u0026quot;17\u0026quot; ## [2,] \u0026quot;24000 Vs 25000\u0026quot; \u0026quot;0.935567380483301\u0026quot; \u0026quot;18\u0026quot; ## [3,] \u0026quot;25000 Vs 26000\u0026quot; \u0026quot;1.33427570681431\u0026quot; \u0026quot;9\u0026quot; ## [4,] \u0026quot;26000 Vs 27000\u0026quot; \u0026quot;0.259967383137317\u0026quot; \u0026quot;232\u0026quot; ## [5,] \u0026quot;27000 Vs 28000\u0026quot; \u0026quot;0.206777477800031\u0026quot; \u0026quot;367\u0026quot; ## [6,] \u0026quot;28000 Vs 29000\u0026quot; \u0026quot;0.49430440628565\u0026quot; \u0026quot;64\u0026quot; We can plot this results for better visualization of pattern.\nRequiredReplicates_for_all_fields \u0026lt;- as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field) RequiredReplicates_for_all_fields$Group \u0026lt;- factor(RequiredReplicates_for_all_fields$Group) RequiredReplicates_for_all_fields$RequiredReplicates \u0026lt;- as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates)) RequiredReplicates_for_all_fields$EffectSize \u0026lt;- as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize)) # Plot RequiredReplicates_for_all_fields ggplot(RequiredReplicates_for_all_fields, aes(x = EffectSize, y = RequiredReplicates)) + ggtitle(\u0026quot;Effect size Vs Required replicates for all fields data \\n(Combined Effect)\u0026quot;) + geom_point(aes(shape = Group), size = 2) + scale_shape_manual(values = 1:nlevels(EffectSize.Plot$Group)) + geom_line() + scale_x_continuous(\u0026quot;EffectSize\u0026quot;, breaks = c( 0, 0.2, 0.5, 1, max(RequiredReplicates_for_all_fields$EffectSize) ), labels = scaleFUN) + theme_bw() + theme( axis.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 10), plot.title = element_text(hjust = 0.5, face = \u0026quot;bold\u0026quot;) ) Based on the combined effect size (from all four fields), we can see that the Effect size is inversely proportional to Required Replicates. We also see that for the small effect of ~0.2, we need exponentially larger replicates, for medium effect (~0.5), we need about 80 replicates and for larger effect (more than 0.8), we only need about 20 or fewer replicates. Our analysis from all fields also indicates that we have highest mean yeild for controlRates of Seeding og 27000-29000.\nFinal discussion and suggestion:\nBased on the results we saw above, if we take individual field (one at a time), we would need thousands of replicates for low effect size. However, for moderate and large effect size, the required replicates number signficantly decreases even if we use individual field data separately. On the other hand, when we combine all four fields, our required replictes decreases for all low, moderate and large effect sizes (Figure: Effect size Vs Required replicates for all fields data (Combined Effect)) as compared to individual fields (Figure: Effect size Vs Required replicates for each field data). I also found that the required replicates decreases for lower rates of seeding (control Rates of 23000 Vs 24000). Based on these results, we can also conclude that the Required replicates is inversely proportional to the Effect size as seen in all four fields data and merged data.\nAditionally, the mean yield significantly increases when we increase the seeding rates at control rates of 26000 to 28000 for each individual field. This was also consistent for combined field data (Figure: ControlRates Vs Mean yield for all four fields). So, my final suggestion for this analysis is that we should used all four fields to calculate required replicates and effect size, though we could still use single field data separately to derrive the conclusion that higher seeding rates (~26000-28000) has higher mean yield which was consistent in all four fields. However, making use of all four fields data is recommended.\n","date":1567376783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567376783,"objectID":"995ce1dda00e69e70526b6fd278060ac","permalink":"/achalneupane.github.io/post/midterm_project/","publishdate":"2019-09-01T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/midterm_project/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Midterm data analysis","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  General Instructions There are 5 exercises, each is worth 10 points. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points.\nFor this exercise, you may use whatever graphics library you desire.\nExperimental Again, you will be allowed to provide one solution using Python. Elaborate on the similarities and differences between Python function definitions and R or IML or Macro language.\n  Exercise 1. Load the ncaa2018.csv data set and create histograms, QQ-norm and box-whisker plots for ELO. Add a title to each plot, identifying the data.\nncaa2018.dat = read.table( \u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/Week_7/ncaa2018.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; ) # head(ncaa2018.dat) # histogram hist(ncaa2018.dat$ELO, xlab = \u0026quot;ELO\u0026quot;, main = \u0026quot;Histogram of ELO\u0026quot;) # QQ plot qqnorm(ncaa2018.dat$ELO, main = \u0026quot;Normal QQ plot\u0026quot;) # box-whisker plot boxplot(ncaa2018.dat$ELO, main = \u0026quot;Box-whisker of ELO\u0026quot;, ylab = \u0026quot;ELO\u0026quot;) Part b A common recommendation to address issues of non-normality is to transform data to correct for skewness. One common transformation is the log transform.\nTransform ELO to log(ELO) and produce histograms, box-whisker and qqnorm plots of the transformed values. Are the transformed values more or less skewed than the original? (Note - the log transform is used to correct skewness, it is less useful for correcting kurtosis).\nhist(log(ncaa2018.dat$ELO), xlab = \u0026quot;ELO\u0026quot;, main = \u0026quot;Histogram of log transformed ELO\u0026quot;) # QQ plot qqnorm(log(ncaa2018.dat$ELO), main = \u0026quot;Normal QQ plot of log transformed ELO\u0026quot;) # box-whisker plot boxplot(log(ncaa2018.dat$ELO), main = \u0026quot;Box-whisker of log transformed ELO\u0026quot;, ylab = \u0026quot;ELO\u0026quot;) The values are less skewed now. Log transformation is a good way to display plot with skewedness in data. # Exercise 2.\nReview Exercise 4, Homework 6, where you calculated skewness and kurtosis. The reference for this exercise, https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm, gives four example statistical distributions. We will reproduce the histograms, and add qqnorm and box-whisker plots.\n Part a Use the code below from lecture to draw 1000 samples from the normal distribution.\nnorm.sample \u0026lt;- rnorm(1000, mean=0, sd=1) Look up the corresponding r* functions in R for the Cauchy distribution (use location=0, scale=1), and the Weibull distribution (use shape = 1.5). For the double exponential, use you can use the *laplace functions from the rmutil library, or you can use rexp(1000) - rexp(1000)\nDraw 1000 samples from each of these distributions. Calculate skewness and kurtosis for each sample. You may use your own function, or use the moments library.\ndcauchy.1000 \u0026lt;- rcauchy(1:1000, location = 0, scale = 1) dweibull.1000 \u0026lt;- rweibull(1:1000, shape = 1.5) rexp.1000 \u0026lt;- rexp(1000) - rexp(1000) # Kurtosis library(moments) norm.sample.kurt \u0026lt;- kurtosis(norm.sample, na.rm = TRUE) norm.sample.kurt ## [1] 3.186137 dcauchy.1000.kurt \u0026lt;- kurtosis(dcauchy.1000, na.rm = TRUE) dcauchy.1000.kurt ## [1] 216.1207 dweibull.1000.kurt \u0026lt;- kurtosis(dweibull.1000, na.rm = TRUE) dweibull.1000.kurt ## [1] 4.464365 rexp.1000.kurt \u0026lt;- kurtosis(rexp.1000, na.rm = TRUE) rexp.1000.kurt ## [1] 5.153933 norm.sample.skew \u0026lt;- skewness(norm.sample, na.rm = TRUE) norm.sample.skew ## [1] -0.06600051 dcauchy.1000.skew \u0026lt;- skewness(dcauchy.1000, na.rm = TRUE) dcauchy.1000.skew ## [1] -10.63517 dweibull.1000.skew \u0026lt;- skewness(dweibull.1000, na.rm = TRUE) dweibull.1000.skew ## [1] 1.108429 rexp.1000.skew \u0026lt;- skewness(rexp.1000, na.rm = TRUE) rexp.1000.skew ## [1] 0.1488806  Part b Plot the histograms for each distribution. Use par(mfrow=c(2,2)) in your code chunk to combine the four histogram in a single plot. Add titles to the histograms indicating the distribution. Set the x-axis label to show the calculated skewness and kurtosis, i.e. skewness = ####, kurtosis = ####\npar(mfrow=c(2,2)) hist(norm.sample, xlab = paste0(\u0026quot;skewness = \u0026quot;, norm.sample.skew , \u0026quot;, kurtosis = \u0026quot;, norm.sample.kurt), main = \u0026quot;Histogram of Normal Distribution\u0026quot;) hist(dcauchy.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, dcauchy.1000.skew , \u0026quot;, kurtosis = \u0026quot;, dcauchy.1000.kurt), main = \u0026quot;Histogram of Cauchy Distribution\u0026quot;) hist(dweibull.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, dweibull.1000.kurt , \u0026quot;, kurtosis = \u0026quot;, dweibull.1000.kurt), main = \u0026quot;Histogram of Dweibull Distribution\u0026quot;) hist(rexp.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, rexp.1000.skew , \u0026quot;, kurtosis = \u0026quot;, rexp.1000.kurt), main = \u0026quot;Histogram of double Exponential Distribution\u0026quot;)  Part c Repeat Part b, but with QQ-norm plots.\npar(mfrow=c(2,2)) qqnorm(norm.sample, xlab = paste0(\u0026quot;skewness = \u0026quot;, norm.sample.skew , \u0026quot;, kurtosis = \u0026quot;, norm.sample.kurt), main = \u0026quot;QQplot of Normal Distribution\u0026quot;) qqnorm(dcauchy.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, dcauchy.1000.skew , \u0026quot;, kurtosis = \u0026quot;, dcauchy.1000.kurt), main = \u0026quot;QQplot of Cauchy Distribution\u0026quot;) qqnorm(dweibull.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, dweibull.1000.kurt , \u0026quot;, kurtosis = \u0026quot;, dweibull.1000.kurt), main = \u0026quot;QQplot of Dweibull Distribution\u0026quot;) qqnorm(rexp.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, rexp.1000.skew , \u0026quot;, kurtosis = \u0026quot;, rexp.1000.kurt), main = \u0026quot;QQplot of rexp Distribution\u0026quot;)  Part d Repeat Part b, but with box-whisker plots.\npar(mfrow=c(2,2)) boxplot(norm.sample, xlab = paste0(\u0026quot;skewness = \u0026quot;, norm.sample.skew , \u0026quot;, kurtosis = \u0026quot;, norm.sample.kurt), main = \u0026quot;Boxplot of Normal Distribution\u0026quot;) boxplot(dcauchy.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, dcauchy.1000.skew , \u0026quot;, kurtosis = \u0026quot;, dcauchy.1000.kurt), main = \u0026quot;Boxplot of Cauchy Distribution\u0026quot;) boxplot(dweibull.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, dweibull.1000.kurt , \u0026quot;, kurtosis = \u0026quot;, dweibull.1000.kurt), main = \u0026quot;Boxplot of Dweibull Distribution\u0026quot;) boxplot(rexp.1000, xlab = paste0(\u0026quot;skewness = \u0026quot;, rexp.1000.skew , \u0026quot;, kurtosis = \u0026quot;, rexp.1000.kurt), main = \u0026quot;Boxplot of double rexp Distribution\u0026quot;) Hints for SAS. If you create the samples in IML, use\nNormal = j(1, 1000, .); call randgen(Normal, \u0026quot;NORMAL\u0026quot;, 0, `); You can generate samples in the data step using\ndo i = 1 to 1000; Normal = rand(\u0026#39;NORMAL\u0026#39;,0,1); output; end; RAND doesn’t provide a Laplace option, but you can create samples from this distribution by\nrand(\u0026#39;EXPONENTIAL\u0026#39;)-rand(\u0026#39;EXPONENTIAL\u0026#39;); To group multiple plots, use\nods graphics / width=8cm height=8cm; ods layout gridded columns=2; ods region; ... first plot ods region; ... second plot ods layout end; You might need to include\nods graphics off; ods graphics on; ODS GRAPHICS / reset=All; to return the SAS graphics output to normal.\n Exercise 3. We will create a series of graphs illustrating how the Poisson distribution approaches the normal distribution with large \\(\\lambda\\). We will iterate over a sequence of lambda, from 2 to 64, doubling lambda each time. For each ‘lambda’ draw 1000 samples from the Poisson distribution.\nCalculate the skewness of each set of samples, and produce histograms, QQ-norm and box-whisker plots. You can use par(mfrow=c(1,3)) to display all three for one lambda in one line. Add lambda=## to the title of the histogram, and skewness=## to the title of the box-whisker plot.\n Part b. Remember that lambda represents the mean of a discrete (counting) variable. At what size mean is Poisson data no longer skewed, relative to normally distributed data? You might run this 2 or 3 times, with different seeds; this number varies in my experience.\nlibrary(moments) rpois_skew_collect \u0026lt;- {} mu \u0026lt;- 2 # set.seed(54321) while(mu \u0026lt;= 64){ rpois_values \u0026lt;- rpois(1000, lambda = mu) rpois_skew \u0026lt;- skewness(rpois_values, na.rm = TRUE) rpois_skew_collect[mu-1] \u0026lt;- skewness(rpois_values) # attaching all three plots together par(mfrow=c(1,3)) # histogram hist(rpois_values, main = paste0(\u0026quot;Histogram of rpois Distribution for lambda = \u0026quot;, mu)) # QQ plot qqnorm(rpois_values, main = \u0026quot;Normal QQ plot for poisson\u0026quot;) # box-whisker plot boxplot(rpois_values, main = paste0(\u0026quot;Box-whisker with skewness = \u0026quot;, rpois_skew), ylab = \u0026quot;rpois_values\u0026quot;) mu \u0026lt;- mu*2 } plot(rpois_skew_collect) It looks like the skewness starts to diminish significantly after mean size 32 (lambda) for poisson data.\nIf you do this in SAS, create a data table with data columns each representing a different \\(\\mu\\). You can see combined histogram, box-whisker and QQ-norm, for all columns, by calling\nproc univariate data=Distributions plot; run; At what \\(\\mu\\) is skewness of the Poisson distribution small enough to be considered normal? It looks like after mu 60, skewness is small enough to be considered normal.\n  Exercise 4 Part a Write a function that accepts a vector vec, a vector of integers, a main axis label and an x axis label. This function should 1. iterate over each element \\(i\\) in the vector of integers 2. produce a histogram for vec setting the number of bins in the histogram to \\(i\\) 3. label main and x-axis with the specified parameters. 4. label the y-axis to read Frequency, bins = and the number of bins.\nHint: You can simplify this function by using the parameter ... - see ?plot or ?hist\nWriting the asked function\n# vec \u0026lt;- c(12,36,60) # dat \u0026lt;- hidalgo.dat[,1] # i=1 plot.histograms \u0026lt;- function(dat, vec, main, xlab){ par(mfrow=c(1,length(vec))) for(i in 1:length(vec)){ print(hist(dat, breaks = vec[i], main = main, xlab = xlab, ylab = paste0(\u0026#39;Frequency, bins = \u0026#39;, vec[i]))) } } # now read the file hidalgo.dat = read.table(\u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/Week_9/hidalgo.dat\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; )  Part b Test your function with the hidalgo data set (see below), using bin numbers 12, 36, and 60. You should be able to call your function with something like\nplot.histograms(hidalgo.dat[,1],c(12,36,60), main=\u0026quot;1872 Hidalgo issue\u0026quot;,xlab= \u0026quot;Thickness (mm)\u0026quot;) ## $breaks ## [1] 0.060 0.065 0.070 0.075 0.080 0.085 0.090 0.095 0.100 0.105 0.110 ## [12] 0.115 0.120 0.125 0.130 0.135 ## ## $counts ## [1] 3 35 93 131 45 24 19 35 31 32 15 8 7 5 1 ## ## $density ## [1] 1.2396694 14.4628099 38.4297521 54.1322314 18.5950413 9.9173554 ## [7] 7.8512397 14.4628099 12.8099174 13.2231405 6.1983471 3.3057851 ## [13] 2.8925620 2.0661157 0.4132231 ## ## $mids ## [1] 0.0625 0.0675 0.0725 0.0775 0.0825 0.0875 0.0925 0.0975 0.1025 0.1075 ## [11] 0.1125 0.1175 0.1225 0.1275 0.1325 ## ## $xname ## [1] \u0026quot;dat\u0026quot; ## ## $equidist ## [1] TRUE ## ## attr(,\u0026quot;class\u0026quot;) ## [1] \u0026quot;histogram\u0026quot; ## $breaks ## [1] 0.064 0.066 0.068 0.070 0.072 0.074 0.076 0.078 0.080 0.082 0.084 ## [12] 0.086 0.088 0.090 0.092 0.094 0.096 0.098 0.100 0.102 0.104 0.106 ## [23] 0.108 0.110 0.112 0.114 0.116 0.118 0.120 0.122 0.124 0.126 0.128 ## [34] 0.130 0.132 ## ## $counts ## [1] 4 1 33 52 21 38 34 79 33 10 4 3 19 8 9 5 12 20 17 9 9 10 18 ## [24] 9 3 3 1 7 3 2 2 1 4 1 ## ## $density ## [1] 4.132231 1.033058 34.090909 53.719008 21.694215 39.256198 35.123967 ## [8] 81.611570 34.090909 10.330579 4.132231 3.099174 19.628099 8.264463 ## [15] 9.297521 5.165289 12.396694 20.661157 17.561983 9.297521 9.297521 ## [22] 10.330579 18.595041 9.297521 3.099174 3.099174 1.033058 7.231405 ## [29] 3.099174 2.066116 2.066116 1.033058 4.132231 1.033058 ## ## $mids ## [1] 0.065 0.067 0.069 0.071 0.073 0.075 0.077 0.079 0.081 0.083 0.085 ## [12] 0.087 0.089 0.091 0.093 0.095 0.097 0.099 0.101 0.103 0.105 0.107 ## [23] 0.109 0.111 0.113 0.115 0.117 0.119 0.121 0.123 0.125 0.127 0.129 ## [34] 0.131 ## ## $xname ## [1] \u0026quot;dat\u0026quot; ## ## $equidist ## [1] TRUE ## ## attr(,\u0026quot;class\u0026quot;) ## [1] \u0026quot;histogram\u0026quot; ## $breaks ## [1] 0.064 0.065 0.066 0.067 0.068 0.069 0.070 0.071 0.072 0.073 0.074 ## [12] 0.075 0.076 0.077 0.078 0.079 0.080 0.081 0.082 0.083 0.084 0.085 ## [23] 0.086 0.087 0.088 0.089 0.090 0.091 0.092 0.093 0.094 0.095 0.096 ## [34] 0.097 0.098 0.099 0.100 0.101 0.102 0.103 0.104 0.105 0.106 0.107 ## [45] 0.108 0.109 0.110 0.111 0.112 0.113 0.114 0.115 0.116 0.117 0.118 ## [56] 0.119 0.120 0.121 0.122 0.123 0.124 0.125 0.126 0.127 0.128 0.129 ## [67] 0.130 0.131 ## ## $counts ## [1] 3 1 0 1 7 26 20 32 11 10 20 18 11 23 42 37 15 18 7 3 2 2 1 ## [24] 2 10 9 3 5 6 3 2 3 7 5 5 15 9 8 7 2 5 4 3 7 7 11 ## [47] 4 5 0 3 3 0 1 0 4 3 1 2 2 0 2 0 0 1 3 1 1 ## ## $density ## [1] 6.198347 2.066116 0.000000 2.066116 14.462810 53.719008 41.322314 ## [8] 66.115702 22.727273 20.661157 41.322314 37.190083 22.727273 47.520661 ## [15] 86.776860 76.446281 30.991736 37.190083 14.462810 6.198347 4.132231 ## [22] 4.132231 2.066116 4.132231 20.661157 18.595041 6.198347 10.330579 ## [29] 12.396694 6.198347 4.132231 6.198347 14.462810 10.330579 10.330579 ## [36] 30.991736 18.595041 16.528926 14.462810 4.132231 10.330579 8.264463 ## [43] 6.198347 14.462810 14.462810 22.727273 8.264463 10.330579 0.000000 ## [50] 6.198347 6.198347 0.000000 2.066116 0.000000 8.264463 6.198347 ## [57] 2.066116 4.132231 4.132231 0.000000 4.132231 0.000000 0.000000 ## [64] 2.066116 6.198347 2.066116 2.066116 ## ## $mids ## [1] 0.0645 0.0655 0.0665 0.0675 0.0685 0.0695 0.0705 0.0715 0.0725 0.0735 ## [11] 0.0745 0.0755 0.0765 0.0775 0.0785 0.0795 0.0805 0.0815 0.0825 0.0835 ## [21] 0.0845 0.0855 0.0865 0.0875 0.0885 0.0895 0.0905 0.0915 0.0925 0.0935 ## [31] 0.0945 0.0955 0.0965 0.0975 0.0985 0.0995 0.1005 0.1015 0.1025 0.1035 ## [41] 0.1045 0.1055 0.1065 0.1075 0.1085 0.1095 0.1105 0.1115 0.1125 0.1135 ## [51] 0.1145 0.1155 0.1165 0.1175 0.1185 0.1195 0.1205 0.1215 0.1225 0.1235 ## [61] 0.1245 0.1255 0.1265 0.1275 0.1285 0.1295 0.1305 ## ## $xname ## [1] \u0026quot;dat\u0026quot; ## ## $equidist ## [1] TRUE ## ## attr(,\u0026quot;class\u0026quot;) ## [1] \u0026quot;histogram\u0026quot; to plot three different histograms of the hidalgo data set.\nIf you do this in SAS, write a macro that accepts a table name, a column name, a list of integers, a main axis label and an x axis label. This macro should scan over each element in the list of integers and produce a histogram for each integer value, setting the bin count to the element in the input list, and labeling main and x-axis with the specified parameters. You should label the y-axis to read Frequency, bins = and the number of bins.\nTest your macro with the hidalgo data set (see below), using bin numbers 12, 36, and 60. You should be able to call your macro with something like\n%plot_histograms(hidalgo, y, 12 36 60, main=\u0026quot;1872 Hidalgo issue\u0026quot;, xlabel=\u0026quot;Thickness (mm)\u0026quot;); to plot three different histograms of the hidalgo data set.\nHint: Assume 12 36 60 resolve to a single macro parameter and use %scan. Your macro definition can look something like\n%macro plot_histograms(table_name, column_name, number_of_bins, main=\u0026quot;Main\u0026quot;, xlabel=\u0026quot;X Label\u0026quot;)  Data The hidalgo data set is in the file hidalgo.dat These data consist of paper thickness measurements of stamps from the 1872 Hidalgo issue of Mexico. This data set is commonly used to illustrate methods of determining the number of components in a mixture (in this case, different batches of paper). See https://www.jstor.org/stable/2290118,\nhttps://books.google.com/books?id=1CuznRORa3EC\u0026amp;lpg=PA95\u0026amp;pg=PA94#v=onepage\u0026amp;q\u0026amp;f=false and https://books.google.com/books?id=c2_fAox0DQoC\u0026amp;pg=PA180\u0026amp;lpg=PA180\u0026amp;f=false .\nSome analysis suggest there are three different mixtures of paper used to produce the 1872 Hidalgo issue; other analysis suggest seven. Why do you think there might be disagreement about the number of mixtures?\nThat is perhaps because of the uncontrollable variation in paper thickness people used to have from sheet to sheet, we would expect more than three or seven mixtures, but in case of hidalgo, thickness was not that high. We can see in the histogram that the thickness was maintianed below 0.07 mm with highest frequency.\n  Exercise 5. We’ve been working with data from Wansink and Payne, Table 1:\nReproducing part of Wansink Table 1     Measure 1936 1946 1951 1963 1975 1997 2006    calories per recipe (SD) 2123.8 (1050.0) 2122.3 (1002.3) 2089.9 (1009.6) 2250.0 (1078.6) 2234.2 (1089.2) 2249.6 (1094.8) 3051.9 (1496.2)  calories per serving (SD) 268.1 (124.8) 271.1 (124.2) 280.9 (116.2) 294.7 (117.7) 285.6 (118.3) 288.6 (122.0) 384.4 (168.3)  servings per recipe (SD) 12.9 (13.3) 12.9 (13.3) 13.0 (14.5) 12.7 (14.6) 12.4 (14.3) 12.4 (14.3) 12.7 (13.0)    However, in Homework 2, we also considered the value given in the text\n The resulting increase of 168.8 calories (from 268.1 calories … to 436.9 calories …) represents a 63.0% increase … in calories per serving.\n There is a discrepancy between two values reported for calories per serving, 2006. We will use graphs to attempt to determine which value is most consistent.\nFirst, consider the relationship between Calories per Serving and Calories per Recipe:\nCalories per Serving = Calories per Recipe / Servings per Recipe Since Servings per Recipe is effectively constant over time (12.4-13.0), we can assume the relationship between Calories per Serving and Calories per Recipe is linear,\n\\[ \\text{Calories per Serving} = \\beta_0 + \\beta_1 \\times \\text{Calories per Recipe} \\] with \\(\\text{Servings per Recipe} = 1/\\beta_1\\)\nWe will fit a linear model, with Calories per Recipe as the independent variable against two sets of values for Calories per Serving, such that\n Assumption 1. The value in the table (\\(384.4\\)) is correct. Assumption 2. The value in the text (\\(436.9\\)) is correct.  We use the data:\nAssumptions.dat \u0026lt;- data.frame( CaloriesPerRecipe = c(2123.8, 2122.3, 2089.9, 2250.0, 2234.2, 2249.6, 3051.9), Assumption1 = c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4), Assumption2 = c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 436.9)) and fit linear models\nAssumption1.lm \u0026lt;- lm(Assumption1 ~ CaloriesPerRecipe,data=Assumptions.dat) Assumption2.lm \u0026lt;- lm(Assumption2 ~ CaloriesPerRecipe,data=Assumptions.dat) summary(Assumption1.lm) ## ## Call: ## lm(formula = Assumption1 ~ CaloriesPerRecipe, data = Assumptions.dat) ## ## Residuals: ## 1 2 3 4 5 6 7 ## -7.0238 -3.8475 9.7610 4.7417 -2.5010 -1.3112 0.1808 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 25.477429 17.351550 1.468 0.202 ## CaloriesPerRecipe 0.117547 0.007466 15.745 1.88e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 6.163 on 5 degrees of freedom ## Multiple R-squared: 0.9802, Adjusted R-squared: 0.9763 ## F-statistic: 247.9 on 1 and 5 DF, p-value: 1.879e-05 summary(Assumption2.lm) ## ## Call: ## lm(formula = Assumption2 ~ CaloriesPerRecipe, data = Assumptions.dat) ## ## Residuals: ## 1 2 3 4 5 6 7 ## -4.1798 -0.9169 14.5608 0.3051 -6.0261 -5.7248 1.9817 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -99.891018 21.933161 -4.554 0.00609 ** ## CaloriesPerRecipe 0.175238 0.009437 18.569 8.34e-06 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 7.79 on 5 degrees of freedom ## Multiple R-squared: 0.9857, Adjusted R-squared: 0.9828 ## F-statistic: 344.8 on 1 and 5 DF, p-value: 8.336e-06  Part a. Plot the regression. Use points to plot Assumption1 vs CaloriesPerRecipe, and Assumption2 vs CaloriesPerRecipe, on the same graph. Add lines (i.e. abline) to show the fit from the regression. Use different colors for the two assumptions. Which of the two lines appears to best explain the data?\nattach(Assumptions.dat) par(mfrow=c(1,1)) plot(Assumption1~CaloriesPerRecipe, cex = 1.5, type = \u0026#39;p\u0026#39;, col = \u0026#39;red\u0026#39;) abline(Assumption1.lm) points(Assumption2~CaloriesPerRecipe, cex = 1.5, col = \u0026#39;blue\u0026#39;) abline(Assumption2.lm)  Part b. Produce diagnostic plots plots of the residuals from both linear models (in R, use residuals(Assumption1.lm)). qqnorm or box-whisker plots will probably be the most effective; there are too few points for a histogram.\nUse the code below to place two plots, side by side. You can produce more than one pair of plots, if you wise.\npar(mfrow=c(1,2)) boxplot(residuals(Assumption1.lm), main = \u0026quot;Boxplot for Assumption1\u0026quot; ) boxplot(residuals(Assumption2.lm), main = \u0026quot;Boxplot for Assumption2\u0026quot; ) par(mfrow=c(1,2)) qqnorm(residuals(Assumption1.lm), main = \u0026quot;QQplot for Assumption1\u0026quot; ) qqline(residuals(Assumption1.lm)) qqnorm(residuals(Assumption2.lm), main = \u0026quot;QQplot for Assumption2\u0026quot; ) qqline(residuals(Assumption2.lm)) From these plots, which assumption is most likely correct. That is, which assumption produces a linear model that least violates assumptions of normality of the residual errors? Which assumption produces outliers in the residuals?\nAnswer: Based on the plots, assumption1 produces a more linear model that least violates assumption of normality of the residuals errors. Assumption2 produces the outlier/s.\nI’ve included similar data and linear models for SAS in the SAS template. If you choose SAS, you will need to modify the PROC GLM code to produce the appropriate diagnostic plots.\n  ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"675639047d1f606664d26bd174d10650","permalink":"/achalneupane.github.io/post/additional_graphs/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/additional_graphs/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Additional graphs","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  Instructions There are six exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve all five problems, mixing the languages as you wish. The first four exercise refer to formula from the previous homework, you may reuse code as you wish.\nExperimental Again, you will be allowed to provide one solution using Python. Elaborate on the similarities and differences between Python arrays vs R or IML.\n  Exercise 1. Part a. We will calculate a number of required replicates for a range of mean differences, comparable to calories per serving estimates found in Wansink, Table 1.\nLet \\(m_1\\) be a sequence of means from 320-420, incremented by 10. Let \\(m_2\\) be 270. Assume a pooled standard deviation of 150.\nCalculate Cohen’s \\(d\\) for the pairs of means \\({320 - 270, 330 - 270, ...., 420 - 270}\\), letting \\(s_i = s_j = s_{pooled}\\). Calculate the required replicates for these same pairs of means. You may reuse code or functions from previous homework at your discretion.\nTo show your results, either create and print a matrix with one colum for effect size and one column for replicates, or plot required replicates versus effect size (effect size will be the independent variable). What does this tell you about the number of observations required to detect medium-size effects? You may include reference lines in your plot to illustrate.\nSince we know that \\(s_{pooled} = \\sqrt{(s_1^2 + s_2^2)/2}\\)\n# Creating a function with constant m2, pooled sd, alpha and beta. combined \u0026lt;- function (m1,m2 = 270, s_pooled = 150, alpha = 0.05, beta = 0.2){ cv \u0026lt;- (s_pooled)/((m1+m2)/2) percent.diff \u0026lt;- ((m1-m2)/((m1+m2)/2)) cohens_d \u0026lt;-(abs(m1-m2)/(s_pooled)) n \u0026lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) n \u0026lt;- round(n,0) value \u0026lt;- (list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0), EffectSize = cohens_d)) return(value) } m1 \u0026lt;- seq(320,420,10) m1 ## [1] 320 330 340 350 360 370 380 390 400 410 420 data \u0026lt;- combined(m1 = m1) # cal.lm \u0026lt;- lm(data$RequiredReplicates ~ data$EffectSize) # or even quadratic term # cal.lm \u0026lt;- lm(data$RequiredReplicates ~ poly(data$EffectSize, 3, raw = TRUE)) plot(data$EffectSize, data$RequiredReplicates) # abline(cal.lm) # As per rule of thumb for medium-size effect, we can choose v= 0.5 as medium-size effects # http://staff.bath.ac.uk/pssiw/stats2/page2/page14/page14.html # Also, http://staff.bath.ac.uk/pssiw/stats2/page2/page14/page14.html abline(v = 0.5, col= \u0026#39;red\u0026#39;) Note: Cohen suggested that d = 0.2 be considered a ‘small’ effect size, 0.5 represents a ‘medium’ effect size and 0.8 a ‘large’ effect size. This means that if two groups’ means don’t differ by 0.2 standard deviations or more, the difference is trivial, even if it is statistically signficant. This plot tells us that with the increasing effect-size, we need fewer replicates. Since effect-size of |0.5| or (v= 0.5) is intersects with the abline at approximately 78 replicates in the plot, it also tells us that for medium-size effect we need about 78 replicates.\n  Exercise 2 Create a table to show the required replicates for a range of combinations of \\(\\%Diff\\) and \\(CV\\). Do this in steps as follows:\nPart a. Define two matrices, one for CV and one for Diff. Each will matrix will be 5 rows by 6 columns. Let the rows in CV be the sequence \\(8, 12, ..., 28\\) and let the columns of Diff be the squence \\(5,10, ... , 25\\). The matrices should look like:\n\\[ \\begin{aligned} CV \u0026amp; = \\left\\{ \\begin{array}{cccccc} 8 \u0026amp; 12 \u0026amp; 16 \u0026amp; 20 \u0026amp; 24 \u0026amp; 28 \\\\ 8 \u0026amp; 12 \u0026amp; 16 \u0026amp; 20 \u0026amp; 24 \u0026amp; 28 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 8 \u0026amp; 12 \u0026amp; 16 \u0026amp; 20 \u0026amp; 24 \u0026amp; 28 \\\\ \\end{array} \\right\\} \\\\ \u0026amp; \\\\ \\%Diff \u0026amp; = \\left\\{ \\begin{array}{ccccc} 5 \u0026amp; 5 \u0026amp; 5 \u0026amp; 5 \u0026amp; 5 \\\\ 10 \u0026amp; 10 \u0026amp; 10 \u0026amp; 10 \u0026amp; 10 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 25 \u0026amp; 25 \u0026amp; 25 \u0026amp; 25 \u0026amp; 25 \\\\ \\end{array} \\right\\} \\end{aligned} \\]\nDefine and print your matrices in the code below.\n# create sequence data and then matrix for CV rep.cv \u0026lt;- rep(seq(8,28,4),5) rep.cv ## [1] 8 12 16 20 24 28 8 12 16 20 24 28 8 12 16 20 24 28 8 12 16 20 24 ## [24] 28 8 12 16 20 24 28 cv \u0026lt;- matrix(rep.cv, nrow = 5, ncol = 6, byrow = TRUE) cv ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 8 12 16 20 24 28 ## [2,] 8 12 16 20 24 28 ## [3,] 8 12 16 20 24 28 ## [4,] 8 12 16 20 24 28 ## [5,] 8 12 16 20 24 28 # create sequence data and then matrix for %Diff rep.Diff \u0026lt;- rep(seq(5,25,5), 6) rep.Diff ## [1] 5 10 15 20 25 5 10 15 20 25 5 10 15 20 25 5 10 15 20 25 5 10 15 ## [24] 20 25 5 10 15 20 25 Diff \u0026lt;- matrix(rep.Diff, nrow = 5, ncol = 6, byrow = FALSE) Diff ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 5 5 5 5 5 5 ## [2,] 10 10 10 10 10 10 ## [3,] 15 15 15 15 15 15 ## [4,] 20 20 20 20 20 20 ## [5,] 25 25 25 25 25 25  Part b. Calculate require replicates for each combination of CV and Diff. Use the same values for \\(z_\\alpha\\) and \\(z_\\beta\\) as from Homework 2 and 3. You should be able to reuse coce from previous exercises, and you should not use iteration.\nPrint the result below. The result should be a \\(5 \\times 6\\) matrix.\n# Now vectorize the matrices CV and %Diff from above cv.vector \u0026lt;- as.vector(cv) cv.vector ## [1] 8 8 8 8 8 12 12 12 12 12 16 16 16 16 16 20 20 20 20 20 24 24 24 ## [24] 24 24 28 28 28 28 28 Diff.vector \u0026lt;- as.vector(Diff) Diff.vector ## [1] 5 10 15 20 25 5 10 15 20 25 5 10 15 20 25 5 10 15 20 25 5 10 15 ## [24] 20 25 5 10 15 20 25 # We can then use the function in exercise 1 by slightly modifying it as: combined \u0026lt;- function (cv, percent.diff, alpha = 0.05, beta = 0.2){ cv \u0026lt;- cv percent.diff \u0026lt;- percent.diff n \u0026lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) n \u0026lt;- round(n,0) value \u0026lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0)) return(value) } value \u0026lt;- combined(cv = cv.vector, percent.diff = Diff.vector) RequiredReplicates \u0026lt;- matrix(value$RequiredReplicates, nrow = 5, ncol = 6, byrow = FALSE) RequiredReplicates ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 40 90 161 251 362 492 ## [2,] 10 23 40 63 90 123 ## [3,] 4 10 18 28 40 55 ## [4,] 3 6 10 16 23 31 ## [5,] 2 4 6 10 14 20 To check your work, repeat the calculations using the rule of thumb from the previous exercises. What is largest deviation of the rule of thumb from the exact calculation?\nFor this, first we can simplify the equations as follows: first for CV,\n\\(CV = \\frac{sd_{pooled}}{(m_1 + m_2)/2}\\) \\((m_1 + m_2)/2) = \\frac{sd_{pooled}}{CV/2}\\) \\((m_1 + m_2)/2) = {2} \\times\\frac{sd_{pooled}}{CV}\\)\nthen, for \\(\\%diff\\):\n\\(\\%Diff = \\frac{m_1 - m_2}{(m_1 + m_2)/2}\\) \\(\\%Diff\\times\\frac{(m_1 + m_2)}{2} = m_1-m_2\\) \\(m_1-m_2 = \\frac{\\%Diff}{2}\\times(m_1 + m_2)\\)\nNow, if we replace \\(m_1 + m_2\\), we get: \\(m_1-m_2 = \\frac{\\%Diff}{2}\\times ({2} \\times\\frac{sd_{pooled}}{CV})\\)\nOr, \\(m_1-m_2 = \\frac{{\\%Diff } \\ \\times\\ {sd_{pooled}}}{CV}\\)\nThis will give us delta (\\(\\triangle\\)): \\(\\triangle = \\frac{m_1-m_2}{sd_{pooled}} = \\frac{1}{sd_{pooled}} \\times \\frac{{\\%Diff } \\ \\times\\ {sd_{pooled}}}{CV} = \\frac{\\%Diff}{CV}\\)\nNow we can use, rule of thumb as: \\(n = \\frac{16}{\\triangle^2}\\)\n# As stated in rule of thumb: http://www.nrcse.washington.edu/research/struts/chapter2.pdf # We can then use the simplified equation rule.of.thumb.n \u0026lt;- function (cv, percent.diff){ cv \u0026lt;- cv percent.diff \u0026lt;- percent.diff delta \u0026lt;- percent.diff/cv n \u0026lt;- (16/(delta^2)) value \u0026lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0)) return(value) } value \u0026lt;- rule.of.thumb.n(cv= cv.vector, percent.diff = Diff.vector) Rule.of.Thumb.matrix \u0026lt;- matrix(value$RequiredReplicates, nrow = 5, ncol = 6, byrow = FALSE) colnames(Rule.of.Thumb.matrix) \u0026lt;- paste0(\u0026quot;CV\u0026quot;,unique(value$CV)) rownames(Rule.of.Thumb.matrix) \u0026lt;- paste0(\u0026quot;Diff\u0026quot;,unique(value$PercentDiff)) # This matrix gives you the combination of all pairs of CV and %Diff for rule of thumb Rule.of.Thumb.matrix ## CV8 CV12 CV16 CV20 CV24 CV28 ## Diff5 41 92 164 256 369 502 ## Diff10 10 23 41 64 92 125 ## Diff15 5 10 18 28 41 56 ## Diff20 3 6 10 16 23 31 ## Diff25 2 4 7 10 15 20 # We can then compare the matrix from exact calculation and required replicates # calculated using rule of thumb method by calculating the percent difference of # two data matrices percent_difference_between_two_df \u0026lt;- data.frame((abs(RequiredReplicates-Rule.of.Thumb.matrix)/(abs(RequiredReplicates+Rule.of.Thumb.matrix)/2))*100) percent_difference_between_two_df ## CV8 CV12 CV16 CV20 CV24 CV28 ## Diff5 2.469136 2.197802 1.846154 1.972387 1.915185 2.012072 ## Diff10 0.000000 0.000000 2.469136 1.574803 2.197802 1.612903 ## Diff15 22.222222 0.000000 0.000000 0.000000 2.469136 1.801802 ## Diff20 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ## Diff25 0.000000 0.000000 15.384615 0.000000 6.896552 0.000000 Therefore, %Diff of 15 and CV of 8 has the highest deviation of 22.22% between calculated vs rule of thumb method for required replicates.\n  Exercise 3 In this exercise, we’ll use your norm.pdf function to illustrate how the formula for required replicates finds a compromise between Type I and Type II error rates. This is also a way to test your normal probability function over a range of arguments.\nDo not print the vectors you create for this exercise in the final typeset submission We will check the results by examining the plots, and printing the vectors themselves will unnecessarily clutter your report. If you get stuck, use the built normal functions to create your plots.\nPart a. Generate a squence of values from \\(-3,...,4\\) incremented by \\(0.1\\); let this be x. Calculate the probability of each value of x using the norm.pdf function from Homework 3, letting mu = 0 and sd = 1. Name the result p.null.\nCalculate the effect size for 1936 versus 2006, calories per serving, as in Homework 2 and 3. Repeat the calculation for the probability of x, but this time use mean= effect size. Name this result p.alt.\nThe results will be the distribution of the expected value of the difference between means; the first is the expectation under the null hypothesis (the true effect size is 0) while the second is the expectation assuming the measured \\(d\\) is the true \\(d\\).\n# Function to calcuate the values for log liklihood. # First, we define the values for sigma as variance, # mu as mean of a normal population to be used for a liklihood of a x observation. norm.pdf \u0026lt;- function(x,mu = 0,sigma = 1){ l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } x \u0026lt;- seq(-3, 4, 0.1) p.null \u0026lt;- norm.pdf(x) #calculating effect size to be used as \u0026#39;mean= \u0026#39; for the analysis below combined \u0026lt;- function (m1,m2,s1,s2, alpha = 0.05, beta = 0.2){ cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) value \u0026lt;- (list(EffectSize = cohens_d)) } #calculate the effect size for 1936 vs 2006, calories per serving m1 = 268.1 m2 = 384.4 s1 = 124.8 s2 = 168.3 value \u0026lt;- combined(m1 = m1, m2 = m2, s1 = s1, s2 = s2) value$EffectSize ## [1] 0.7849876 # now repeating the calculation mean = value$EffectSize mean ## [1] 0.7849876 p.alt \u0026lt;- norm.pdf(x, mu = mean)  Part b. Repeat the calculations of p.null and p.alt, but this time let sigma = \\(\\sqrt{2/n}\\) where \\(n = 10\\). Name these p.null.10 and p.alt.10. These calculations narrow the distribbutions by an amount proportional to standard error.\nn \u0026lt;- 10 norm.pdf \u0026lt;- function(x, n, mu = 0){ sigma = (sqrt(2/n)) l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } x \u0026lt;- seq(-3, 4, 0.1) p.null.10 \u0026lt;- norm.pdf(x = x, n = n) mean = value$EffectSize mean ## [1] 0.7849876 p.alt.10 \u0026lt;- norm.pdf(x = x, n = n, mu = mean)  Part c. Repeat the calculations of p.null and p.alt, but this time let sigma = \\(\\sqrt{2/n}\\) where \\(n\\) is the minimun mumber of replicates for calories per recipe, 1936 versus 2006, as calculated previously. Call these p.null.req and p.alt.req.\n# Re-using the function to calculate minimum required replicates required.replicates \u0026lt;- function (m1,m2, s1,s2, alpha = 0.05, beta = 0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) return(round(n,0)) } m1 = 268.1 m2 = 384.4 s1 = 124.8 s2 = 168.3 n \u0026lt;- required.replicates(m1 = m1, m2 = m2, s1 = s1, s2 = s2) n.min.replicates \u0026lt;- n n.min.replicates ## [1] 25 norm.pdf \u0026lt;- function(x, n, mu = 0){ sigma = (sqrt(2/n)) l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } x \u0026lt;- seq(-3, 4, 0.1) p.null.req \u0026lt;- norm.pdf(x = x, n = n) mean = value$EffectSize mean ## [1] 0.7849876 p.alt.req \u0026lt;- norm.pdf(x = x, n = n, mu = mean)  Part d. Plot p.null versus x as a black line and in the same plot add p.alt vs x as a red line. Add a green vertical line at \\(z_\\alpha\\) and a blue vertical line at \\(z_\\beta\\), using values as in previous exercises. The green line represents the critical value for Type I error, and the error under the black curve to the left of the green line is the probability of that error (97.5%). The area under the red curve, to the left of the green line, represents the achieved Type II error rate, the blue line represents the desired Type II rate.\nRepeat the plot with p.null.10 and p.alt.10, but this time add vertical lines at \\(z_\\alpha \\times \\sqrt{2/10}\\) and at \\(z_\\beta \\times \\sqrt{2/10}\\). The lines representing critical values for Type I and Type II error should move closer as the distributions narrow.\nRepeat the plot with p.null.req and p.alt.req, but this time add vertical lines at \\(z_\\alpha \\times \\sqrt{2/n}\\) and at \\(z_\\beta \\times \\sqrt{2/n}\\), where \\(n\\) is the minimum replicates. Do the lines for Type I and Type II error overlap?\nIt will improve the readability of the three plots if you plot all three in the chunk below. The arguments inside the braces specify the dimensions of the plot, while par(mfrow = c(3,1)) combines three plots into one graph.\npar(mfrow = c(3,1)) # using base plot function part A alpha = 0.05; beta = 0.2 Zalpha \u0026lt;- qnorm(1-alpha/2) Zbeta \u0026lt;- qnorm(1-beta) plot(x,p.null,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x,p.alt,col=\u0026quot;red\u0026quot;) abline(v = Zalpha, col= \u0026#39;green\u0026#39;) abline(v = value$EffectSize-Zbeta, col= \u0026#39;blue\u0026#39;) ### using ggplot # plot1 \u0026lt;- ggplot()+ # geom_line(aes(x = x, y = p.null), color = \u0026quot;black\u0026quot; ) + # geom_line(aes(x = x, y = p.alt), color = \u0026quot;red\u0026quot;) + # geom_vline(xintercept = Zalpha, color = \u0026quot;green\u0026quot;)+ # geom_vline(xintercept = value$EffectSize - Zbeta, color = \u0026quot;blue\u0026quot;) # plot1 # using base plot function Part B alpha = 0.05; beta = 0.2; n \u0026lt;- 10 Zalpha \u0026lt;- qnorm(1-alpha/2) Zalpha \u0026lt;- Zalpha * sqrt(2/n) Zbeta \u0026lt;- qnorm(1-beta) Zbeta \u0026lt;- Zbeta * sqrt(2/n) plot(x,p.null.10,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x,p.alt.10,col=\u0026quot;red\u0026quot;) abline(v = Zalpha, col= \u0026#39;green\u0026#39;) abline(v = (value$EffectSize-Zbeta), col= \u0026#39;blue\u0026#39;) ## Using ggplot # plot2 \u0026lt;- ggplot()+ # geom_line(aes(x = x, y = p.null.10), color = \u0026quot;black\u0026quot; ) + # geom_line(aes(x = x, y = p.alt.10), color = \u0026quot;red\u0026quot;) + # geom_vline(xintercept = Zalpha, color = \u0026quot;green\u0026quot;) + # geom_vline(xintercept = value$EffectSize - Zbeta, color = \u0026quot;blue\u0026quot;) # plot2 # using base plot function Part C alpha = 0.05; beta = 0.2; n \u0026lt;- n.min.replicates Zalpha \u0026lt;- qnorm(1-alpha/2) Zalpha \u0026lt;- Zalpha * sqrt(2/n) Zbeta \u0026lt;- qnorm(1-beta) Zbeta \u0026lt;- Zbeta * sqrt(2/n) plot(x,p.null.req,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x,p.alt.req,col=\u0026quot;red\u0026quot;) abline(v = Zalpha, col= \u0026#39;green\u0026#39;) abline(v = value$EffectSize-Zbeta, col= \u0026#39;blue\u0026#39;) ## Using ggplot # plot3 \u0026lt;- ggplot()+ # geom_line(aes(x = x, y = p.null.req), color = \u0026quot;black\u0026quot; ) + # geom_line(aes(x = x, y = p.alt.req), color = \u0026quot;red\u0026quot;) + # geom_vline(xintercept = Zalpha, color = \u0026quot;green\u0026quot;) + # geom_vline(xintercept = value$EffectSize - Zbeta, color = \u0026quot;blue\u0026quot;) # plot3 # # Then merge all ggplots: # library(grid) # library(gridExtra) # grid.newpage() # grid.draw(arrangeGrob(plot1, plot2, plot3, heights = c(1/3, 1/3, 1/3)) ) Here, The lines representing critical values for Type I and Type II error move closer as the distributions narrow. Yes, the lines for Type I and Type II error overlap.\nNote: z_beta should be plotted relative to the alternative hypothesis value (in this case, d), since z_beta is the critical value to control for Type II error, where we would (fail to reject)* the null hypothesis and miss a true effect. Thus, the line for z_beta needs to be plotted relative to d, and it needs to be offset to the left, so you should be plotting a line at d-z_beta, etc. The alternative hypothesis is that the effect size we measured, d, is the true effect size. Perhaps another way to phrase this is, the right-hand distribution illustrates values of the true effect size d that are consistent with a measured effect size. There’s a big debate in statistics on the use of null hypothesis tests. Perhaps a better visualization would be to demonstrate how narrowing the distributions (2, 10, n) changes likelihood ratio or Bayes factor.\nIf you choose to solve this with SAS, I’ve included code in the SAS template to create the graphs, since combining plots in IML is not as easy as in R.\n  Exercise 4 In this, we compare the normal and Poisson distributions, using the functions you’ve written previously. This is also a way to test your normal and Poisson functions over a range of arguments.\nDo not print the vectors you create for this exercise in the final typeset submission We will check the results by examining the plots, and printing the vectors themselves will unnecessarily clutter your report. If you get stuck, use the built functions to create your plots. However, the final submission must call your functions.\nPart a Create a sequence of \\(x_a\\) from \\(( -5 ... 5 )\\), incremented by 0.1. Calculate the normal likelihood for each \\(x\\), assuming \\(\\mu = 0\\) and \\(\\sigma = 1\\). Also calculate Poisson probability of each \\(x\\) given a lambda = 1.\nNOTE: The Poisson parameter Lambda (λ) is the total number of events (k) divided by the number of units (n) in the data (λ = k/n)\nPlot both sets of probablities against x as lines, using a different color for each curve. Make sure that both curves fit in the plot; you may need to determine minimum and maximum values and set these as graphic parameters (see ylim).\nWarning: if you do this in SAS, you may have to adjust the lower bound of \\(x\\).\n# Suppressing warnings that may be generated by Poisson function for negative values # options(warn=-1) x_a \u0026lt;- seq(-5, 5, 0.1) x_a ## [1] -5.0 -4.9 -4.8 -4.7 -4.6 -4.5 -4.4 -4.3 -4.2 -4.1 -4.0 -3.9 -3.8 -3.7 ## [15] -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 ## [29] -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 ## [43] -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 ## [57] 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 ## [71] 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 ## [85] 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 ## [99] 4.8 4.9 5.0 norm.pdf \u0026lt;- function(x,mu=0,sigma=1){ l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } normal.liklihood.x_a \u0026lt;- norm.pdf(x_a) # The function to calculate probability mass function for poisson # data with a mean and variance lambda = 1. pois.pmf \u0026lt;- function(x, lambda){ poisson.d \u0026lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda))) return(poisson.d) } lambda \u0026lt;- 1 poisson.probability.x_a \u0026lt;- pois.pmf(x=x_a, lambda = lambda) ## Warning in gamma(x + 1): NaNs produced plot(x_a,normal.liklihood.x_a,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_a,poisson.probability.x_a,col=\u0026quot;red\u0026quot;) Does this graph tell you if your Normal PDF function behaves properly? Does your Poisson handle negative or non-integer values as expected?\nNo, based on this plot, the normal pdf does behave properly, but not the poisson as there NAs inserted for negative and non-integer values.\n Part b Create a sequence of \\(x_b = \\left \\lfloor{\\mu - 5 \\times \\sigma } \\right \\rfloor , \\dots, \\left \\lceil{\\mu+ 5 \\times \\sigma } \\right \\rceil\\) using mean and standard deviation for servings per recipe from 1936.\nCalculate the normal and Poission probability for each \\(x\\) as in part a, again using mean and standard deviation from servings per recipe, 1936. The length of this vector should be the same length as the \\(x\\) vector as in part a (\\(\\pm 1\\)), so you will need to calculate an interval based on the range x_b and the number of elements in x_a\nShow the the length of both \\(x\\) vectors are similar by calling length for each.\nRepeat the plot from part a with this sequence.\nIf you choose to solve this with SAS, I’ve included code in the SAS template to create the graphs, since combining plots in IML is not as easy as in R.\n# Using mean and standard deviation for servings per recipe from 1936: mu = 12.9; sigma = 13.3 # Now we find the upper and lower bounds as: x.lower \u0026lt;- floor(mu-5*sigma) x.upper \u0026lt;- ceiling(mu+5*sigma) # Now taking the length of x_a as a reference, we create the equeally spaced # sequence from lower to upper bound as followed: spacer \u0026lt;- (x.upper - x.lower)/(length(x_a) - 1) x_b \u0026lt;- seq(x.lower, x.upper, spacer) # To show both x_a and x_b \u0026#39;s lenghts are equal: length(x_a) ## [1] 101 length(x_b) ## [1] 101 norm.pdf \u0026lt;- function(x, mu = 12.9, sigma = 13.3){ l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } normal.liklihood.x_b \u0026lt;- norm.pdf(x_b) # The function to calculate probability mass function for poisson # data with a mean and variance lambda = 12. pois.pmf \u0026lt;- function(x, lambda){ poisson.d \u0026lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda))) return(poisson.d) } #using sigma = 13.3 to compare the difference with the first plot poisson.probability.x_b \u0026lt;- pois.pmf(x=x_b, lambda = 12) ## Warning in gamma(x + 1): NaNs produced # plot plot(x_b,normal.liklihood.x_b,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_b,poisson.probability.x_b,col=\u0026quot;red\u0026quot;) To check you work, duplicate the plots by calling built in normal and Poisson functions. Does the system Poisson function handle negative \\(x\\) differently than your function?\n# Using base functions for part a base.normal.liklihood.x_a \u0026lt;- dnorm(x_a,0, 1) base.poiss.x_a \u0026lt;- dpois(x=x_a, lambda = lambda) ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.900000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.100000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.200000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.300000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.400000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.500000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.600000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.700000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.800000 ## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.900000 plot(x_a,base.normal.liklihood.x_a,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_a,base.poiss.x_a,col=\u0026quot;red\u0026quot;) # Using base functions for part b base.normal.liklihood.x_b \u0026lt;- dnorm(x_b,0, 1) base.poiss.x_b \u0026lt;- dpois(x=x_b, lambda = 12) ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -52.660000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -51.320000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -49.980000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -48.640000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -47.300000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -45.960000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -44.620000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -43.280000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -41.940000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -40.600000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -39.260000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -37.920000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -36.580000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -35.240000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -33.900000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -32.560000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -31.220000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -29.880000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -28.540000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -27.200000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -25.860000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -24.520000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -23.180000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -21.840000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -20.500000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -19.160000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -17.820000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -16.480000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -15.140000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -13.800000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -12.460000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -11.120000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -9.780000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -8.440000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -7.100000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -5.760000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -4.420000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -3.080000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -1.740000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = -0.400000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 0.940000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 2.280000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 3.620000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 4.960000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 6.300000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 7.640000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 8.980000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 10.320000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 11.660000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 14.340000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 15.680000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 17.020000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 18.360000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 19.700000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 21.040000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 22.380000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 23.720000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 25.060000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 26.400000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 27.740000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 29.080000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 30.420000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 31.760000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 33.100000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 34.440000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 35.780000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 37.120000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 38.460000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 39.800000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 41.140000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 42.480000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 43.820000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 45.160000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 46.500000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 47.840000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 49.180000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 50.520000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 51.860000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 53.200000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 54.540000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 55.880000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 57.220000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 58.560000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 59.900000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 61.240000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 62.580000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 63.920000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 65.260000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 66.600000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 67.940000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 69.280000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 70.620000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 71.960000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 73.300000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 74.640000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 75.980000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 77.320000 ## Warning in dpois(x = x_b, lambda = 12): non-integer x = 78.660000 plot(x_b,base.normal.liklihood.x_b,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_b,base.poiss.x_b,col=\u0026quot;red\u0026quot;) Yes, the system Poisson function handles negative values and non-integers differently (inserts zero’s) whereas the function we wrote inserts NAs for negative and non-integer values.\n  Exercise 5 Consider the table:\n  Rate 23000 24000 25000 26000 27000 28000 29000    Yield 111.4216 155.0326 181.1176 227.5800 233.4623 242.1753 231.3890    Suppose we wish to determine the linear relationship between per Rate and Yield. We can determine this by solving a system of linear equations, of the form\n\\[ \\begin{aligned} 111.4216 \u0026amp; = \\beta_1 + \\beta_2 \\times 23000 \\\\ 155.0326 \u0026amp; = \\beta_1 + \\beta_2 \\times 24000 \\\\ \\vdots \u0026amp; = \\vdots \\\\ 231.3890 \u0026amp; = \\beta_1 + \\beta_2 \\times 29000 \\\\ \\end{aligned} \\]\nWe write this in matrix notation as\n\\[ \\left(\\begin{array}{c} 111.4216 \\\\ 155.0326 \\\\ \\vdots \\\\ 231.3890 \\end{array}\\right) = \\left(\\begin{array}{rr} 1 \u0026amp; 23000 \\\\ 1 \u0026amp; 24000 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; 29000 \\end{array}\\right) \\left(\\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\end{array}\\right)^t \\]\nWe might write this as\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} \\]\nand find a solution by computing \\(\\mathbf{\\hat{\\beta}} = \\mathbf{X}^{- 1}\\mathbf{y}\\).\nHowever, an exact solution for the inverse, \\(\\mathbf{X}^{- 1}\\) require square matrices, so commonly we use the normal equations,\n\\[ \\mathbf{X}^{t} \\mathbf{y} = \\mathbf{X}^{t} \\mathbf{X} \\mathbf{\\beta} \\] (where \\(\\mathbf{X}^{t}\\) is the transpose of \\(\\mathbf{X}\\)). We then find \\(\\hat{\\mathbf{\\beta}} = \\mathbf{X}^{t} \\mathbf{X} ^{-1} \\mathbf{X}^{t} \\mathbf{y}\\)\nAnswer Define appropriate X and y matrices (y can be a vector in R) in the chunk below.\nMultiply the transpose of X by X, then use solve (R) or inv (IML) to find the inverse. Multiply this by the product of transpose X and y to find hat.beta.\nPrint your hat.beta.\ny \u0026lt;- matrix( c(111.4216, 155.0326, 181.1176, 227.5800, 233.4623, 242.1753, 231.3890), byrow = FALSE) y ## [,1] ## [1,] 111.4216 ## [2,] 155.0326 ## [3,] 181.1176 ## [4,] 227.5800 ## [5,] 233.4623 ## [6,] 242.1753 ## [7,] 231.3890 #creating a matrix for bias term bias=rep(1:1, length.out=length(y)) bias ## [1] 1 1 1 1 1 1 1 cx \u0026lt;- c(23000, 24000, 25000, 26000 , 27000, 28000, 29000) X=matrix(c(bias,cx), ncol = 2) X ## [,1] [,2] ## [1,] 1 23000 ## [2,] 1 24000 ## [3,] 1 25000 ## [4,] 1 26000 ## [5,] 1 27000 ## [6,] 1 28000 ## [7,] 1 29000 #multplication of transpose of x and x tX=t(X) tX ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 1 1 1 1 1 1 ## [2,] 23000 24000 25000 26000 27000 28000 29000 Xm=tX%*%X Xm ## [,1] [,2] ## [1,] 7 1.82e+05 ## [2,] 182000 4.76e+09 A=solve(Xm) hat.beta=A%*%(tX%*%y) hat.beta ## [,1] ## [1,] -347.18307857 ## [2,] 0.02094758 To check your work, calculate the values predicted by your statistical model. Compute hat.y by multiplying X and hat.beta, \\[\\hat{y} = \\mathbf{X} \\hat{\\beta}\\] Plot y vs the independent variable (the second column of X) as points, and hat.y vs independent variable as a line, preferably a different colors. The hat.y values should fall a straight line that interpolates y values.\n# compute hat.y hat.y \u0026lt;- X %*% hat.beta # plot plot(X[,2], y, type = \u0026#39;l\u0026#39;, col = \u0026quot;black\u0026quot;) lines(X[,2], hat.y, col = \u0026quot;blue\u0026quot;) You can also compare your result to the R function (set eval = TRUE).\nsummary(lm(y~X)) ## ## Call: ## lm(formula = y ~ X) ## ## Residuals: ## 1 2 3 4 5 6 7 ## -23.1897 -0.5263 4.6111 30.1259 15.0607 2.8261 -28.9078 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.472e+02 1.110e+02 -3.127 0.0260 * ## X1 NA NA NA NA ## X2 2.095e-02 4.257e-03 4.920 0.0044 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.53 on 5 degrees of freedom ## Multiple R-squared: 0.8288, Adjusted R-squared: 0.7946 ## F-statistic: 24.21 on 1 and 5 DF, p-value: 0.004396 Based on my calculation the beta estimates are similar to the beta estimates from the liner model\nAlternative methods You can also compute \\(\\hat{\\beta}\\) by passing both \\(\\mathbf{X}^{t} \\mathbf{X} ^{-1}\\) and \\(\\mathbf{X}^{t} \\mathbf{y}\\) as arguments to solve.\nAlternatively, you can install the MASS library and use ginv to compute a generalized inverse \\(\\mathbf{X}^{- 1}\\). Use this to compute \\(\\mathbf{\\hat{\\beta}} = \\mathbf{X}^-\\mathbf{y}\\) in the chunk below:\nlibrary(MASS)    Exercise 6 Given a vector of mean estimates \\(x = x_1, x_2, \\dots, x_k\\), a vector of standard deviations \\(s = s_1, s_2, \\dots, s_k\\) and a vector of sample sizes \\(n = n_1, n_2, \\dots, n_k\\), we can calculate a one-way analysis of variance by\n\\[ MSB = \\frac{n_1(x_1-\\bar{x})^2 + n_2(x_2-\\bar{x})^2 + \\dots + n_k(x_k-\\bar{x})^2} {k-1} = \\frac{\\sum_i n_i(x_i-\\bar{x})^2}{k-1} \\] and\n\\[ MSW = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\dots (n_k-1)s_k^2 }{N-k} = \\frac{\\sum_i (n_i-1)s_i^2}{N-k} \\]\nwhere \\(\\bar{x}\\) is the mean of \\(x_i\\) and \\(N = \\sum_i n_i\\). The test statistic is \\(F = \\frac{MSB}{MSW}\\) which is distributed as \\(F_{\\alpha,k-1,N-k}\\)\nPart a Calculate MSW and MSB for Calories per Serving from Wansink Table 1. You can use the variables CaloriesPerServingMean and CaloriesPerServingSD defined below. Let \\(n_1 = n_2 ... = n_k = 18\\)\nUse array functions and arithmetic for your calculations, you should not need iteration (for loops). Do not hard code values for \\(N\\) and \\(k\\), calculate these from the CaloriesPerServingMean or CaloriesPerServingSD.\nPrint both MSB and MSW.\nCaloriesPerServingMean \u0026lt;- c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4) CaloriesPerServingSD \u0026lt;- c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3) #mean for servingperrecipe mean = mean(CaloriesPerServingMean) mean ## [1] 296.2 k \u0026lt;- length(CaloriesPerServingMean) k ## [1] 7 # We have 18 samples in each year group; so our N is: n \u0026lt;- rep(18, k) N \u0026lt;- sum(n) #calculating MSB mean.CaloriesPerServingMean \u0026lt;- mean(CaloriesPerServingMean) MSB = (sum(n*(CaloriesPerServingMean-mean.CaloriesPerServingMean)^2))/(k-1) MSB ## [1] 28815.96 MSW \u0026lt;- sum((n-1) * CaloriesPerServingSD^2)/(N-k) MSW ## [1] 16508.6  Part b Calculate an F-ratio and a \\(p\\) for this \\(F\\), using the \\(F\\) distribution with \\(k-1\\) and \\(N-k\\) degrees of freedom. Use \\(\\alpha = 0.05\\). Compare these values to the corresponding values reported in Wansink Table 1.\n# Calculating F.ratio F.ratio \u0026lt;- MSB/MSW df1 \u0026lt;- k-1 df2 \u0026lt;- N-k p.value \u0026lt;- pf(F.ratio, df1, df2, lower.tail = FALSE) p.value ## [1] 0.1163133 You can check your results by entering appropriate values in an online calculator like http://statpages.info/anova1sm.html .\nGroup Name N(count) Mean Std.Dev Group1 18 268.1 124.8 Group2 18 271.1 124.2 Group3 18 280.9 116.2 . . . Group7 18 384.4 168.3\nThen set Desired confidence level for post-hoc confidence intervals: 5\nFinally, cross-check your answers!\n  ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"5e2bf293f06f951ed61c4ed11ddec598","permalink":"/achalneupane.github.io/post/arrays/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/arrays/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Arrays","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  General instructions. There are 5 exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve all five problems, mixing the languages as you wish. Warning - we will be reusing the formulas from the first three exercises in later homework, so if you implement them now later exercises will be easier.\nExperimental I’ve been arguing that this course should also include Python. To explore this idea, I’ll allow one solution (10 of your 50 points) to be implemented in Python. To get full credit for a Python solution :\n Solve one of the first three exercises.\n Explain how the Python solution differs from the corresonding R or SAS solution. Note the differences in the languages. For example, does Python use the same assignment operator? Are all math operators the same as R or SAS? Are the math or statistics functions loaded by default? For the first exercise, I’ve found three important differences between R and Python.\n You can include Python in RMarkdown by replacing r with python in the code chunk prefix.\n I won’t be teaching Python this summer, but if you’re familiar with Python, this may help understand the inner workings of R or SAS.\n    Exercise 1 Cohen gives a formula for effect size, \\(d\\), for the difference between two means \\(m_1\\) and \\(m_2\\), as\n\\[ d = \\frac{|m_1-m_2|}{s_{pooled}} \\] where \\(s_{pooled}\\) is a pooled standard deviation. Use the formula \\(s_{pooled} = \\sqrt{(s_1^2 + s_2^2)/2}\\).\nCalculate the effect size \\(d\\) for the differences among calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006. Use the values from Wansink, Table 1 as given in Homework 1 or in the course outline.\nAnswer Enter the R code in the chunks below. If you choose SAS for this exercise, use the marked portion in the SAS homework template.\n1936 versus 2006 #calories per serving #for this calcuation, m1 and m2 are mean for calories per serving for 1936 and 2006 respectively. s1 and s2 are standard deviation for 1936 vs 2006. m1=268.1 m2=384.4 s1=124.8 s2=168.3 d_1936_2006 \u0026lt;- (abs(m1-m2)/sqrt((s1^2+s2^2)/2)) d_1936_2006 ## [1] 0.7849876  1936 versus 1997 #calories per serving #for this calcuation, m1 and m2 are mean for calories per serving for 1936 and 1997 respectively. s1 and s2 are standard deviation for 1936 and 1997. m1=268.1 m2=288.6 s1=124.8 s2=122.0 d_1936_1997 \u0026lt;- (abs(m1-m2)/sqrt((s1^2+s2^2)/2)) d_1936_1997 ## [1] 0.1661157  1997 versus 2006 #calories per serving #for this calcuation, m1 and m2 are mean for calories per serving for 1997 and 2006 respectively. s1 and s2 are standard deviation for 1997 vs 2006. m1=288.6 m2=384.4 s1=122.0 s2=168.3 d_1997_2006 \u0026lt;- (abs(m1-m2)/sqrt((s1^2+s2^2)/2)) d_1997_2006 ## [1] 0.6517694 To check your work, consider that Cohen recommends that \\(d=0.2\\) be considered a small effect, \\(d=0.5\\) a medium effect and \\(d=0.8\\) a large effect. I don’t find any of these to be fully large effects.\nHere, I also found none of these to be of larger effect.\n   Exercise 2. Suppose you are planning an experiment and you want to determine how many observations you should make for each experimental condition. One simple formula (see Kuehl, “Design of Experiments : Statistical Principles of Research Design and Analysis”) for the required replicates \\(n\\) is given by\n\\[ n \\ge 2\\times \\left( \\frac{CV}{\\%Diff} \\right)^2 \\times \\left(z_{\\alpha/2}+ z_\\beta \\right)^2 \\] where \\(\\%Diff = \\frac{m_1 - m_2}{(m_1 + m_2)/2}\\) and \\(CV = \\frac{sd_{pooled}}{(m_1 + m_2)/2}\\).\nUse this formula to calculate the number of replicates required to detect differences between calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006. You will need to research how to use the normal distribution functions (*norm in R, ). Use \\(\\alpha=0.05\\) and \\(\\beta = 0.8\\) for probabilities, and let mean = 0 and sd = 1 (both \\(z\\) should be positive).\nSince \\(n\\) must be an integer, you will need to round up. Look up the built in functions for this.\nAnswer Enter the R code in the chunks below. If you choose SAS for this exercise, use the marked portion in the SAS homework template.\n1936 versus 2006 alpha = 0.05 beta = 0.8 m1=268.1 m2=384.4 s1=124.8 s2=168.3 z.half.alpha=abs(qnorm(alpha/2, 0, 1 )) z.half.alpha ## [1] 1.959964 z.beta=abs(qnorm(beta, 0, 1)) z.beta ## [1] 0.8416212 n \u0026lt;- 2* (((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * ((z.half.alpha + z.beta)^2) ```\n 1936 versus 1997 m1=268.1 m2=288.6 s1=124.8 s2=122.0 z.half.alpha=abs(qnorm(alpha/2, 0, 1 )) z.half.alpha ## [1] 1.959964 z.beta=abs(qnorm(beta, 0, 1)) z.beta ## [1] 0.8416212 n \u0026lt;- 2* (((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * ((z.half.alpha + z.beta)^2)  1997 versus 2006 m1=288.6 m2=384.4 s1=122.0 s2=168.3 z.half.alpha=abs(qnorm(alpha/2, 0, 1 )) z.half.alpha ## [1] 1.959964 z.beta=abs(qnorm(beta, 0, 1)) z.beta ## [1] 0.8416212 n \u0026lt;- 2* (((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * ((z.half.alpha + z.beta)^2) To check your work, use the rule of thumb suggested by van Belle (“Statistical Rules of Thumb”), where\n\\[ n= \\frac{16}{\\Delta^2} \\]\nwith \\(\\Delta = \\frac{\\mu_1 - \\mu_2}{\\sigma}\\). How does this compare with your results? Why does this rule of thumb work? How good is this rule of thumb?\n# rule of thumb sigma=1 # Here, use either s1 or s2 or pooled sd for the sake of estimating. Comment from the instructor: Can we agree that the formula given in to check your work will be an approximation, and not an exact answer? If so, then does it matter if the approximate answer is based on the larger of two sd, the smaller of two sd, or some pooled value? Delta=(m1 - m2)/s2 n= 16/Delta^2 n ## [1] 49.38069    Exercise 3 The probablity of an observation \\(x\\), when taken from a normal population with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is calculated by\n\\[ L (x ; \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}^{}} e^{- \\frac{(x - \\mu)^2}{2 \\sigma^2}} \\] For values of \\(x = \\{-0.1, 0.0, 0.1 \\}\\), write code to calculate \\(L (x ; \\mu = 0, \\sigma = 1)\\).\nAnswer Enter the R code in the chunks below. If you choose SAS for this exercise, use the marked portion in the SAS homework template.\n\\(x=-0.1\\) # to calcuate the values for l_1 from above equation, first we define the values for sigma, mu and x as below and the code for the formula is shown. sigma=1 mu=0 x=-0.1 l_1 \u0026lt;- 1/ (sigma*sqrt(2*pi))*exp(-(x-mu)^2/2*sigma^2) l_1 ## [1] 0.3969525  \\(x=0.0\\) # to calcuate the values for l_2 from above equation, first we define the values for sigma, mu and x as below and the code for the formula is shown. sigma=1 mu=0 x=0.0 l_2 \u0026lt;- 1/ (sigma*sqrt(2*pi))*exp(-(x-mu)^2/2*sigma^2) l_2 ## [1] 0.3989423  \\(x=0.1\\) # to calcuate the values for l_3 from above equation, first we define the values for sigma, mu and x as below and the code for the formula is shown. sigma=1 mu=0 x=0.1 l_3 \u0026lt;- 1/ (sigma*sqrt(2*pi))*exp(-(x-mu)^2/2*sigma^2) l_3 ## [1] 0.3969525 You can confirm your results using the built in normal distribution function. Look up dnorm in R help and use the same values for x, mean and sigma as above. You should get matching results to at least 12 decimal places.\nif(abs(l_1 -dnorm(-0.1,0, 1))\u0026lt;1e-12) { print(\u0026quot;likelihood for x = -0.1 correct\u0026quot;) }else{ print(\u0026quot;likelihood for x = -0.1 incorrect\u0026quot;) } ## [1] \u0026quot;likelihood for x = -0.1 correct\u0026quot; if(abs(l_2 -dnorm(0,0, 1))\u0026lt;1e-12) { print(\u0026quot;likelihood for x = 0.0 correct\u0026quot;) }else{ print(\u0026quot;likelihood for x = 0.0 incorrect\u0026quot;) } ## [1] \u0026quot;likelihood for x = 0.0 correct\u0026quot; if(abs(l_3 -dnorm(0.1,0, 1))\u0026lt;1e-12) { print(\u0026quot;likelihood for x = 0.1 correct\u0026quot;) } else { print(\u0026quot;likelihood for x = 0.1 incorrect\u0026quot;) } ## [1] \u0026quot;likelihood for x = 0.1 correct\u0026quot; Thus, this shows that the matching resuls are less than 12 decimal places.\n   Exercise 4 Part a Write code to compute\n\\[7 - 1 \\times 0 + 3 \\div 3\\]\nType this in verbatim, using only numbers, -,* and /, with no parenthesis. Do you agree with the result? Explain why, one or two sentences.\nAnswer a=7-1*0+3/3 a ## [1] 8 Yes I agree with the results because it follows BODMAS rule. The order of calculation should be brackets, Order, Division/Multiplication, Addition/Subtraction.\n  Part b According to “Why Did 74% of Facebook Users Get This Wrong?” (https://profpete.com/blog/2012/11/04/why-did-74-of-facebook-users-get-this-wrong/), most people would compute the result as 1. Use parenthesis ( ) to produce this result.\nAnswer #For this, we can get 1 if we do the substraction at first and then the multiplication later by addition and division. Therefore, the value would be wrong as it doesnot follow the rule of calculations (BODMAS). a=(7-1)*0+3/3 a ## [1] 1   Part c Several respondents to the survey cited in Part 2 gave the answer 6. Add one set of parenthesis to produce this result.\nAnswer a=7-1*(0+3/3) a ## [1] 6    Exercise 5. Part a Quoting from Wansink and Payne\n Because of changes in ingredients, the mean average calories in a recipe increased by 928.1 (from 2123.8 calories … to 3051.9 calories … ), representing a 43.7% increase.\n Show how 43.7% is calculated from 2123.8 and 3051.9, and confirm W\u0026amp;P result.\nAnswer #here we define the variable m1 and m2 as mean for total caloires for 1936 and 2006 respectively m1=2123.8 m2=3051.9 percentchange=(m2-m1)/m1*100 round(percentchange, 2) ## [1] 43.7 # 43.7  The resulting increase of 168.8 calories (from 268.1 calories … to 436.9 calories …) represents a 63.0% increase … in calories per serving.\n   Part b Repeat the calculations from above and confirm the reported 63.0% increase in calories per serving. Why is there such a difference between the change in calories per recipe and in calories per serving?\nAnswer #let m1 and m2 be the mean for calories per serving for 1936 and 2006 respectively. m1=268.1 m2=436.9 percentchange=abs(m1-m2)/m1*100 round(percentchange, 1) ## [1] 63 # 63.0 The value for the percentage change in average calories per recipe from 1936 to 2006 is 43.7% which is lower than the percent change in average calories per serving (63.0 %) because the percent change for the year 1936 to 2006 is higher for average calories per serving compared to average calories per recipe.\n  Part c Calculate an average calories per serving by dividing average calories per recipe by average servings per recipe, for years 1936 and 2006, then calculate a percent increase. Which of the two reported increases (a or b) are consistent with this result?\nAnswer #Here, we divide average calories per recipe by average servings per recipe to get m1 and m2 (average calories per serving for year 1936 and 2006, respectively). m1=2123.8/12.9 m2=3051.9/12.7 percentchange=abs(m1-m2)/m1*100 round(percentchange, 2) ## [1] 45.96 The reported percent change here (45.96%) is somewhat consistent with 43.7 %. So it is somewhat consistent with a.\nFinally, I choose to work on exercise 1, 3, 4 and 5 using R and also excercise 1 using SAS.\n   ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"828421506d95cae89ee4e3f3f77d42e3","permalink":"/achalneupane.github.io/post/calculations/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/calculations/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Calculations","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  There are six exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve five of the sixth problems, mixing the languages as you wish.\nWarning I will continue restricting the use of external libraries in R, particularly tidyverse libraries. You may choose to use ggplot2, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the midterm and final projects.\nExperimental Again, you will be allowed to provide one solution using Python. Elaborate on the similarities and differences between Ptyhon function definitions and R or IML or Macro language.\n Reuse For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here. I’m also including data vectors that can be used in some exercises.\nCaloriesPerServingMean \u0026lt;- c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4) CaloriesPerServingSD \u0026lt;- c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3)  Exercise 1 Write a general Cohen \\(d\\) function to be more generally useful, accepting a wider range of arguments. For convenience, name this general.d.\nThe new function should accept two parameters, m, s\nIn your function, check for these condititions:\n If m is of length 1 and s is length 1, then simply divide m/s - that is, proceed with the calculations as if m = %Diff and s = CV.\n If m is of length 2, then calculate the difference and proceed with the calculations. If m is of length greater than 2, find the difference between the min and max of m and proceed with the calculations.\n If s is of length greater than 1 calculate pooled sd as\n  \\[ s^2_{pooled} = \\sqrt{\\frac{\\sum_i^k s_i^2}{k}} \\]\n# Creating the general.d function with all possible combinations for m and s # conditions: general.d \u0026lt;- function(m, s){ if(length(m) == 1 \u0026amp;\u0026amp; length(s) == 1){ Diff = m CV = s print(\u0026quot;EffectSize calculated for length(m) == 1, and length(s) == 1\u0026quot;) }else if (length(m) == 2 \u0026amp;\u0026amp; length(s) \u0026gt; 1){ Diff = abs(m[1]-m[2]) CV = sqrt(sum((s^2))/length(s)) print(\u0026quot;EffectSize calculated for length(m) == 2, and length(s) \u0026gt; 1\u0026quot;) }else if (length(m) == 2 \u0026amp;\u0026amp; length(s) == 1){ Diff = abs(m[1]-m[2]) CV = s print(\u0026quot;EffectSize calculated for length(m) == 2, and length(s) == 1\u0026quot;) }else if (length(m) \u0026gt; 2 \u0026amp;\u0026amp; length(s) \u0026gt; 1){ Diff = max(m)-min(m) CV = sqrt(sum((s^2))/length(s)) print(\u0026quot;EffectSize calculated for length(m) \u0026gt; 2, and length(s) \u0026gt; 1\u0026quot;) }else if (length(m) \u0026gt; 2 \u0026amp;\u0026amp; length(s) == 1){ Diff = max(m)-min(m) CV = s print(\u0026quot;EffectSize calculated for length(m) \u0026gt; 2, and length(s) == 1\u0026quot;) } EffectSize \u0026lt;- Diff/CV return(EffectSize) } Test your function with the three cases:\n1936 Effect Size Use just the mean from 1936 and the associated standard deviation\ngeneral.d(268.1, 124.8) ## [1] \u0026quot;EffectSize calculated for length(m) == 1, and length(s) == 1\u0026quot; ## [1] 2.148237  1936 versus 2006 This should duplicate results from prior exercises.\ngeneral.d(c(268.1,384.4), c(124.8,168.3)) ## [1] \u0026quot;EffectSize calculated for length(m) == 2, and length(s) \u0026gt; 1\u0026quot; ## [1] 0.7849876 Yes, it matches the result from Homework 4\n All of Calories per Serving. Use the vectors from the Reuse chunk.\ngeneral.d(CaloriesPerServingMean,CaloriesPerServingSD) ## [1] \u0026quot;EffectSize calculated for length(m) \u0026gt; 2, and length(s) \u0026gt; 1\u0026quot; ## [1] 0.9051585 Are these three scenarios sufficient to test every path through general.d?\nYes, these three scenarious are sufficient to test for all conditions mentioned above.\n  Exercise 2 Previously, we’ve calculated required replicates based on the \\(z\\) distribution. In this exercise, you will calculate required replicates based on the \\(t\\) disttibution. You must implement one of two algorithms given below. For both algorithmss, alculate degrees of freedom as \\(\\nu = n*k-k\\) where \\(n\\) is the current estimate for required replicates and let \\(k=2\\)\nAlgorithm 1 (from Cochran and Cox, ) Use the formula:\n\\[ n \\ge 2\\times \\left( \\frac{CV}{\\%Diff} \\right)^2 \\times \\left(t_{\\alpha/2,\\nu}+ t_{\\beta,\\nu} \\right)^2 \\]\nStart with a small \\(n\\), say, 2. Calculate critical \\(t_\\alpha/2\\) and \\(t_\\beta\\) quantiles with \\(\\nu\\) d.f, then calculate required replicates. Label this \\(n_{current}\\). Update \\(\\nu\\) using \\(n_{current}\\), then recalculate critical values and required replicates. Label this \\(n_{next}\\). If \\(n_{current}=n_{next}\\) then the algorithm has converged. Otherwise, set \\(n_{current}\\) to \\(n_{next}\\), and repeat 2-3. If after some sufficiently large number (say, 20), the algorithm hasn’t converged, print a message and return the largest of \\(n_{current}\\) and \\(n_{next}\\)  required.replicates.t \u0026lt;- function (m1, s1, m2, s2,n = 2, k = 2, alpha = 0.05, beta = 0.20){ temp.required.replicates.t \u0026lt;- function(...){ sd.pooled \u0026lt;- function(...){ sqrt((s1^2 + s2^2)/2) } d \u0026lt;- (sd.pooled(s1,s2))/abs(m1-m2) ceiling(2*d^2*(qt(1-alpha/2, n*k-k) + qt(1-beta, n*k-k))^2) } # n \u0026lt;- 2 i \u0026lt;- 1 ncurrent.indexed \u0026lt;- { } nnext.indexed \u0026lt;- { } success \u0026lt;- FALSE # while (!success) { while (i \u0026lt; 20 \u0026amp;\u0026amp; !success) { ncurrent \u0026lt;- n ncurrent.indexed[i] \u0026lt;- ncurrent # so we can keep track of ncurrent nnext \u0026lt;- temp.required.replicates.t(m1, s1, m2, s2, n = ncurrent) nnext.indexed[i] \u0026lt;- nnext # so we can keep track of nnext success \u0026lt;- nnext == ncurrent if (!success \u0026amp;\u0026amp; i == 20) { # if (success ){ print(\u0026quot;The algorithm DO NOT converge!!!!!\u0026quot;) print(paste0(\u0026quot;The largest of ncurrent is : \u0026quot;, max(ncurrent.indexed))) print(paste0(\u0026quot;The nnext is: \u0026quot;, nnext)) } else if (success) { print(paste0(\u0026quot;The algorithm DO converge at iteration \u0026quot; , i , \u0026quot; !!!!!\u0026quot;)) break } n \u0026lt;- nnext i \u0026lt;- i + 1 } values \u0026lt;- list(n.current = ncurrent.indexed, n.next = nnext.indexed) return(values) } # Copying required.replicates function from last homework required.replicates \u0026lt;- function (m1,m2, s1,s2, alpha=0.05, beta=0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) return(round(n,0)) }  Algorithm 2 Start with a small \\(n\\), say, 2. Calculate critical \\(t_\\alpha\\) quantile using the central \\(t\\) distribution with \\(\\nu\\) d.f. Estimate Type IIShow in New WindowClear OutputExpand/Collapse Output error (p-value) under the alternate hypothesis using the non-central \\(t\\) distribution with \\(\\nu\\) d.f, at the critical \\(t\\) from 2. Calculate non-centrality parameter as \\[ NCP = \\frac{\\%Diff}{CV} \\sqrt{\\frac{n}{2}} \\] If the resulting error is less than \\(1-\\beta\\), accept the current value of \\(n\\). Otherwise increment \\(n\\) and repeate 2-3. If desired power is not achieved after a large number of iterations (say, 1000), terminate the calculations and return NA.  Implement the algorithm as a function or macro named required.replicates.t, with parameters mu, sigma and an optional parameter k. Test your function by comoparing with required replicates from prior exercises for calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006.\nFor either algoorithm, you might consider starting with an initial value of \\(n\\) calculated using the \\(z\\) critical values as before. Can you be certain that the \\(z\\) formula will not estimate more required replicates than the \\(t\\) algorithm?\n1936 versus 2006 required.replicates(268.1, 124.8, 384.4, 168.3) ## [1] 67 required.replicates.t(268.1, 124.8, 384.4, 168.3) ## [1] \u0026quot;The algorithm DO converge at iteration 4 !!!!!\u0026quot; ## $n.current ## [1] 2 94 26 27 ## ## $n.next ## [1] 94 26 27 27  1936 versus 1997 required.replicates(268.1, 124.8, 288.6, 122.0) ## [1] 38 required.replicates.t(268.1, 124.8, 288.6, 122.0) ## [1] \u0026quot;The algorithm DO converge at iteration 3 !!!!!\u0026quot; ## $n.current ## [1] 2 2085 570 ## ## $n.next ## [1] 2085 570 570  1997 versus 2006 required.replicates(384.4,168.3,288.6,122.0) ## [1] 17 required.replicates.t(384.4,168.3,288.6,122.0) ## [1] \u0026quot;The algorithm DO converge at iteration 3 !!!!!\u0026quot; ## $n.current ## [1] 2 136 38 ## ## $n.next ## [1] 136 38 38 You might find it useful to reproduce the plots from Homework 4, Ex. 3. Plot the central and non-central \\(t\\) distributions over the range \\(-3,4\\), and produce plots for seledted \\(n\\).\n   Exercise 3 Calculate a cumulative probability value from the normal pdf, using the Newton-Cotes formula\n\\[ \\int _{x_0} ^{x_n} f(x) dx \\approx \\sum _{i=0} ^n h f(x_i) \\]\nwhere \\(x_1, ..., x_n\\) are a sequence of evenly spaced numbers from \\(-2 \\dots 2\\), with \\(x_i = x_0 + h i\\), \\(n\\) is the number of \\(x_i\\) in the sequence and step size \\(h = (x_n -x_0)/n\\).\nWe will calculate this integral by calculating successive approximations of \\(f = L(x;0,1)\\) = norm.pdf over series of \\(x\\) with increasingly smaller step sizes.\nPart a. Calculate \\(L_0\\) by summing over \\(L(\\bf{X_0})\\), where \\(X_0\\) is a series from \\(x_0=-2, \\dots, x_n=2\\) incremented by \\(h_0=0.1\\). Multiply this sum by \\(h_0\\) for an approximate \\(\\int _{x_0} ^{x_n} L(x) dx\\)\nh0_val \u0026lt;- 0.1 x0_val\u0026lt;-{} x0_val \u0026lt;- seq(-2,2,h0_val) result0 \u0026lt;- sum(x0_val) result0  ## [1] 4.662937e-15 normal.pdf.0 \u0026lt;- result0 *h0_val normal.pdf.0 ## [1] 4.662937e-16 Think of this as the sum of a series of rectangles, each \\(h\\) wide and a height given by the normal pdf.\n Part b. Create a second series \\(X_1\\) by setting \\(h_1 = h_0/2\\). Compute \\(L_1\\) from this series as in part a. Let \\(i=1\\) You now have the are of twice as many rectangles as part a, but each is half as wide.\n# Now, Create a second series X_1 h1_val \u0026lt;- h0_val/2 x1_val\u0026lt;-{} x1_val \u0026lt;- seq(-2,2,h1_val) result1 \u0026lt;- sum(x1_val) result1  ## [1] 8.881784e-15 normal.pdf.1 \u0026lt;- result1 *h1_val normal.pdf.1 ## [1] 4.440892e-16  Part c. Compute \\(\\delta=|L_i - L_{i-1}|\\). If \\(\\delta \u0026lt; 0.0001\\), your sequence of iterations has converged on a solution for \\(L\\). Finish with Part d. Otherwise, increment \\(i\\), let \\(h_i = h_{i-1}/2\\). Create the next series \\(X_i\\) and compute the next \\(L_i\\).\nHint: code this first as a for loop of a small number of \\(i\\) until you know your code will converge toward a solution.\nh_val\u0026lt;- 0 seq1\u0026lt;-{} seq2\u0026lt;-{} for (x in 1:10){ if (x == 1 ){ hx \u0026lt;- 0.1 h_val[x] \u0026lt;- hx } else { hx \u0026lt;- h_val[x] } y \u0026lt;- x +1 hy \u0026lt;- hx/2 h_val[y] \u0026lt;- hy seq1 \u0026lt;- seq(-2,2,hx) sumSeq1 \u0026lt;- sum(seq1) pdf1 \u0026lt;- sumSeq1*hx seq2 \u0026lt;- seq(-2,2,hy) sumSeq2 \u0026lt;- sum(seq2) pdf2 \u0026lt;- sumSeq2*hy del \u0026lt;- abs(sumSeq2 - sumSeq1) if (del \u0026lt; 0.0001){ print(\u0026quot;The sequence has been converged\u0026quot;) break } } ## [1] \u0026quot;The sequence has been converged\u0026quot;   Part d Report \\(i\\), \\(n\\) and \\(h\\).\nx ## [1] 1 h_val[y] ## [1] 0.05 length(seq2)-length(seq1) ## [1] 40 To check your results, compare your final \\(L_i\\) to\npnorm(-2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE) ## [1] 0 Is your estimate within \\(0.0001\\) of this value?\nYou might find it useful to produce staircase plots for the first 2-4 iterations (plot \\(L_i\\) vs \\(X_i\\) on one graph). You might also find it interesting to plot \\(\\delta\\) or \\(L\\) versus \\(i\\) or \\(h\\). You can create vectors to hold the intermediate steps - 10 iterations should be enough. How many iterations might it take to get within 0.000001 of the expected value from R?\n Exercise 4 Part a. Write a function to compute mean, standard deviation, skewness and kurtosis from a single vector of numeric values. You can use the built-in mean function, but must use one (and only one) for loop to compute the rest. Be sure to include a check for missing values. Note that computationally efficient implementations of moments calculations take advantage of \\((Y_i-\\bar{Y})^4 = (Y_i-\\bar{Y}) \\times (Y_i-\\bar{Y})^3\\), etc.\nSee https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm for formula for skewness and kurtosis. This reference gives several definitions for both skewness and kurtosis, you only need to implement one formula for each. Note that for computing skewness and kurtosis, standard deviation is computed using \\(N\\) as a divisor, not \\(N-1\\).\nYour function should return a list with Mean, SD, Skewness and Kurtosis. If you use IML, you will need to implement this as a subroutie and use call by reference; include these variables in parameter list.\n ss_x=0 temp.kurtosis=0 temp.skewness=0 i=1 k=0 # length.x=length(x) # creating a function for calculating mean, sd, skewness and kurtosis kurtskw=function(x){ # while (i \u0026lt;= length.x) { for (i in 1:length(x)){ if(!is.na(x[i])){ meanX=mean(x,na.rm = TRUE) ss_x=ss_x + ((x[i]- meanX)^2) temp.kurtosis= temp.kurtosis + ((x[i] - meanX)^4) temp.skewness=temp.skewness + ((x[i] - meanX)^3) k=k+1 } # i=i+1 } meanX=mean(x,na.rm = TRUE) Sd=sqrt(ss_x/k) kurtosis = temp.kurtosis/(k*(Sd^4)) skewness = temp.skewness/((Sd^3)*k) return(list(meanX=meanX,Sd=Sd,kurtosis=kurtosis,skewness=skewness)) }  Part b. Test your function by computing moments for Price from pumpkins.csv, for ELO from elo.csv or the combine observations from SiRstvt. If find that ELO shows both skewness and kurtosis, Price is kurtotic but not skewed, while SiRstvt are approximately normal.\npumpkins.dat =read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot;) ELO.dat =read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/elo.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot;) SiRstvt.dat \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master//SiRstvt.dat\u0026quot;, header = FALSE, skip = 59) x = pumpkins.dat$Price y = ELO.dat$ELO # z = vector(SiRstvt.dat) library(reshape2) SiRstvt.dat \u0026lt;- suppressWarnings(melt(SiRstvt.dat)) ## No id variables; using all as measure variables z = SiRstvt.dat$value print(kurtskw(x)) ## $meanX ## [1] 178.0667 ## ## $Sd ## [1] 40.33438 ## ## $kurtosis ## [1] 1.670802 ## ## $skewness ## [1] -0.04981076 print(kurtskw(y)) ## $meanX ## [1] 1378.776 ## ## $Sd ## [1] 56.42003 ## ## $kurtosis ## [1] 3.684744 ## ## $skewness ## [1] 0.6822352 print(kurtskw(z)) ## $meanX ## [1] 196.1892 ## ## $Sd ## [1] 0.1034955 ## ## $kurtosis ## [1] 2.337141 ## ## $skewness ## [1] -0.1456679 I also found that ELO shows both skewness and kurtosis, Price is kurtotic but not skewed, while SiRstvt are approximately normal.\nNote: https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm A fundamental task in many statistical analyses is to characterize the location and variability of a data set. A further characterization of the data includes skewness and kurtosis. Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case. The values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George \u0026amp; Mallery, 2010). George, D., \u0026amp; Mallery, M. (2010). SPSS for Windows Step by Step: A Simple Guide and Reference, 17.0 update (10a ed.) Boston: Pearson.\nAlso, If skewness is not close to zero, then your data set is not normally distributed. If skewness is less than -1 or greater than 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\nlibrary(moments) kurtosis(x, na.rm = TRUE) ## [1] 1.670802 skewness(x, na.rm = TRUE) ## [1] -0.04981076 kurtosis(y, na.rm = TRUE) ## [1] 3.684744 skewness(y, na.rm = TRUE) ## [1] 0.6822352 kurtosis(z, na.rm = TRUE) ## [1] 2.337141 skewness(z, na.rm = TRUE) ## [1] -0.1456679   Exercise 5 In this exercise, we will use run-time profiling and timing to compare the speed of execution for different functons or calculations. In the general, the algorithm will be\nWrite a loop to execute a large number of iterations. I find \\(10^6\\) to be useful; you might start with a smaller number as you develop your code. In this loop, call a function or perform a calculation. You don’t need to use or print the results, just assign the result to a local variable. Repeat 1 and 2, but with a different function or formula. Repeat steps 1-3 10 times, saving the time of execution for each pair of the 10 tests. Calculate mean, standard deviation and effect size for the two methods tested.  If you choose R, I’ve included framework code using Rprof; I’ve included framework code for IML in the SAS template.\nTest options - In homework 3, you were given two formula for the Poisson pmf,\n\\[ \\begin{aligned} f(x;\\lambda) \u0026amp; = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\\\ \u0026amp; = exp(-\\lambda)(\\frac{1}{x!}) exp[x\\times log(\\lambda)] \\\\ \\end{aligned} \\] Compare the computationally efficiency of these two formula.\n Create a sequence x of numbers -3 to 3 of length \\(10^6\\) or so. In the first test, detetermine the amoung of time it takes to compute \\(10^5\\) estimates of norm.pdf by visiting each element of x in a loop. In the second test, simply pass x as an argument to norm.pdf. Does R or IML optimize vector operations?  # A x0 = -2 xn = 2 h0 \u0026lt;- 0.1 # spaced by 0.1 # Xi = x_0+h X0 \u0026lt;- seq(x0,xn,h0) X0 ## [1] -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 ## [15] -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 ## [29] 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 L0 \u0026lt;- sum(X0) L0 ## [1] 4.662937e-15 normal_pdf0 \u0026lt;- L0*h0 normal_pdf0 ## [1] 4.662937e-16 # B h1 \u0026lt;- h0/2 X1 \u0026lt;- seq(-2,2,h1) L1 \u0026lt;- sum(X1) L1 ## [1] 8.881784e-15 normal_pdf1 \u0026lt;- L1*h1 normal_pdf1 ## [1] 4.440892e-16 # C # Computing delta to meet the condition H \u0026lt;- 0 for (i in 1:1){ if (i == 1 ){ hi \u0026lt;- 0.1 H[i] \u0026lt;- hi } else { hi \u0026lt;- H[i] } # Set j as i+1 j \u0026lt;- i +1 hj \u0026lt;- hi/2 H[j] \u0026lt;- hj Xi \u0026lt;- seq(-2,2,hi) Li \u0026lt;- sum(Xi) normal_pdf.i \u0026lt;- Li*hi xj \u0026lt;- seq(-2,2,hj) Lj \u0026lt;- sum(xj) normal_pdf.j \u0026lt;- Lj*hj delta \u0026lt;- abs(Lj - Li) # If converge print if (delta \u0026lt; 0.0001){ print(paste0(\u0026quot;The sequence has converged at iteration i: \u0026quot;, i, \u0026quot; and iteration j: \u0026quot;, j)) break } } ## [1] \u0026quot;The sequence has converged at iteration i: 1 and iteration j: 2\u0026quot;  The mathematical statement \\(\\sqrt{x}\\) can be coded as either sqrt(x) or x^(1/2). Similarly, \\(e^x\\) can be written as \\(exp(1)^x\\) or \\(exp(x)\\). These pairs are mathematically equivalent, but are they computationally equivalent. Write two test loops to compare formula with either \\(\\sqrt{x}\\) or \\(e^x\\) of some form (the normal pdf, perhaps).  I find \\(10^5 - 10^7\\) give useful results, and that effect size increases with the number iterations; there is some overhead in the loop itself that contributes relatively less with increasing interations.\nRprof(\u0026quot;test1\u0026quot;) ## iteration code 1 Rprof(NULL) Rprof(\u0026quot;test2\u0026quot;) ## iteration code 2 Rprof(NULL) #execution time for test 1 summaryRprof(\u0026quot;test1\u0026quot;)$sampling.time #execution time for test 2 summaryRprof(\u0026quot;test2\u0026quot;)$sampling.time #functions called in test 1 summaryRprof(\u0026quot;test1\u0026quot;)$by.self #functions called in test 2 summaryRprof(\u0026quot;test2\u0026quot;)$by.self  Exercise 6 Write an improved Poisson pmf function, call this function smart.pois, using the same parameters x and lamba as before, but check \\(x\\) for the following conditions. 1. If \\(x\\) is negative, return a missing value (NA, .). 2. If \\(x\\) is non-integer, truncate \\(x\\) then proceed. 3. If \\(x\\) is too large for the factorial function, return the smallest possible numeric value for your machine. What \\(x\\) is too large? You could test the return value of factorial against Inf.\nYou can reuse previously tested code writing this function as a wrapper for a previously written pois.pmf and call that function only after testing the for specified conditions.\nTest this function by repeating the plots from Homework 4, Ex 4. How is the function different than dpois?\nWarning You may not be able to call this new function exactly as in the last exercise (Hint - what are the rules for conditions in if statements?). Instead, you might need to create a matrix or data table and use apply functions, or write a loop for visit each element in a vector of x.\noptions(warn=-1) smart.pois \u0026lt;- function(x, lambda){ if(x\u0026lt;0){ poisson.d \u0026lt;- NA # poisson.d \u0026lt;- \u0026quot;NA check\u0026quot; }else if(class(x) != \u0026quot;integer\u0026quot; \u0026amp; suppressWarnings(factorial(x)) != Inf){ x \u0026lt;- trunc(x) poisson.d \u0026lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda))) # poisson.d \u0026lt;- \u0026quot;integer.check\u0026quot; }else if(suppressWarnings(factorial(x)) == Inf){ smallest.value \u0026lt;- .Machine poisson.d \u0026lt;- smallest.value$double.xmin # poisson.d \u0026lt;- \u0026quot;factorial.check\u0026quot; } return(poisson.d) } x_a \u0026lt;- seq(-5, 5, 0.1) smart.poisson.probability.x_a \u0026lt;- {} lambda \u0026lt;- 1 for(i in 1:length(x_a)){ out \u0026lt;- smart.pois(x_a[i], lambda) smart.poisson.probability.x_a \u0026lt;- c(smart.poisson.probability.x_a, out) } smart.poisson.probability.x_a ## [1] NA NA NA NA NA ## [6] NA NA NA NA NA ## [11] NA NA NA NA NA ## [16] NA NA NA NA NA ## [21] NA NA NA NA NA ## [26] NA NA NA NA NA ## [31] NA NA NA NA NA ## [36] NA NA NA NA NA ## [41] NA NA NA NA NA ## [46] NA NA NA NA NA ## [51] 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441 ## [56] 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441 ## [61] 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441 ## [66] 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441 ## [71] 0.183939721 0.183939721 0.183939721 0.183939721 0.183939721 ## [76] 0.183939721 0.183939721 0.183939721 0.183939721 0.183939721 ## [81] 0.061313240 0.061313240 0.061313240 0.061313240 0.061313240 ## [86] 0.061313240 0.061313240 0.061313240 0.061313240 0.061313240 ## [91] 0.015328310 0.015328310 0.015328310 0.015328310 0.015328310 ## [96] 0.015328310 0.015328310 0.015328310 0.015328310 0.015328310 ## [101] 0.003065662 norm.pdf \u0026lt;- function(x,mu=0,sigma=1){ l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } normal.liklihood.x_a \u0026lt;- norm.pdf(x_a) plot(x_a,normal.liklihood.x_a,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_a,smart.poisson.probability.x_a,col=\u0026quot;red\u0026quot;) # using base functions for norm.pdf and poisson probability for x_a base.normal.liklihood.x_a \u0026lt;- dnorm(x_a,0, 1) base.poiss.x_a \u0026lt;- dpois(x=x_a, lambda = lambda) plot(x_a,base.normal.liklihood.x_a,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_a,base.poiss.x_a,col=\u0026quot;red\u0026quot;) # Now # Using mean and standard deviation for servings per recipe from 1936: mu = 12.9; sigma = 13.3 # Now we find the upper and lower bounds as: x.lower \u0026lt;- floor(mu-5*sigma) x.upper \u0026lt;- ceiling(mu+5*sigma) # Now taking the length of x_a as a reference, we create the equeally spaced # sequence from lower to upper bound as followed: spacer \u0026lt;- (x.upper - x.lower)/(length(x_a) - 1) x_b \u0026lt;- seq(x.lower, x.upper, spacer) # To show both x_a and x_b \u0026#39;s lenghts are equal: length(x_a) ## [1] 101 length(x_b) ## [1] 101 norm.pdf \u0026lt;- function(x, mu = 12.9, sigma = 13.3){ l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } normal.liklihood.x_b \u0026lt;- norm.pdf(x_b) # Calculate poisson probability for x_b using smart.pois #using sigma = 13.3 to compare the difference with the first plot smart.poisson.probability.x_b \u0026lt;- {} lambda \u0026lt;- 12 for(i in 1:length(x_a)){ out \u0026lt;- smart.pois(x_a[i], lambda) smart.poisson.probability.x_b \u0026lt;- c(smart.poisson.probability.x_b, out) } # # smart.pois output for x_b # smart.poisson.probability.x_b # Repeating the plot from homework 4 Ex 4 plot(x_b,normal.liklihood.x_b,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_b,smart.poisson.probability.x_b,col=\u0026quot;red\u0026quot;) # Using base functions for x_b base.normal.liklihood.x_b \u0026lt;- dnorm(x_b,0, 1) base.poiss.x_b \u0026lt;- dpois(x=x_b, lambda = 12) plot(x_b,base.normal.liklihood.x_b,type=\u0026quot;l\u0026quot;,col=\u0026quot;black\u0026quot;) lines(x_b,base.poiss.x_b,col=\u0026quot;red\u0026quot;) # FInally, we can also check how smart.pois and dpois functions behave for the following numerical values # Testing three conditions with smart.pois smart.pois(-1, 12) ## [1] NA smart.pois(2, 12)  ## [1] 0.0004423833 smart.pois(2.233, 12) ## [1] 0.0004423833 smart.pois(300, 12) ## [1] 2.225074e-308 #comparing same parameters with dpois dpois(-1, 12) ## [1] 0 dpois(2, 12) ## [1] 0.0004423833 dpois(2.233, 12) ## [1] 0 dpois(300, 12) ## [1] 1.140347e-296 Smart pois is different from dpois as smart.pois gives NAs for negative values and dpois gives, zero. In other word, we can say that dpois cannot handle non-integer, but smart.pois can.Also, for non-integer, dpois gives zero. Also, I couldn’t find any similar pattern in the output of smart.poisson.probability.x_a and base.poiss.x_a\n ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"e7aace7313adbf583b2f6b2c4586a13d","permalink":"/achalneupane.github.io/post/control_structures/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/control_structures/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Control Structures","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  There are six exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve five of the sixth problems, mixing the languages as you wish.\nIf you choose SAS for an exercise, you may use IML, DATA operations or PROC SQL at your discretion.\nWarning I will continue restricting the use of external libraries in R, particularly tidyverse libraries. You may choose to use ggplot2, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the midterm and final projects.\nReuse For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here.\n Exercise 1. Part a. Repeat the table from Homework 5, Exercise 2. The table will contain 30 rows, each corresponding to a unique combination of CV from \\(8, 12, ..., 28\\) and Diff from \\(5,10, ... , 25\\). However, for this exercise you only need to calculate one column for required replicates (\\(\\alpha=0.05\\) and \\(\\beta=0.2\\))\nDefine the table in the space below. Do not print this table.\nnewdata.dat \u0026lt;- data.frame(CV = rep(seq(8,28,4), each = 5), Diff = rep(seq(5,25,5),6)) newdata.dat ## CV Diff ## 1 8 5 ## 2 8 10 ## 3 8 15 ## 4 8 20 ## 5 8 25 ## 6 12 5 ## 7 12 10 ## 8 12 15 ## 9 12 20 ## 10 12 25 ## 11 16 5 ## 12 16 10 ## 13 16 15 ## 14 16 20 ## 15 16 25 ## 16 20 5 ## 17 20 10 ## 18 20 15 ## 19 20 20 ## 20 20 25 ## 21 24 5 ## 22 24 10 ## 23 24 15 ## 24 24 20 ## 25 24 25 ## 26 28 5 ## 27 28 10 ## 28 28 15 ## 29 28 20 ## 30 28 25 combined \u0026lt;- function (cv, percent.diff, alpha = 0.05, beta = 0.2){ cv \u0026lt;- cv percent.diff \u0026lt;- percent.diff n \u0026lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) n \u0026lt;- round(n,0) value \u0026lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0)) return(value) } value \u0026lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff) # Adding RequiredReplicates column newdata.dat$RequiredReplicates \u0026lt;- value$RequiredReplicates # Print the table # newdata.dat  Part b. Create two subset tables, one that contains the combinations of CV and Diff that require the five largest number of replicates and one the contains the combinations of CV and Diff the five smallest number of replicates. You can determine the subset by ranking or sorting by required replicates. You can add a rank column to your table if you wish. Call one table LargestFive and one table SmallestFive.\n# Sort the table by decreasing Replicate values ordered.new.dat \u0026lt;- newdata.dat[order(newdata.dat$RequiredReplicates,decreasing = TRUE),] # Creating the required subset tables LargestFive \u0026lt;- head(ordered.new.dat, 5) # Now for smallest SmallestFive \u0026lt;- tail(ordered.new.dat, 5)  Part c. Print LargestFive sorted by required replicates in descending order, and print SmallestFive in ascending order.\n# adding rank for LargestFive table. Largest value gets rank 1 LargestFive$Rank \u0026lt;- rank(order(LargestFive$RequiredReplicates, decreasing = TRUE)) # printing larget five in descending order, Largest value gets the first rank LargestFive ## CV Diff RequiredReplicates Rank ## 26 28 5 492 1 ## 21 24 5 362 2 ## 16 20 5 251 3 ## 11 16 5 161 4 ## 27 28 10 123 5 # adding rank for SmallestFive table. Smallest value gets rank 1 SmallestFive \u0026lt;- SmallestFive[order(SmallestFive$RequiredReplicates, decreasing = FALSE),] SmallestFive$Rank \u0026lt;- rank((SmallestFive$RequiredReplicates),ties.method=\u0026quot;min\u0026quot;) # printing smallest five in ascending order, smallest value gets the first rank SmallestFive ## CV Diff RequiredReplicates Rank ## 5 8 25 2 1 ## 4 8 20 3 2 ## 3 8 15 4 3 ## 10 12 25 4 3 ## 15 16 25 6 5   Exercise 2 Part a Go to http://www.itl.nist.gov/div898/strd/anova/SiRstv.html and use the data listed under Data File in Table Format (https://www.itl.nist.gov/div898/strd/anova/SiRstvt.dat). You may reuse the file from Homework 6. Load the data into a table below.\nSiRstvt.dat \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/SiRstvt.dat\u0026quot;, header = FALSE, skip = 59) SiRstvt.dat ## V1 V2 V3 V4 V5 ## 1 196.3052 196.3042 196.1303 196.2795 196.2119 ## 2 196.1240 196.3825 196.2005 196.1748 196.1051 ## 3 196.1890 196.1669 196.2889 196.1494 196.1850 ## 4 196.2569 196.3257 196.0343 196.1485 196.0052 ## 5 196.3403 196.0422 196.1811 195.9885 196.2090  Part b Reshape or transpose this table from the wide format to the long format. Make sure the resulting table has two columns - Resistance and Instrument.\nlong.SiRstvt.dat \u0026lt;- with(reshape(SiRstvt.dat, idvar = \u0026quot;id\u0026quot;, varying = list(1:5), v.names = \u0026quot;Resistance\u0026quot;, timevar = \u0026quot;Instrument\u0026quot;, direction = \u0026quot;long\u0026quot;), data.frame(Instrument, Resistance)) # the desired table in two column format: long.SiRstvt.dat ## Instrument Resistance ## 1 1 196.3052 ## 2 1 196.1240 ## 3 1 196.1890 ## 4 1 196.2569 ## 5 1 196.3403 ## 6 2 196.3042 ## 7 2 196.3825 ## 8 2 196.1669 ## 9 2 196.3257 ## 10 2 196.0422 ## 11 3 196.1303 ## 12 3 196.2005 ## 13 3 196.2889 ## 14 3 196.0343 ## 15 3 196.1811 ## 16 4 196.2795 ## 17 4 196.1748 ## 18 4 196.1494 ## 19 4 196.1485 ## 20 4 195.9885 ## 21 5 196.2119 ## 22 5 196.1051 ## 23 5 196.1850 ## 24 5 196.0052 ## 25 5 196.2090  Part c To confirm that the table was reshaped correctly, use aggregate or tapply to calculate mean Resistance grouped by Instrument from the long table, and use apply or colMeans to calculate column means from the wide table. Print and compare the results.\n# Resistance mean grouped by Instrument from long format tapply(long.SiRstvt.dat$Resistance, long.SiRstvt.dat$Instrument, mean) ## 1 2 3 4 5 ## 196.2431 196.2443 196.1670 196.1481 196.1432 # aggregate(long.SiRstvt.dat$Resistance, by = list(long.SiRstvt.dat$Instrument), mean) # Resistance mean from wide format colMeans(SiRstvt.dat) ## V1 V2 V3 V4 V5 ## 196.2431 196.2443 196.1670 196.1481 196.1432 Note that the reshaped table should be equivalent to the file linked under ‘Data File in Two-Column Format’.\nYes, the reshaped table is equivalent to the linked data file in Two-column format\n  Exercise 3 Create an ordered treatment pairs table from the pumpkin.csv. In the submitted work print the table only once at the end of the exercise.\nPart a. Read the pumpkin data and compute mean \\(m_i\\), standard deviation \\(s_i\\) and count \\(n_i\\) for each level \\(i\\) of Class.\npumpkins.dat =read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot;) # pumpkin.table \u0026lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), # FUN = function(x) c(Mean = mean(x), SD = sd(x), Count.n = length(x))) pumpkin.mean \u0026lt;- setNames(aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = mean), c(\u0026quot;Class\u0026quot;, \u0026quot;Mean\u0026quot;)) pumpkin.sd \u0026lt;- setNames(aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = sd), c(\u0026quot;Class\u0026quot;, \u0026quot;SD\u0026quot;)) pumpkin.count \u0026lt;- setNames(aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = length), c(\u0026quot;Class\u0026quot;, \u0026quot;Counts\u0026quot;)) merged.class \u0026lt;- Reduce(function(x, y) merge(x, y, all=TRUE), list(pumpkin.mean, pumpkin.sd, pumpkin.count)) # This table includes mean, sd and counts for all Class merged.class ## Class Mean SD Counts ## 1 Blue 175.0000 0.000000 6 ## 2 Cinderella 218.4286 17.924445 7 ## 3 Howden 127.9000 3.695342 10 ## 4 Pie 212.0000 18.565200 7  Part b Create a table over all possible pairs \\(i,j\\) of \\(k\\) Classes from these data. Let one table column be \\(i\\) and another column be \\(j\\). Let \\(i = 1, 2, \\dots (k-1)\\) and \\(j = i+1, i+2, \\dots k\\). There will be \\((k \\times (k-1))/2\\) rows in this table. I usually create an empty table, then fill the table using a pair of nested loops, the outer loop over \\(i\\) and the inner loop over \\(j\\). Use a counter variable to keep track of the current row and increment the counter in each step of the inner loop.\n# There are 4 classes, 4 choose 2: class.level \u0026lt;- levels(pumpkins.dat$Class) comb.matrix \u0026lt;- as.data.frame(t(combn(4,2))) # This is how the combination matrix would look like comb.matrix ## V1 V2 ## 1 1 2 ## 2 1 3 ## 3 1 4 ## 4 2 3 ## 5 2 4 ## 6 3 4 # Creating an empty matrix so we could fill in with the required values collect \u0026lt;- data.frame(col1 = numeric(6), col2 = numeric(6)) for (i in 1:length(comb.matrix$V1)){ collect$col1[i] \u0026lt;- class.level[comb.matrix$V1[i]] collect$col2[i] \u0026lt;- class.level[comb.matrix$V2[i]] } # The possible pairs table collect ## col1 col2 ## 1 Blue Cinderella ## 2 Blue Howden ## 3 Blue Pie ## 4 Cinderella Howden ## 5 Cinderella Pie ## 6 Howden Pie # Now we merge the possible pairs (collect) table with the merged.class table with mean, # sd and counts values we created above collect \u0026lt;- merge(collect, merged.class, by.x = \u0026quot;col1\u0026quot;, by.y = \u0026quot;Class\u0026quot; ) collect \u0026lt;- merge(collect, merged.class, by.x = \u0026quot;col2\u0026quot;, by.y = \u0026quot;Class\u0026quot; ) collect ## col2 col1 Mean.x SD.x Counts.x Mean.y SD.y ## 1 Cinderella Blue 175.0000 0.000000 6 218.4286 17.924445 ## 2 Howden Blue 175.0000 0.000000 6 127.9000 3.695342 ## 3 Howden Cinderella 218.4286 17.924445 7 127.9000 3.695342 ## 4 Pie Blue 175.0000 0.000000 6 212.0000 18.565200 ## 5 Pie Cinderella 218.4286 17.924445 7 212.0000 18.565200 ## 6 Pie Howden 127.9000 3.695342 10 212.0000 18.565200 ## Counts.y ## 1 7 ## 2 10 ## 3 10 ## 4 7 ## 5 7 ## 6 7  Part c. Calculate Cohen’s \\(d\\) for each Class pair in this table. Use a pooled standard deviation given by\n\\[ s_{pooled} = \\sqrt{\\frac{\\sum_i (n_i-1)s_i^2}{N-k}} \\]\nYou may add Class means to the table if you wish. Sort the table by \\(d\\) in descending order and print the table.\ncohen.d \u0026lt;- function(m1,s1,m2,s2){ cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) return(cohens_d) } for(i in 1:nrow(collect)) { collect$cohens_d[i] \u0026lt;- cohen.d( m1 = collect$Mean.x[i], s1 = collect$SD.x[i], m2 = collect$Mean.y[i], s2 = collect$SD.y[i] ) } collect \u0026lt;- collect[order(collect$cohens_d, decreasing = TRUE),] # This is the final table we need collect ## col2 col1 Mean.x SD.x Counts.x Mean.y SD.y ## 2 Howden Blue 175.0000 0.000000 6 127.9000 3.695342 ## 3 Howden Cinderella 218.4286 17.924445 7 127.9000 3.695342 ## 6 Pie Howden 127.9000 3.695342 10 212.0000 18.565200 ## 1 Cinderella Blue 175.0000 0.000000 6 218.4286 17.924445 ## 4 Pie Blue 175.0000 0.000000 6 212.0000 18.565200 ## 5 Pie Cinderella 218.4286 17.924445 7 212.0000 18.565200 ## Counts.y cohens_d ## 2 10 18.0252467 ## 3 10 6.9954609 ## 6 7 6.2831022 ## 1 7 3.4264534 ## 4 7 2.8184938 ## 5 7 0.3522961   Exercise 4. Part a. Download the two files from D2L ncaa2018.csv and ncaa2019.csv, and read into data frames or tables. ncaa2018.csv comes from the same source as elo.csv from Homework 5, while ncaa2019.csv is the corresponding more recent data. These tables do not contain identical sets of columns, but we will be able to merge Finish by individual wrestlers. Do not print these tables.\nncaa2018.dat =read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2018.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot;) head(ncaa2018.dat) ## Weight Last First Conference Finish ELO ## 1 125 Atkins Thayer ACC NQ 1297.06 ## 2 125 Bentley LJ ACC NQ 1343.66 ## 3 125 Fausz Sean ACC cons 24 1380.84 ## 4 125 Hayes Louie ACC cons 12 1404.51 ## 5 125 Norstrem Kyle ACC cons 24 1348.79 ## 6 125 Bianchi Paul Big 12 cons 32 1312.73 ncaa2019.dat =read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2019.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot;) head(ncaa2019.dat) ## Weight Last First Finish ## 1 125 Lee Spencer 1 ## 2 125 Mueller Jack 2 ## 3 125 Rivera Sebastian 3 ## 4 125 Arujau Vitali 4 ## 5 125 Piccininni Nicholas 5 ## 6 125 Glory Pat 6 merged.ncaa \u0026lt;- merge(ncaa2018.dat, ncaa2019.dat, by = c(\u0026quot;Last\u0026quot;, \u0026quot;First\u0026quot;), all = FALSE)  Part b. The tables list the wrestlers qualifying for the NCAA 2018 and 2019 National Champions, respectively. Merge the tables into a single table that contains only those wrestlers who qualified for both tournaments. Use the columns Last and First to merge on; neither is unique for all wrestlers.\nThe merged table should have columns corresponding to Finish 2018 and Finish 2019 - you can leave the column names as the defaults produced by R or SAS. To check the merge, print the number of rows in the table, and determine if there are any missing values in either Finish column (sum or any are sufficient. Do not print the table.\nmerged.common.players \u0026lt;- merge(ncaa2018.dat, ncaa2019.dat, by=c(\u0026quot;Last\u0026quot;,\u0026quot;First\u0026quot;), all = FALSE) # numer of rows print(nrow(merged.common.players)) ## [1] 198 sum(is.na(merged.common.players$Finish.x)) ## [1] 0  Part c. Print a contingency table comparing Weight for 2018 and Weight for 2019. The sum of all cells in this table will be equal to the total number of wrestlers that competed in both tournaments; the sum of the main diagonal will be the number of wrestlers that competed in the same weight class for both. How many wrestlers changed weight classes?\nweight_contingency \u0026lt;- table( merged.common.players$Weight.x, merged.common.players$Weight.y, dnn = c(\u0026quot;Weight for 2018\u0026quot;, \u0026quot;Weight for 2019\u0026quot;) ) weight_contingency ## Weight for 2019 ## Weight for 2018 125 133 141 149 157 165 174 184 197 285 ## 125 20 2 0 0 0 0 0 0 0 0 ## 133 2 17 6 0 0 0 0 0 0 0 ## 141 0 2 14 5 0 0 0 0 0 0 ## 149 0 0 2 12 4 0 0 0 0 0 ## 157 0 0 0 3 12 2 0 0 0 0 ## 165 0 0 0 0 1 17 3 0 0 0 ## 174 0 0 0 0 0 1 18 1 0 0 ## 184 0 0 0 0 0 0 0 14 4 0 ## 197 0 0 0 0 0 0 0 4 15 0 ## 285 0 0 0 0 0 0 0 0 0 17 # The sum of all cells in this table will be equal to the total number of # wrestlers that competed in both tournaments sum(weight_contingency) ## [1] 198 # Diagonal sum sum(diag(weight_contingency)) ## [1] 156 # How many wrestlers changed weight classes? This gives: sum(weight_contingency)-sum(diag(weight_contingency)) ## [1] 42 The sum of all cells in this table will be equal to the total number of wrestlers that competed in both tournaments: which is 198\n  Exercise 5 Background I’m working on software that produces a repeated measures analysis. To test my code, I use published data and compare results. For one analysis, I used data from Contemporary Statistical Models for the Plant and Soil Sciences, Oliver Schabenberger and Francis J. Pierce, 2001. These data are measurements of the diameter of individual apples from selected apple trees.\n Part a. Download the AppleData.csv if you choose R, the SAS data is included in the SAS template. Note the file include comments for the data; you may need to specify comment character in import. Do not print this table.\nTo simplify this exercise, create a subset of the AppleData including only trees number 3, 7 and 10.\nAppleData=read.csv(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/AppleData.csv\u0026quot;, header = TRUE, comment.char = \u0026#39;#\u0026#39;) #subsetting data to include tree 3, 7 and 10 AppleData\u0026lt;- AppleData[AppleData$tree %in% c(3,7,10),] AppleData ## tree apple time diam ## 111 3 1 1 2.91 ## 112 3 1 2 3.00 ## 113 3 1 3 3.02 ## 114 3 1 4 3.03 ## 115 3 10 1 2.81 ## 116 3 10 2 2.89 ## 117 3 10 3 2.87 ## 118 3 10 4 2.93 ## 119 3 10 5 2.93 ## 120 3 10 6 2.94 ## 121 3 16 1 2.95 ## 122 3 16 2 3.00 ## 123 3 16 3 3.03 ## 124 3 16 4 3.03 ## 125 3 16 5 3.06 ## 126 3 16 6 3.08 ## 127 3 17 1 2.79 ## 128 3 17 2 2.83 ## 129 3 17 3 2.86 ## 130 3 17 4 2.87 ## 131 3 17 5 2.87 ## 132 3 17 6 2.93 ## 133 3 18 1 2.98 ## 134 3 18 2 3.03 ## 135 3 18 3 3.06 ## 136 3 18 4 3.09 ## 137 3 18 5 3.09 ## 138 3 18 6 3.09 ## 139 3 20 1 2.76 ## 140 3 20 2 2.82 ## 141 3 20 3 2.83 ## 142 3 20 4 2.85 ## 143 3 20 5 2.86 ## 144 3 20 6 2.88 ## 145 3 22 1 2.76 ## 146 3 22 2 2.82 ## 147 3 22 3 2.85 ## 148 3 22 4 2.87 ## 149 3 22 5 2.90 ## 150 3 22 6 2.90 ## 151 3 23 1 2.76 ## 152 3 23 2 2.78 ## 153 3 23 3 2.77 ## 154 3 23 4 2.79 ## 155 3 23 5 2.79 ## 156 3 23 6 2.79 ## 157 3 24 1 2.80 ## 158 3 24 2 2.85 ## 159 3 24 3 2.87 ## 160 3 24 4 2.87 ## 161 3 24 5 2.89 ## 162 3 24 6 2.92 ## 317 7 2 1 2.79 ## 318 7 2 2 2.89 ## 319 7 2 3 2.89 ## 320 7 2 4 2.91 ## 321 7 2 5 2.91 ## 322 7 2 6 2.95 ## 323 7 4 1 2.80 ## 324 7 4 2 2.81 ## 325 7 4 3 2.85 ## 326 7 4 4 2.91 ## 327 7 4 5 2.92 ## 328 7 4 6 2.96 ## 329 7 9 1 3.06 ## 330 7 9 2 3.15 ## 331 7 9 3 3.15 ## 332 7 9 4 3.23 ## 333 7 9 5 3.27 ## 334 7 9 6 3.31 ## 335 7 25 1 2.84 ## 336 7 25 2 2.86 ## 337 7 25 3 2.88 ## 338 7 25 4 2.93 ## 339 7 25 5 2.93 ## 340 7 25 6 2.96 ## 399 10 2 1 2.92 ## 400 10 2 2 2.95 ## 401 10 2 3 3.00 ## 402 10 2 4 3.01 ## 403 10 2 5 3.07 ## 404 10 5 1 2.87 ## 405 10 5 2 2.89 ## 406 10 5 3 2.94 ## 407 10 5 4 2.95 ## 408 10 5 5 3.01 ## 409 10 5 6 3.02 ## 410 10 8 1 2.76 ## 411 10 8 2 2.81 ## 412 10 8 3 2.86 ## 413 10 8 4 2.90 ## 414 10 9 1 2.91 ## 415 10 9 2 3.01 ## 416 10 9 3 3.07 ## 417 10 9 4 3.09 ## 418 10 9 5 3.11 ## 419 10 10 1 2.88 ## 420 10 10 2 2.88 ## 421 10 10 3 2.92 ## 422 10 10 4 2.97 ## 423 10 10 5 2.97 ## 424 10 10 6 2.99 ## 425 10 17 1 3.00 ## 426 10 17 2 3.05 ## 427 10 17 3 3.05 ## 428 10 17 4 3.06 ## 429 10 17 5 3.11 ## 430 10 18 1 2.85 ## 431 10 18 2 2.87 ## 432 10 18 3 2.91 ## 433 10 18 4 2.95 ## 434 10 18 5 2.98 ## 435 10 18 6 3.00 ## 436 10 21 1 2.76 ## 437 10 21 2 2.83 ## 438 10 21 3 2.84 ## 439 10 21 4 2.87 ## 440 10 21 5 2.88 ## 441 10 21 6 2.91 ## 442 10 22 1 3.25 ## 443 10 22 2 3.34 ## 444 10 22 3 3.34 ## 445 10 22 4 3.38 ## 446 10 22 5 3.47 ## 447 10 23 1 3.00 ## 448 10 23 2 3.06 ## 449 10 23 3 3.08 ## 450 10 23 4 3.14 ## 451 10 23 5 3.18  Part b. Reshape or transpose this data from the long form to the wide form. Call this data AppleWide. This table should have one column for Tree, one column for Apple and six columns, diam.1 - diam.6. The values in the time columns come from diam in AppleData.\nAppleData1\u0026lt;- as.data.frame(AppleData) AppleWide\u0026lt;- reshape(AppleData1, direction = \u0026quot;wide\u0026quot;, idvar = c(\u0026quot;apple\u0026quot;,\u0026quot;tree\u0026quot;), timevar = \u0026quot;time\u0026quot;) AppleWide ## tree apple diam.1 diam.2 diam.3 diam.4 diam.5 diam.6 ## 111 3 1 2.91 3.00 3.02 3.03 NA NA ## 115 3 10 2.81 2.89 2.87 2.93 2.93 2.94 ## 121 3 16 2.95 3.00 3.03 3.03 3.06 3.08 ## 127 3 17 2.79 2.83 2.86 2.87 2.87 2.93 ## 133 3 18 2.98 3.03 3.06 3.09 3.09 3.09 ## 139 3 20 2.76 2.82 2.83 2.85 2.86 2.88 ## 145 3 22 2.76 2.82 2.85 2.87 2.90 2.90 ## 151 3 23 2.76 2.78 2.77 2.79 2.79 2.79 ## 157 3 24 2.80 2.85 2.87 2.87 2.89 2.92 ## 317 7 2 2.79 2.89 2.89 2.91 2.91 2.95 ## 323 7 4 2.80 2.81 2.85 2.91 2.92 2.96 ## 329 7 9 3.06 3.15 3.15 3.23 3.27 3.31 ## 335 7 25 2.84 2.86 2.88 2.93 2.93 2.96 ## 399 10 2 2.92 2.95 3.00 3.01 3.07 NA ## 404 10 5 2.87 2.89 2.94 2.95 3.01 3.02 ## 410 10 8 2.76 2.81 2.86 2.90 NA NA ## 414 10 9 2.91 3.01 3.07 3.09 3.11 NA ## 419 10 10 2.88 2.88 2.92 2.97 2.97 2.99 ## 425 10 17 3.00 3.05 3.05 3.06 3.11 NA ## 430 10 18 2.85 2.87 2.91 2.95 2.98 3.00 ## 436 10 21 2.76 2.83 2.84 2.87 2.88 2.91 ## 442 10 22 3.25 3.34 3.34 3.38 3.47 NA ## 447 10 23 3.00 3.06 3.08 3.14 3.18 NA  Part c. To confirm that you’ve reshaped correctly, print column means for the wide data set and use an aggregate or apply function to compute time means for the long format.\nmeanWide= colMeans(AppleWide, na.rm = T) meanWide ## tree apple diam.1 diam.2 diam.3 diam.4 diam.5 ## 6.739130 14.173913 2.878696 2.931304 2.953913 2.983913 3.009524 ## diam.6 ## 2.976875 meanLong = tapply(AppleData1$diam, AppleData1$time, mean, na.rm=T) meanLong ## 1 2 3 4 5 6 ## 2.878696 2.931304 2.953913 2.983913 3.009524 2.976875  Part d. I choose this example for a test case because it shows a case where the best repeated measures model is an auto-regressive model - each measure is correlated with the preceding measure. We can estimate the degree of using the following R code. You don’t need to evaluate this code for this exercise; it’s provided as a motivation for reshaping the data.\nmult.lm \u0026lt;- lm(cbind(diam.1, diam.2, diam.3, diam.4, diam.5, diam.6) ~ tree, data=AppleWide) mult.manova \u0026lt;- manova(mult.lm) print(cov2cor(estVar(mult.lm)))   Exercise 6 This is an exercise in computing the Wilcoxon Signed Rank test. We will be using an example from NIST (NATR332.DAT). See https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/signrank.htm .\nThe data are provided:\nNATR332.DAT \u0026lt;- data.frame( Y1 = c(146,141,135,142,140,143,138,137,142,136), Y2 = c(141,143,139,139,140,141,138,140,142,138) ) Part a. Add a column Difference that is the difference between Y1 and Y2. For further analysis, exclude any rows where the difference is 0.\nNext add add the column Rank, which will be the rank of the absolute value of Difference.\n#adding column Difference in the data frame NATR332.DAT$Difference \u0026lt;- NATR332.DAT$Y1-NATR332.DAT$Y2 NATR332.DAT\u0026lt;- NATR332.DAT[NATR332.DAT$Difference != 0,] NATR332.DAT ## Y1 Y2 Difference ## 1 146 141 5 ## 2 141 143 -2 ## 3 135 139 -4 ## 4 142 139 3 ## 6 143 141 2 ## 8 137 140 -3 ## 10 136 138 -2 #Next, we rank and add a column in the data frame NATR332.DAT$Rank \u0026lt;- rank(abs(NATR332.DAT$Difference)) NATR332.DAT ## Y1 Y2 Difference Rank ## 1 146 141 5 7.0 ## 2 141 143 -2 2.0 ## 3 135 139 -4 6.0 ## 4 142 139 3 4.5 ## 6 143 141 2 2.0 ## 8 137 140 -3 4.5 ## 10 136 138 -2 2.0  Part c. Add the column SignedRank by applying the sign (+ or -) of Difference, to to Rank (that is, if Difference is \u0026lt; 0, then SignedRank is -Rank, otherwise SignedRank is Rank).\nSignedRank \u0026lt;- ifelse(NATR332.DAT$Difference \u0026lt; 0, - NATR332.DAT$Rank, + NATR332.DAT$Rank) NATR332.DAT$SignedRank\u0026lt;- round(SignedRank, 0) NATR332.DAT ## Y1 Y2 Difference Rank SignedRank ## 1 146 141 5 7.0 7 ## 2 141 143 -2 2.0 -2 ## 3 135 139 -4 6.0 -6 ## 4 142 139 3 4.5 4 ## 6 143 141 2 2.0 2 ## 8 137 140 -3 4.5 -4 ## 10 136 138 -2 2.0 -2  Part d. Compute the sum of the positive ranks, and the absolute value of the sum of the negative ranks. Let \\(W\\) be the minimum of these two sums. Print \\(W\\).\npositive.sum = sum(NATR332.DAT$SignedRank[NATR332.DAT$SignedRank \u0026gt; 0]) positive.sum ## [1] 13 abs.negative.sum = abs(sum(NATR332.DAT$SignedRank[NATR332.DAT$SignedRank \u0026lt; 0])) abs.negative.sum ## [1] 14 # Let $W$ be the minimum of these two sums. W = min(abs.negative.sum, positive.sum) W ## [1] 13 The expected mean of \\(W\\) is calculated by\n\\[\\mu_W = N_r*(N_r+1)/4\\] with a standard deviation of\n\\[ \\sigma_W = \\sqrt{\\frac{N_r(N_r+1)(2N_r+1)}{6}} \\]\nwhere \\(N_r\\) is the number of ranked values (excluding differences of 0). Calculate a \\(z\\) score by\n\\[z_W = (W - \\mu_W)/\\sigma_W\\] Print both \\(\\mu_W\\) and \\(z_W\\).\nmu_W = (7*(7+1))/4 mu_W ## [1] 14 sigma_W = sqrt((7*(7+1)*(2*7+1))/6) sigma_W ## [1] 11.83216 z_W = (W - mu_W)/sigma_W z_W ## [1] -0.08451543 p\u0026lt;- pnorm(z_W, lower.tail = TRUE) p ## [1] 0.4663233 The p-value matches with the value calculated using Wilcoxon test below.\nThe NIST page gives a p-values based on the continuity correction. We are not computing this correction. You can compute the \\(P(z\u0026gt;z_W)\\) of your \\(z_W\\) (using the normal distribution) and compare it to\nwilcox.test(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, correct = FALSE, alternative = \u0026quot;less\u0026quot;) ## Warning in wilcox.test.default(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = ## TRUE, : cannot compute exact p-value with ties ## ## Wilcoxon signed rank test ## ## data: NATR332.DAT$Y1 and NATR332.DAT$Y2 ## V = 13.5, p-value = 0.466 ## alternative hypothesis: true location shift is less than 0 while the corrected p-values are given by\nwilcox.test(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, correct = TRUE, alternative = \u0026quot;less\u0026quot;) ## Warning in wilcox.test.default(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = ## TRUE, : cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: NATR332.DAT$Y1 and NATR332.DAT$Y2 ## V = 13.5, p-value = 0.5 ## alternative hypothesis: true location shift is less than 0 wilcox.test(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, correct = TRUE, alternative = \u0026quot;greater\u0026quot;) ## Warning in wilcox.test.default(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = ## TRUE, : cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: NATR332.DAT$Y1 and NATR332.DAT$Y2 ## V = 13.5, p-value = 0.5677 ## alternative hypothesis: true location shift is greater than 0   ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"e552a64896cce541241d8779410fd423","permalink":"/achalneupane.github.io/post/data_manipulation/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/data_manipulation/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Data manipulation","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  Exercise 1. This exercise will repeat Exercise 1 from Homework 4, but using a data table.\nPart a. Create a data table or frame with 3 defined columns: - Let M1 be a sequence of means from 320-420, incremented by 10. - Let M2 be 270. - Let SD be a pooled standard deviation of 150.\nDefine and print the tabke in the space below. Do not create individual vectors for this exercise, outside of the data frame, if you use R. In SAS, you may use IML to create a matrix and save the matrix as a data table, or define a sequence (DO) in the DATA step. I’ve included framework code in the SAS template.\nM.table \u0026lt;- data.frame(M1= seq(320, 420, 10), M2= 270, SD = 150) M.table ## M1 M2 SD ## 1 320 270 150 ## 2 330 270 150 ## 3 340 270 150 ## 4 350 270 150 ## 5 360 270 150 ## 6 370 270 150 ## 7 380 270 150 ## 8 390 270 150 ## 9 400 270 150 ## 10 410 270 150 ## 11 420 270 150 Add to the data table a column containing required replicates, letting \\(s_i = s_j = s_{pooled}\\). Also add a column containing Cohen’s \\(d\\).\nTo show results, either print the table or plot required replicates versus \\(d\\) as in the previous homework.\n# Using combined function from last week to calculate effect size and Required replicates combined \u0026lt;- function (m1,m2 = 270, s_pooled = 150, alpha = 0.05, beta = 0.2){ cv \u0026lt;- (s_pooled)/((m1+m2)/2) percent.diff \u0026lt;- ((m1-m2)/((m1+m2)/2)) cohens_d \u0026lt;-(abs(m1-m2)/(s_pooled)) n \u0026lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) n \u0026lt;- round(n,0) value \u0026lt;- (list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0), EffectSize = cohens_d)) return(value) } data \u0026lt;- combined(m1 = M.table$M1, s_pooled = M.table$SD) M.table$RequiredReplicates \u0026lt;- data$RequiredReplicates M.table$EffectSize \u0026lt;- data$EffectSize # Wanted table M.table ## M1 M2 SD RequiredReplicates EffectSize ## 1 320 270 150 141 0.3333333 ## 2 330 270 150 98 0.4000000 ## 3 340 270 150 72 0.4666667 ## 4 350 270 150 55 0.5333333 ## 5 360 270 150 44 0.6000000 ## 6 370 270 150 35 0.6666667 ## 7 380 270 150 29 0.7333333 ## 8 390 270 150 25 0.8000000 ## 9 400 270 150 21 0.8666667 ## 10 410 270 150 18 0.9333333 ## 11 420 270 150 16 1.0000000 # Now plotting # attach(M.table) # cal.lm \u0026lt;- lm(M.table$RequiredReplicates ~ M.table$EffectSize) # plot(RequiredReplicates ~ EffectSize) plot(M.table$RequiredReplicates ~ M.table$EffectSize) # abline(cal.lm) abline(v = 0.5, col= \u0026#39;red\u0026#39;)   Exercise 2 Part a. You will repeat the calculations from Homework 4, Ex 2, but this time, using a data table. However, instead of a \\(5 \\times 6\\) matrix, the result with be a table with 30 rows, each corresponding to a unique combination of CV from \\(8, 12, ..., 28\\) and Diff from \\(5,10, ... , 25\\).\nThe table should look something like\n\\[ \\left\\{ \\begin{array}{cc} CV \u0026amp; Diff \\\\ 8 \u0026amp; 5 \\\\ 8 \u0026amp; 10 \\\\ 8 \u0026amp; 15 \\\\ \\vdots \u0026amp; \\vdots \\\\ 12 \u0026amp; 5 \\\\ 12 \u0026amp; 10 \\\\ 12 \u0026amp; 15 \\\\ \\vdots \u0026amp; \\vdots \\\\ 28 \u0026amp; 5 \\\\ 28 \u0026amp; 10 \\\\ 28 \u0026amp; 15 \\\\ \\end{array} \\right\\} \\] Test your required replicates calculations by calculating required replicates for each combination of CV and Diff using the default values for \\(\\alpha\\) and \\(\\beta\\). Name this column Moderate.\nCalculate required replicaes again, but this time let \\(\\alpha = 0.01\\) and let \\(\\beta = 0.1\\). Label this column Conservative. Repeat the calculations, but this time let \\(\\alpha = 0.1\\) and let \\(\\beta = 0.2\\).\nIf you choose SAS, you can use the framework code from the first exercise.\nnewdata.dat \u0026lt;- data.frame(CV = rep(seq(8,28,4), each = 5), Diff = rep(seq(5,25,5),6)) newdata.dat ## CV Diff ## 1 8 5 ## 2 8 10 ## 3 8 15 ## 4 8 20 ## 5 8 25 ## 6 12 5 ## 7 12 10 ## 8 12 15 ## 9 12 20 ## 10 12 25 ## 11 16 5 ## 12 16 10 ## 13 16 15 ## 14 16 20 ## 15 16 25 ## 16 20 5 ## 17 20 10 ## 18 20 15 ## 19 20 20 ## 20 20 25 ## 21 24 5 ## 22 24 10 ## 23 24 15 ## 24 24 20 ## 25 24 25 ## 26 28 5 ## 27 28 10 ## 28 28 15 ## 29 28 20 ## 30 28 25 combined \u0026lt;- function (cv, percent.diff, alpha = 0.05, beta = 0.2){ cv \u0026lt;- cv percent.diff \u0026lt;- percent.diff n \u0026lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) n \u0026lt;- round(n,0) value \u0026lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0)) return(value) } value \u0026lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff) # Adding Moderate column newdata.dat$Moderate \u0026lt;- value$RequiredReplicates # Adding Conservative column value \u0026lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff, alpha = 0.01, beta = 0.1) newdata.dat$Conservative \u0026lt;- value$RequiredReplicates # Repeat the calculations, but this time let $\\alpha = 0.1$ and let $\\beta = # 0.2$. # Adding Liberal column value \u0026lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff, alpha = 0.1, beta = 0.2) newdata.dat$Liberal \u0026lt;- value$RequiredReplicates # Print the table # newdata.dat To show your work, some ideas for graphs:\n Plot required replicates vs CV and Diff, using different colors or symbols for Moderate,Conservative and Liberal  attach(newdata.dat) # Effect size Vs Required replicates plot(CV, Moderate, col = \u0026quot;black\u0026quot;, type = \u0026#39;b\u0026#39;, pch = 8, xlab=\u0026quot;Effect Size\u0026quot;, ylab=\u0026quot;Required replicates\u0026quot;, ylim = c(min(Moderate, Conservative, Liberal),max(Moderate, Conservative, Liberal))) points(CV, Conservative, type = \u0026#39;b\u0026#39;, pch = 2, col = \u0026quot;red\u0026quot;) points(CV, Liberal, type = \u0026#39;b\u0026#39;, pch = 20, col = \u0026quot;blue\u0026quot;) legend(\u0026quot;topleft\u0026quot;, legend = c(\u0026quot;Moderate\u0026quot;, \u0026quot;Conservative\u0026quot;, \u0026quot;Liberal\u0026quot;), col = c(\u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), bty = \u0026quot;o\u0026quot;, pch = c(8, 2, 20), pt.cex = 2, cex = 1.2, text.col = \u0026quot;black\u0026quot;, horiz = F , inset = c(0.1, 0.1)) # Percent diff vs Required replicates plot(Diff, Moderate, col = \u0026quot;black\u0026quot;, type = \u0026#39;b\u0026#39;,pch = 8, xlab=\u0026quot;Percent diff\u0026quot;, ylab=\u0026quot;Required replicates\u0026quot;, ylim = c(min(Moderate, Conservative, Liberal),max(Moderate, Conservative, Liberal))) points(Diff, Conservative, type = \u0026#39;b\u0026#39;, pch = 2, col = \u0026quot;red\u0026quot;) points(Diff, Liberal, type = \u0026#39;b\u0026#39;, pch = 20, col = \u0026quot;blue\u0026quot;) legend(\u0026quot;topright\u0026quot;, legend = c(\u0026quot;Moderate\u0026quot;, \u0026quot;Conservative\u0026quot;, \u0026quot;Liberal\u0026quot;), col = c(\u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), bty = \u0026quot;o\u0026quot;, pch = c(8, 2, 20), pt.cex = 2, cex = 1.2, text.col = \u0026quot;black\u0026quot;, horiz = F , inset = c(0.1, 0.1))  Plot Conservative vs Moderate and Liberal vs Moderate, including a line with slope 1 and intercept 0.  attach(newdata.dat) ## The following objects are masked from newdata.dat (pos = 3): ## ## Conservative, CV, Diff, Liberal, Moderate #Conservative Vs Moderate plot(Conservative ~ Moderate, type = \u0026#39;b\u0026#39;, col = \u0026quot;black\u0026quot;) abline(a=1, b = 0) #Liberal Vs Moderate plot(Liberal ~ Moderate, type = \u0026#39;b\u0026#39;, col = \u0026quot;black\u0026quot;) abline(a=1, b = 0)   Exercise 3 You’ll work with data from U.S. Wholesale price for pumpkins 2018 (https://www.ers.usda.gov/newsroom/trending-topics/pumpkins-background-statistics/, Table 1)\nPart a Download the file pumpkins.csv from D2L and read the file into a data frame. Print a summary of the table.\npumpkins \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;) pumpkins \u0026lt;- data.frame(pumpkins) attach(pumpkins) print(summary(pumpkins)) ## Month Week Class Size ## October :15 Min. :1.000 Blue : 6 Large :18 ## September:15 1st Qu.:3.000 Cinderella: 7 Medium:12 ## Median :4.500 Howden :10 ## Mean :4.433 Pie : 7 ## 3rd Qu.:6.000 ## Max. :7.000 ## Price ## Min. :121.0 ## 1st Qu.:130.2 ## Median :175.0 ## Mean :178.1 ## 3rd Qu.:216.2 ## Max. :257.0  Part b To show that the data was read correctly, create three plots. Plot 1. Price vs Week 2. Price vs Class 3. Size vs Class\nThese three plots should reproduce the three types of plots shown in the RegressionEtcPlots video, Categorical vs Categorical, Continuous vs Continuous and Continuous vs Categorical. Add these as titles to your plots, as appropriate.\nattach(pumpkins) ## The following objects are masked from pumpkins (pos = 3): ## ## Class, Month, Price, Size, Week plot(Price ~ Week, main = \u0026quot;**Continuous vs Continuous**\u0026quot;) plot(Price ~ Class, main = \u0026quot;**Continuous vs Categorical**\u0026quot;) plot(Size ~ Class, main = \u0026quot;**Categorical vs Categorical**\u0026quot;) From these plots, you should be able to answer these questions:\nAre some Weeks missing Price observations? Yes some Weeks are missing some Price observations.\n Do Prices vary more for some Classes? Yes, prices vary more for some classes, such as Pie.\n Do all Classes have the same Sizes? No, they are of different sizes.\n    Exercise 4 Calculate a one-way analysis of variance from the pumpkin data in Exercise 3.\nOption A Let \\(y\\) be the Price. Let the \\(k\\) treatments be Class. Let \\(T_i\\) be the Price total for Class \\(i\\) and let \\(r_i\\) be the number of observations for Class \\(i\\). Denote the total number of observations \\(N = \\sum r_i\\).\nPart a Find the treatment totals \\(\\mathbf{T} = T_1 \\dots T_k\\) and replicates per treatment \\(\\mathbf{r} = r_1 \\dots r_k\\) from the pumpkin data, using group summary functions and compute a grand total \\(G\\) for Price. Print \\(\\mathbf{T}\\), \\(\\mathbf{r}\\) and \\(G\\) below. In SAS, you can use proc summary or proc means to compute \\(T\\) adn \\(r\\) and output a summary table. I find the rest is easier in IML (see use to access data tables in IML).\npumpkins.dat \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;) head(pumpkins.dat) ## Month Week Class Size Price ## 1 September 1 Pie Medium 175 ## 2 September 2 Pie Medium 199 ## 3 September 3 Pie Medium 224 ## 4 September 4 Pie Medium 224 ## 5 October 5 Pie Medium 219 ## 6 October 6 Pie Medium 219 class(pumpkins.dat) ## [1] \u0026quot;data.frame\u0026quot; k \u0026lt;- length(levels(pumpkins.dat$Class)) k ## [1] 4 T \u0026lt;- tapply(pumpkins.dat$Price, pumpkins.dat$Class, sum) # can also use aggregate # T \u0026lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = sum) T ## Blue Cinderella Howden Pie ## 1050 1529 1279 1484 r \u0026lt;- table(pumpkins.dat$Class) # can also use aggregate # aggregate(pumpkins.dat$Class, by = list(pumpkins.dat$Class), FUN = length) r ## ## Blue Cinderella Howden Pie ## 6 7 10 7 N \u0026lt;- sum(r) G \u0026lt;- sum(pumpkins.dat$Price) G ## [1] 5342  Part b Calculate sums of squares as\n\\[ \\begin{aligned} \\text{Correction Factor : } C \u0026amp;= \\frac{G^2}{N} \\\\ \\text{Total SS : } \u0026amp;= \\sum y^2 - C \\\\ \\text{Treatments SS : } \u0026amp;= \\sum \\frac{T_i^2}{r_i} -C \\\\ \\text{Error SS : } \u0026amp;= \\text{Total SS} - \\text{Treatments SS} \\\\ \\end{aligned} \\] and calcute \\(MSB = (\\text{Treatments SS})/(k-1)\\) and \\(MSW = (\\text{Error SS})/(N-k)\\).\nC \u0026lt;- G^2/N C ## [1] 951232.1 TotalSS \u0026lt;- sum((pumpkins.dat$Price)^2)-C TotalSS ## [1] 48805.87 TreatmentsSS \u0026lt;- sum(T^2/r) - C TreatmentsSS ## [1] 44687.25 ErrorSS \u0026lt;- TotalSS - TreatmentsSS ErrorSS ## [1] 4118.614 MSB \u0026lt;- TreatmentsSS/(k-1) MSB ## [1] 14895.75 MSW \u0026lt;- ErrorSS/(N-k) MSW ## [1] 158.4082  Part c. Calculate an F-ratio and a \\(p\\) for this \\(F\\), using the \\(F\\) distribution with \\(k-1\\) and \\(N-k\\) degrees of freedom. Use \\(\\alpha=0.05\\).\nF.ratio \u0026lt;- MSB/MSW F.ratio ## [1] 94.03394 df1 \u0026lt;- k-1 df2 \u0026lt;- N-k p.value \u0026lt;- pf(F.ratio, df1, df2, lower.tail = FALSE) p.value ## [1] 4.421291e-14 To check your work, use aov as illustated in the chunk below:\nsummary(aov(Price ~ Class, data=pumpkins.dat)) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Class 3 44687 14896 94.03 4.42e-14 *** ## Residuals 26 4119 158 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1   Option B You may resue code from Exercise 6, Homework 4. Use group summary functions to calculate means, standard deviations and replicates from the pumpkin data, then calculate \\(MSW\\) and \\(MSB\\) as previously. Report the F-ratio and \\(p\\) value as above.\nmean.groups \u0026lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = mean) mean.groups ## Group.1 x ## 1 Blue 175.0000 ## 2 Cinderella 218.4286 ## 3 Howden 127.9000 ## 4 Pie 212.0000 mean.groups \u0026lt;- mean.groups$x sd.groups \u0026lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = sd) sd.groups ## Group.1 x ## 1 Blue 0.000000 ## 2 Cinderella 17.924445 ## 3 Howden 3.695342 ## 4 Pie 18.565200 sd.groups \u0026lt;- sd.groups$x k \u0026lt;- length(mean.groups) k ## [1] 4 length.groups \u0026lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = length) # We have n samples in each pumpkin.dat Class; so our population size is N: n \u0026lt;- length.groups$x n ## [1] 6 7 10 7 N \u0026lt;- sum(n) N ## [1] 30 #calculating MSB mean.mean.groups \u0026lt;- mean(mean.groups) MSB = (sum(n*(mean.groups-mean.mean.groups)^2))/(k-1) MSB ## [1] 15173 MSW \u0026lt;- sum((n-1) * sd.groups^2)/(N-k) MSW ## [1] 158.4082 F.ratio \u0026lt;- MSB/MSW F.ratio ## [1] 95.78418 df1 \u0026lt;- k-1 df2 \u0026lt;- N-k p.value \u0026lt;- pf(F.ratio, df1, df2, lower.tail = FALSE) p.value ## [1] 3.551828e-14   Exercise 5 Part a Go to http://www.itl.nist.gov/div898/strd/anova/SiRstv.html and use the data listed under Data File in Table Format (https://www.itl.nist.gov/div898/strd/anova/SiRstvt.dat)\n Part b Edit this into a file that can be read into R or SAS, or find an appropriate function that can read the file as-is. You will need to upload the edited file to D2L along with your Rmd/SAS files. Provide a brief comment on changes you make, or assumptions about the file needed for you file to be read into R/SAS. Read the data into a data frame or data table.\n# Here we read the .dat file first skipping the 59 lines as indicated in SiRstvt.dat file. df \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/SiRstvt.dat\u0026quot;, header = FALSE, skip = 59) # read file as dataframe df \u0026lt;- as.data.frame(df) # We need to change the column names as alpha-numeric to work with ease names(df) \u0026lt;- paste0(\u0026quot;col_\u0026quot;, seq(1,5,1)) df ## col_1 col_2 col_3 col_4 col_5 ## 1 196.3052 196.3042 196.1303 196.2795 196.2119 ## 2 196.1240 196.3825 196.2005 196.1748 196.1051 ## 3 196.1890 196.1669 196.2889 196.1494 196.1850 ## 4 196.2569 196.3257 196.0343 196.1485 196.0052 ## 5 196.3403 196.0422 196.1811 195.9885 196.2090  Part c There are 5 columns in these data. Calculate mean and sd and sample size for each column in this data, using column summary functions. Print the results below\n# Calculating the mean (or summary) sapply(df, mean) ## col_1 col_2 col_3 col_4 col_5 ## 196.2431 196.2443 196.1670 196.1481 196.1432 # summary(df, digits = 7) print(data.frame(sapply(df, summary))) ## col_1 col_2 col_3 col_4 col_5 ## Min. 196.1240 196.0422 196.0343 195.9885 196.0052 ## 1st Qu. 196.1890 196.1669 196.1303 196.1485 196.1051 ## Median 196.2569 196.3042 196.1811 196.1494 196.1850 ## Mean 196.2431 196.2443 196.1670 196.1481 196.1432 ## 3rd Qu. 196.3052 196.3257 196.2005 196.1748 196.2090 ## Max. 196.3403 196.3825 196.2889 196.2795 196.2119 # calculating length for all columns sapply(df, length) ## col_1 col_2 col_3 col_4 col_5 ## 5 5 5 5 5 # calculating sd for all columns sapply(df, sd) ## col_1 col_2 col_3 col_4 col_5 ## 0.08747329 0.13797498 0.09372413 0.10422674 0.08844797 Determine the largest and smallest means, and their corresponding standard deviations, and calculate an effect size and required replicates to experimentally detect this effect.\nIf you defined functions in the previous exercises, you should be able to call them here.\ntt\u0026lt;- as.data.frame(sapply(df, summary)) # Assigning smallest mean as m1 (Col_5) m1 \u0026lt;- min(tt[rownames(tt)==\u0026quot;Mean\u0026quot;,]) m1 ## [1] 196.1432 # Assigning largest mean as m2 (col_2) m2 \u0026lt;- max(tt[rownames(tt)==\u0026quot;Mean\u0026quot;,]) m2 ## [1] 196.2443 # Now, getting sd for column 2 (largest mean) and 5 (smallest mean) sd \u0026lt;- sapply(df, sd) # SD from smallest mean column and largest mean column # SD for column 5 (smallest mean) s1 \u0026lt;- as.numeric(sd[5]) s1 ## [1] 0.08844797 # SD for column 2 (largest mean) s2 \u0026lt;- as.numeric(sd[2]) s2 ## [1] 0.137975 # Calculating the effect size cohen.d \u0026lt;- function(m1,s1,m2,s2){ cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) return(cohens_d) } cohen.d(m1 = m1, s1 = s1, m2 = m2, s2 = s2) ## [1] 0.8720476 # calculating the Required replicates required.replicates \u0026lt;- function (m1,s1, m2,s2, alpha=0.05, beta=0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) return(round(n,0)) } # required replicates required.replicates(m1 = m1, s1 = s1, m2 = m2, s2 = s2) ## [1] 21   Exercise 6 There is a web site (https://www.wrestlestat.com/rankings/starters) that ranks college wrestlers using an ELO scoring system (https://en.wikipedia.org/wiki/Elo_rating_system). I was curious how well the rankings predicted performance, so I gathered data from th 2018 NCAA Wrestling Championships (https://i.turner.ncaa.com/sites/default/files/external/gametool/brackets/wrestling_d1_2018.pdf). Part of the data are on D2L in the file elo.csv. You will need to download the file to your computer for this exercise.\nRead the dzta below and print a summary. The dzta were created by writing a data frame from R to csv (write.csv), so the first column is row number and does not have a header entry (the header name is an empty string).\nelo.dat \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/elo.csv\u0026quot;, header = TRUE, row.names = 1, sep = \u0026quot;,\u0026quot;) # elo.dat print(summary(elo.dat)) ## Weight Conference ELO ActualFinish ## Min. :125.0 Big Ten:87 Min. :1228 AA :80 ## 1st Qu.:141.0 EIWA :55 1st Qu.:1342 cons 12:40 ## Median :157.0 Big 12 :52 Median :1372 cons 16:40 ## Mean :170.9 ACC :40 Mean :1379 cons 24:79 ## 3rd Qu.:184.0 MAC :34 3rd Qu.:1410 cons 32:80 ## Max. :285.0 Pac 12 :25 Max. :1584 cons 33:10 ## (Other):36 ## ExpectedFinish ## E[AA] :80 ## E[cons 12]:36 ## E[cons 16]:36 ## E[cons 24]:66 ## E[cons 32]:46 ## E[NQ] :65 ##  Each row corresponds to an individual wrestler, his weight class and collegiate conference. The WrestleStat ELO score is listed, along with his tournament finish round (i.e. AA = 1-8 place, cons 12 = lost in the final consolation round, etc.). I calculated an expected finish based on his ELO ranking within the weight class, where E[AA] = top 8 ranked, expected to finish as AA, etc.\nProduce group summaries or plots to answer the following:\n What are the mean and standard deviations of ELO by Expected Finish and by Actual Finish?  # By both groups # mean.ExActFinish \u0026lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ExpectedFinish, elo.dat$ActualFinish), mean) # mean.ExActFinish # First By Expected Finish: mean.ExFinish \u0026lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ExpectedFinish), FUN = mean) mean.ExFinish ## Group.1 x ## 1 E[AA] 1451.336 ## 2 E[cons 12] 1395.442 ## 3 E[cons 16] 1379.404 ## 4 E[cons 24] 1357.369 ## 5 E[cons 32] 1334.704 ## 6 E[NQ] 1332.821 SD.ExFinish \u0026lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ExpectedFinish), FUN = sd) SD.ExFinish ## Group.1 x ## 1 E[AA] 41.04978 ## 2 E[cons 12] 17.77768 ## 3 E[cons 16] 13.11593 ## 4 E[cons 24] 16.02282 ## 5 E[cons 32] 18.02051 ## 6 E[NQ] 52.69272 # Now, by Actual Finish mean.ActFinish \u0026lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ActualFinish), FUN = mean) mean.ActFinish ## Group.1 x ## 1 AA 1444.556 ## 2 cons 12 1400.708 ## 3 cons 16 1371.745 ## 4 cons 24 1355.130 ## 5 cons 32 1333.270 ## 6 cons 33 1343.795 SD.ActFinish \u0026lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ActualFinish), FUN = sd) SD.ActFinish ## Group.1 x ## 1 AA 50.93285 ## 2 cons 12 29.22633 ## 3 cons 16 34.28861 ## 4 cons 24 30.95125 ## 5 cons 32 34.08563 ## 6 cons 33 28.30588  Do all conferences have similar quality, or might we suspect one or more conferences have better wrestlers than the rest? (You don’t need to perform an analysis, just argue, based on the summary, if a deeper analysis is warranted).  aggregate(elo.dat$Conference, by = list(elo.dat$ExpectedFinish, elo.dat$ActualFinish), FUN = summary) ## Group.1 Group.2 x.ACC x.Big 12 x.Big Ten x.EIWA x.EWL x.MAC x.Pac 12 ## 1 E[AA] AA 7 4 30 8 1 3 4 ## 2 E[cons 12] AA 2 3 1 1 1 1 0 ## 3 E[cons 16] AA 1 1 4 0 0 0 0 ## 4 E[cons 24] AA 0 1 0 0 0 0 0 ## 5 E[cons 32] AA 0 1 0 0 0 0 0 ## 6 E[NQ] AA 0 1 2 0 0 2 1 ## 7 E[AA] cons 12 2 3 2 4 1 1 0 ## 8 E[cons 12] cons 12 1 1 4 4 1 0 1 ## 9 E[cons 16] cons 12 1 1 1 0 0 1 0 ## 10 E[cons 24] cons 12 0 2 2 1 0 0 1 ## 11 E[NQ] cons 12 0 1 1 0 0 1 0 ## 12 E[AA] cons 16 2 0 2 1 0 0 2 ## 13 E[cons 12] cons 16 0 0 1 0 2 0 1 ## 14 E[cons 16] cons 16 0 1 4 1 0 0 1 ## 15 E[cons 24] cons 16 0 1 3 3 0 1 0 ## 16 E[cons 32] cons 16 1 1 2 2 0 0 0 ## 17 E[NQ] cons 16 1 1 2 0 1 0 2 ## 18 E[AA] cons 24 1 0 1 1 0 0 0 ## 19 E[cons 12] cons 24 1 1 1 4 0 1 0 ## 20 E[cons 16] cons 24 5 2 1 1 0 1 1 ## 21 E[cons 24] cons 24 8 4 5 4 3 4 1 ## 22 E[cons 32] cons 24 0 3 3 5 1 3 1 ## 23 E[NQ] cons 24 0 2 4 2 1 2 1 ## 24 E[cons 12] cons 32 0 0 0 2 0 0 0 ## 25 E[cons 16] cons 32 0 2 1 1 1 0 1 ## 26 E[cons 24] cons 32 1 5 3 2 2 2 1 ## 27 E[cons 32] cons 32 2 4 3 4 3 5 1 ## 28 E[NQ] cons 32 3 6 3 2 4 4 4 ## 29 E[cons 16] cons 33 0 0 1 0 0 0 0 ## 30 E[cons 24] cons 33 0 0 0 1 0 2 1 ## 31 E[cons 32] cons 33 0 0 0 1 0 0 0 ## 32 E[NQ] cons 33 1 0 0 0 0 0 0 ## x.SoCon ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 1 ## 9 1 ## 10 0 ## 11 0 ## 12 0 ## 13 0 ## 14 0 ## 15 0 ## 16 0 ## 17 1 ## 18 0 ## 19 0 ## 20 0 ## 21 0 ## 22 0 ## 23 0 ## 24 0 ## 25 0 ## 26 1 ## 27 0 ## 28 7 ## 29 0 ## 30 1 ## 31 0 ## 32 2 # aggregate(elo.dat$Conference, by = list(elo.dat$ExpectedFinish, elo.dat$ActualFinish), length) Based on this table, Big Ten seems to have better wrestlers.\n How well does ELO predict finish? Use a contingency table or mosaic plot to show how often, say, and AA finish corresponds to an E[AA] finish.  contingency.table \u0026lt;- table(elo.dat$ExpectedFinish, elo.dat$ActualFinish) # Contingency table contingency.table ## ## AA cons 12 cons 16 cons 24 cons 32 cons 33 ## E[AA] 57 13 7 3 0 0 ## E[cons 12] 9 13 4 8 2 0 ## E[cons 16] 6 5 7 11 6 1 ## E[cons 24] 1 6 8 29 17 5 ## E[cons 32] 1 0 6 16 22 1 ## E[NQ] 6 3 8 12 33 3 # # can convert table to dataframe # contingency.table.dat \u0026lt;- as.data.frame.matrix(contingency.table) # Also, plot attach(elo.dat) plot(ActualFinish ~ ExpectedFinish) Based on the contingency table, E[AA] AA are associated 57 times\n Does this data set include non-qualifiers? (The NCAA tournament only allows 33 wreslers per weight class).  table(elo.dat$Weight) ## ## 125 133 141 149 157 165 174 184 197 285 ## 33 33 33 33 33 33 33 33 32 33 Based on this, all weight class have 33 wrestlers with only 32 wrestlers in 197 weigh class\n ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"b297bca87f9bb2980877e0437581cc6d","permalink":"/achalneupane.github.io/post/data_tables/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/data_tables/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Data tables","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  Exercise 1 Implement Cohen’s \\(d\\) as a function of\n\\[ d = f(m_1, s_1, m_2, s_2) = \\frac{|m_1-m_2|}{s_{pooled}} \\] where \\(s_{pooled}\\) is a pooled standard deviation. Use the formula \\(s_{pooled} = \\sqrt{(s_1^2 + s_2^2)/2}\\). You may implement pooled standard deviation as a function as well.\nCalculate the effect size \\(d\\) for the differences among calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006. Use the values from Wansink, Table 1 as given in Homework 1 or in the course outline. Name this function cohen.d (or similar if using SAS)\nAnswer Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections\n# function definition # Variables m1 and m2 are means, and s1 and s2 are standard deviations # for two dates of comparison among calories per serving we are interested in, respectively. cohen.d \u0026lt;- function(m1,s1,m2,s2){ cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) return(cohens_d) } 1936 versus 2006 m1=268.1 m2=384.4 s1=124.8 s2=168.3 cohen.d(m1=m1,s1=s1,m2=m2,s2=s2) ## [1] 0.7849876  1936 versus 1997 m1=268.1 m2=288.6 s1=124.8 s2=122.0 cohen.d(m1=m1,s1=s1,m2=m2,s2=s2) ## [1] 0.1661157  1997 versus 2006 m1=288.6 m2=384.4 s1=122.0 s2=168.3 cohen.d(m1=m1,s1=s1,m2=m2,s2=s2) ## [1] 0.6517694 Check your work by comparing with the previous homework. -Answers match with previous homework!\n   Exercise 2. Implement the required replicates calculation as a function of \\(m_1\\), \\(s_1\\), \\(m_2\\) and \\(s_2\\) as required parameters, and \\(\\alpha\\) and \\(\\beta\\) as optional parameters. Let alpha=0.05 and beta=0.2, so you’ll need to compute quantiles for 1-alpha/2 and 1-beta.\nYour function should return an integer \\(n\\), such that\n\\[ n \\ge 2\\times \\left( \\frac{CV}{\\%Diff} \\right)^2 \\times \\left(z_{\\alpha/2}+ z_\\beta \\right)^2 \\] where \\(\\%Diff = \\frac{m_1 - m_2}{(m_1 + m_2)/2}\\) and \\(CV = \\frac{sd_{pooled}}{(m_1 + m_2)/2}\\).\nYou may use the pooled standarad deviation function from Ex. 1 (if you defined such a function).\nName this function required.replicates (or similar if using SAS)\nAnswer Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections\n# function definition # Variables m1 and m2 are means, and s1 and s2 are standard deviations # for two dates of comparison among calories per serving we are interested in, respectively. required.replicates \u0026lt;- function (m1,m2, s1,s2, alpha=0.05, beta=0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) return(round(n,0)) } 1936 versus 2006 m1=268.1 m2=384.4 s1=124.8 s2=168.3 required.replicates(m1=m1, m2=m2, s1=s1, s2=s2) ## [1] 25  1936 versus 1997 m1=268.1 m2=288.6 s1=124.8 s2=122.0 required.replicates(m1=m1, m2=m2, s1=s1, s2=s2) ## [1] 569  1997 versus 2006 m1=288.6 m2=384.4 s1=122.0 s2=168.3 required.replicates(m1=m1, m2=m2, s1=s1, s2=s2) ## [1] 37 Check your work by comparing with the previous homework. -Answers match with previous homework!\nNote: for Alpha=0.05 , we can use the r function qnorm(1-alpha/2) assuming u=0 and sd=1, As for Beta, we need additional information. z-score is the a standardized value of the value the hypothesized x. and alpha is about rejecting the value x when its true. but beta is about x failing to reject in when it is false… which means there is other value of x which we don’t have in the formula z=(x-u)/sd.\n   Exercise 3 Implement the likelihood formula as a function or macro.\n\\[ L (x ; \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}^{}} e^{- \\frac{(x - \\mu)^2}{2 \\sigma^2}} \\]\nDefine \\(\\mu\\) and \\(\\sigma\\) as optional parameters, taking values mu=0 and sigma=1. Name this function norm.pdf\nAnswer Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections\n# function definition # Function to calcuate the values for log liklihood from above equation. # First, we define the values for sigma as variance, # mu as mean of a normal population to be used for a liklihood of a x observation. norm.pdf \u0026lt;- function(x,mu=0,sigma=1){ l\u0026lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2)) return(l) } \\(x=-0.1\\) x=-0.1 norm.pdf(x) ## [1] 0.3969525  \\(x=0.0\\) x=0.0 norm.pdf(x) ## [1] 0.3989423  \\(x=0.1\\) x=0.1 norm.pdf(x) ## [1] 0.3969525 Check your work by comparing with the previous homework. -Answers match with previous homework!\n   Exercise 4 The probability mass function for value \\(y\\) from Poisson data with a mean and variance \\(\\lambda\\) is given by\n\\[ f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} = exp(-\\lambda)(\\frac{1}{x!}) exp[x\\times log(\\lambda)] \\] Write a function pois.pmf that accepts two parameters, x and lambda. Use the built in factorial function for \\(x!\\). Note that \\(x\\) should be an integer value, so call a rounding function inside your function. Test your function with \\(\\lambda = 12\\) at \\(x = 8,12,16\\)\nAnswer Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections\n# function definition # The function to calculate probability mass function for poisson # data with a mean and variance lambda. pois.pmf \u0026lt;- function(x, lambda){ poisson.d \u0026lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda))) return(poisson.d) } \\(x=4\\) lambda=12 x=4 pois.pmf(x=x, lambda = lambda) ## [1] 0.005308599  \\(x=12\\) lambda=12 x=12 pois.pmf(x=x, lambda = lambda) ## [1] 0.1143679  \\(x=20\\) lambda=12 x=20 pois.pmf(x=x, lambda = lambda) ## [1] 0.009682032 You can check your work against the built in Poisson distribution functions.\n# Using built-in function \u0026#39;dpois\u0026#39;, we can check our answers: # for x=4 x= 4 lambda=12 dpois(x=x, lambda = lambda) ## [1] 0.005308599 # #or # ppois(x,lambda)-ppois(x-1,lambda) # for x =12 x= 12 lambda=12 dpois(x=x, lambda = lambda) ## [1] 0.1143679 # for x=20 x= 20 lambda=12 dpois(x=x, lambda = lambda) ## [1] 0.009682032 # which was correct for all three x\u0026#39;s Something to ponder. Note that there are two formula given. Can you implement both forms in R/IML/Macro language? Would there be a difference in computational speed or efficiency?\n# Implementation of the first formula pois.pmf.first \u0026lt;- function (x, lambda){ poisson.d \u0026lt;- (exp(-lambda)*((lambda^(round(x,0))))/(factorial(round(x,0)))) return(poisson.d) } # To test the execution time of two formulas: library(microbenchmark) lambda =12 x=20 mbm \u0026lt;- microbenchmark(\u0026quot;Using first formula\u0026quot; = pois.pmf.first(x=x, lambda = lambda), \u0026quot;Using second formula\u0026quot; = pois.pmf(x=x, lambda = lambda)) mbm ## Unit: microseconds ## expr min lq mean median uq max neval ## Using first formula 2.478 2.5830 66.55456 2.6430 2.6945 6367.856 100 ## Using second formula 2.687 2.7335 2.84033 2.7675 2.8175 6.687 100 ## cld ## a ## a library(ggplot2) autoplot(mbm) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. Based on the execution time, looks like using the first formula takes a bit longer time to execute.\n   Exercise 5 Write a function, stat.power that combines calculations from Exercises 1 and 2. This function should accept \\(m_1, s_1, m_2\\) and \\(s_2\\) as required parameters, and \\(\\alpha\\) and \\(\\beta\\) as optional parameters. This function must return a list with named elements CV, PercentDiff, EffectSize and RequiredReplicates.\nIf you choose to do this exercise in SAS, you will need to write a subroutine that accepts the same parameters as the R function, but also accepts output parameters CV, PercentDiff, EffectSize and RequiredReplicates. See https://blogs.sas.com/content/iml/2012/08/20/how-to-return-multiple-values-from-a-sasiml-function.html.\nAnother option for SAS is to package the calculations in a macro and create a data table, using the code from Course Outline SAS Source (under Course Outline \u0026gt; Outline Source and Output Files), about line 320.\nAnswer Define your function(s) in the code chunk below, the call the function with appropriate parameters in the following sections\n# function definition combined \u0026lt;- function (m1,m2, s1,s2, alpha=0.05, beta=0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) cv \u0026lt;- (sqrt((s1^2+s2^2)/2))/((m1+m2)/2) percentdiff \u0026lt;- ((m1-m2)/((m1+m2)/2)) tt \u0026lt;- (list(CV=cv, PercentDiff= percentdiff, RequiredReplicates=round(n,0), EffectSize=cohens_d)) # attributes(tt) # names(tt) attr(tt, \u0026quot;class\u0026quot;) \u0026lt;- \u0026quot;stat.power\u0026quot; #Setting a new class print.stat.power(tt) #use print.stat.power function below } If you define the class of the list returned by your function as stat.power, this function should work automatically; you shouln’t need to call the function explicity.\nprint.stat.power \u0026lt;- function(value) { cat(paste(\u0026quot;Coefficient of Variation :\u0026quot;,value$CV*100,\u0026quot;\\n\u0026quot;)) cat(paste(\u0026quot;Percent Difference :\u0026quot;,value$PercentDiff*100,\u0026quot;\\n\u0026quot;)) cat(paste(\u0026quot;Effect Size :\u0026quot;,value$EffectSize,\u0026quot;\\n\u0026quot;)) cat(paste(\u0026quot;Required Replicates :\u0026quot;,value$RequiredReplicates,\u0026quot;\\n\u0026quot;)) } 1936 versus 2006 m1=268.1 m2=384.4 s1=124.8 s2=168.3 combined(m1=m1, m2=m2, s1=s1, s2=s2) ## Coefficient of Variation : 45.4115573274988 ## Percent Difference : -35.647509578544 ## Effect Size : 0.784987603958648 ## Required Replicates : 25  1936 versus 1997 m1=268.1 m2=288.6 s1=124.8 s2=122.0 combined(m1=m1, m2=m2, s1=s1, s2=s2) ## Coefficient of Variation : 44.3355277160504 ## Percent Difference : -7.36482845338602 ## Effect Size : 0.166115727787307 ## Required Replicates : 569  1997 versus 2006 m1=288.6 m2=384.4 s1=122.0 s2=168.3 combined(m1=m1, m2=m2, s1=s1, s2=s2) ## Coefficient of Variation : 43.6803881088188 ## Percent Difference : -28.4695393759287 ## Effect Size : 0.651769377712577 ## Required Replicates : 37    ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"4b8c96ea118dd80f9711c70149c75a58","permalink":"/achalneupane.github.io/post/functions/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/functions/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Functions","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  Instructions This document runs a simple analysis of the Table 1 from [@wansink2009joy]. Edit the header information to show your name and the date you complete the assignment.\nModify this document to analyze either Calories per Serving or Servings per Recipe. Document any changes you make in the literate portion of the file. Comment on your choice of measure to analyze.\nChange the name of this file to match your user name on D2L, keeping the ‘Rmd’ extension, and include week number in the title (for example, Peter.Claussen.1.Rmd). Upload this file to D2L. Typeset this file to Word or PDF and upload the result to D2L as well.\nData  Mean and (SD) for selected recipes from “Joy of Cooking”    Measure 1936 1946 1951 1963 1975 1997 2006    calories per recipe (SD) 2123.8 (1050.0) 2122.3 (1002.3) 2089.9 (1009.6) 2250.0 (1078.6) 2234.2 (1089.2) 2249.6 (1094.8) 3051.9 (1496.2)  calories per serving (SD) 268.1 (124.8) 271.1 (124.2) 280.9 (116.2) 294.7 (117.7) 285.6 (118.3) 288.6 (122.0) 384.4 (168.3)  servings per recipe (SD) 12.9 (13.3) 12.9 (13.3) 13.0 (14.5) 12.7 (14.6) 12.4 (14.3) 12.4 (14.3) 12.7 (13.0)      Analysis Enter data CookingTooMuch.dat \u0026lt;- data.frame( Year=c(1936, 1946, 1951, 1963, 1975, 1997, 2006), CaloriesPerRecipeMean = c(2123.8, 2122.3, 2089.9, 2250.0, 2234.2, 2249.6, 3051.9), CaloriesPerRecipeSD = c(1050.0, 1002.3, 1009.6, 1078.6, 1089.2, 1094.8, 1496.2), CaloriesPerServingMean = c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4), CaloriesPerServingSD = c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3), ServingsPerRecipeMean = c(12.9, 12.9, 13.0, 12.7, 12.4, 12.4, 12.7), ServingsPerRecipeSD = c(13.3, 13.3, 14.5, 14.6, 14.3, 14.3, 13.0) )  Create values for confidence interval plot Wansink reports that 18 recipes were analyzed.\nn \u0026lt;- 18 Assume a significance level \\(\\alpha\\) of 5%.\nalpha \u0026lt;- 0.05 Use standard formula for standard error \\(\\sigma / \\sqrt{n}\\) and confidence interval \\(t_{\\alpha/2} \\times s.e.\\).\nStandardError \u0026lt;- function(sigma, n) { sigma/sqrt(n) } ConfidenceInterval \u0026lt;- function(sigma, n) { qt(1-alpha/2, Inf)*StandardError(sigma,n) } Create a variable for plotting and calculate upper and lower bounds using confidence intervals. For this assignment, I am plotting ServingsPerRecipe.\nPlotCookingTooMuch.dat \u0026lt;- CookingTooMuch.dat PlotCookingTooMuch.dat$ServingsPerRecipe \u0026lt;- PlotCookingTooMuch.dat$ServingsPerRecipeMean PlotCookingTooMuch.dat$Lower \u0026lt;- PlotCookingTooMuch.dat$ServingsPerRecipe - ConfidenceInterval(CookingTooMuch.dat$ServingsPerRecipeSD, n) PlotCookingTooMuch.dat$Upper \u0026lt;- PlotCookingTooMuch.dat$ServingsPerRecipe + ConfidenceInterval(CookingTooMuch.dat$ServingsPerRecipeSD, n) Here, we are selecting only ServingsPerRecipe variable for plotting.\nPlotCookingTooMuch.dat \u0026lt;- PlotCookingTooMuch.dat[, c(\u0026quot;Year\u0026quot;, \u0026quot;ServingsPerRecipe\u0026quot;, \u0026quot;Lower\u0026quot;, \u0026quot;Upper\u0026quot;)] Examine the values to make sure we’ve entered correctly.\nprint(PlotCookingTooMuch.dat) ## Year ServingsPerRecipe Lower Upper ## 1 1936 12.9 6.755826 19.04417 ## 2 1946 12.9 6.755826 19.04417 ## 3 1951 13.0 6.301465 19.69854 ## 4 1963 12.7 5.955268 19.44473 ## 5 1975 12.4 5.793858 19.00614 ## 6 1997 12.4 5.793858 19.00614 ## 7 2006 12.7 6.694417 18.70558 Couldn’t find the confidence intervals for ServingsPerRecipe in Wanskins report for 1936 and 2006. So I am using the Reference CI as calculated CI using confidence interval function above. So here, I am using ComValues as ReferneceValues\nCompValues \u0026lt;- PlotCookingTooMuch.dat[c(1, 7), c(\u0026quot;Lower\u0026quot;, \u0026quot;Upper\u0026quot;)] #ReferenceValues \u0026lt;- matrix(c(1638.7, 2608.9, 2360.7, 3743.1),nrow=2,byrow=TRUE) ReferenceValues \u0026lt;- CompValues CompValues ## Lower Upper ## 1 6.755826 19.04417 ## 7 6.694417 18.70558 ReferenceValues ## Lower Upper ## 1 6.755826 19.04417 ## 7 6.694417 18.70558 any(abs(CompValues - ReferenceValues) \u0026gt; 0.1) ## [1] FALSE We no longer need the original data.\nCookingTooMuch.dat \u0026lt;- NULL   Plot the table #Here, changed the title and y and x labels to Servings Per Recipe\nplot( ServingsPerRecipe ~ Year, data = PlotCookingTooMuch.dat, col = \u0026quot;blue\u0026quot;, pch = 19, main = \u0026quot;Servings per Recipe\u0026quot;, ylab = \u0026quot;Servings\u0026quot;, ylim = c( min(PlotCookingTooMuch.dat$Lower), max(PlotCookingTooMuch.dat$Upper) ) ) lines( ServingsPerRecipe ~ Year, data = PlotCookingTooMuch.dat, lty = \u0026quot;dashed\u0026quot;, col = \u0026quot;blue\u0026quot;, lend = 2 ) segments( x0 = PlotCookingTooMuch.dat$Year, y0 = PlotCookingTooMuch.dat$Lower, x1 = PlotCookingTooMuch.dat$Year, y1 = PlotCookingTooMuch.dat$Upper )  Comments From this plot, it appears that average servings per recipe doesn’t seem to change from 1936 to 2006.\n References  ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"2ec3ea8b6d6ae0cde3ae81e07e5a2feb","permalink":"/achalneupane.github.io/post/getting_started/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/getting_started/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Getting Started","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  There are six exercises below. You are required to provide solutions for at least four of the six. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve five of the sixth problems, mixing the languages as you wish.\nIf you choose SAS for an exercise, you may use IML, DATA operations or PROC SQL at your discretion.\nWarning I will continue restricting the use of external libraries in R, particularly tidyverse libraries. You may choose to use ggplot2, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the midterm and final projects.\nReuse For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here.\n# Functions: # Ex 1 collapse_df \u0026lt;- function(x) { createPattern \u0026lt;- function(n) { sprintf(\u0026quot;(%s[^,]+),\u0026quot;, strrep(\u0026quot;[^,]+,\u0026quot;, n - 1)) } mystring \u0026lt;- do.call(paste, c(as.list(colnames(x)), sep = \u0026quot;,\u0026quot;, do.call(paste, c(x, sep = \u0026quot;,\u0026quot;)))) my_pattern \u0026lt;- createPattern(ncol(x)) gsub(my_pattern, \u0026quot;\\\\1\\n\u0026quot;, paste(mystring, collapse = \u0026quot;,\u0026quot;)) } # Exercise 4 reformat \u0026lt;- function(x) { x \u0026lt;- unlist(strsplit(x, split = \u0026#39;|\u0026#39;, fixed = TRUE)) x \u0026lt;- gsub(\u0026quot; \u0026quot;, \u0026quot;\u0026quot;, x)[-1] return(as.numeric(x)) } # Exercise 5 # Function to calculate R-square lm_eqn \u0026lt;- function(df) { m \u0026lt;- lm(Weight2019 ~ Weight2015, df) eq \u0026lt;- substitute( italic(Weight2019) == a + b %.% italic(Weight2015) * \u0026quot;,\u0026quot; ~ ~ italic(r) ^ 2 ~ \u0026quot;=\u0026quot; ~ r2, list( a = format(unname(coef(m)[1]), digits = 2), b = format(unname(coef(m)[2]), digits = 2), r2 = format(summary(m)$r.squared, digits = 3) ) ) as.character(as.expression(eq)) }  Exercise 1. Write a loop or a function to convert a matrix to a CSV compatible string. Given a matrix of the form\n  C1 C2 C3    a b c  d e f  g h i    produce a string of the form\na,b,c\\nd,e,f\\ng,h,i\nwhere \\n is the newline character.\nmy.dat \u0026lt;- setNames(as.data.frame(matrix( letters[1:9], ncol = 3, byrow = TRUE )), c(\u0026quot;C1\u0026quot;, \u0026quot;C2\u0026quot;, \u0026quot;C3\u0026quot;)) collapse_df(my.dat) ## [1] \u0026quot;C1,C2,C3\\na,b,c\\nd,e,f\\ng,h,i\u0026quot; You are only required to convert a matrix to CSV format, but you may choose to write code to convert data tables to CSV; in this case, include column names in the output string. Use NATR332.DAT as a test case.\nNATR332.DAT \u0026lt;- data.frame( Y1 = c(146,141,135,142,140,143,138,137,142,136), Y2 = c(141,143,139,139,140,141,138,140,142,138) ) # Test our function collapse_df(NATR332.DAT) ## [1] \u0026quot;Y1,Y2\\n146,141\\n141,143\\n135,139\\n142,139\\n140,140\\n143,141\\n138,138\\n137,140\\n142,142\\n136,138\u0026quot; If you choose SAS, I’ve include the NATR332 data table and framework code for IML in the template. I used the CATX function in IML. I found I could do this in one line in R, with judicious use of apply, but I haven’t found the equivalent in IML. Instead, I used a pair of nested loops to “accumulate” an increasingly longer string.\n Exercise 2. Create an ordered treatment pairs table from the pumpkin data, as described in Homework 7. Before printing the table, iterate over each row to create a vector of row names that are more descriptive. First, use levels to get the text associated with each Class, then combine the Class text to create a row name of the form:\nBlue vs Cinderella\n(where Blue is the Class description for class 1, Cinderella is the description for class 2. This text should be the row name in the row corresponding to \\(i=1\\) and \\(j=2\\)). You may choose to add a column with the specified descriptions, if you wish, but this must be the first column of the printed table.\npumpkins.dat = read.table( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; ) # There are 4 classes, so 4 choose 2: class.level \u0026lt;- levels(pumpkins.dat$Class) comb.matrix \u0026lt;- setNames(as.data.frame(t(combn(4,2))), c(\u0026quot;CLASSi\u0026quot;, \u0026quot;CLASSj\u0026quot;)) # Now we loop over the class level indices to get the row names for (i in 1:nrow(comb.matrix)) { rownames(comb.matrix)[i] \u0026lt;- paste(class.level[comb.matrix$CLASSi[i]], class.level[comb.matrix$CLASSj[i]], sep = \u0026quot; Vs \u0026quot;) } # This is how the combination matrix would look like comb.matrix ## CLASSi CLASSj ## Blue Vs Cinderella 1 2 ## Blue Vs Howden 1 3 ## Blue Vs Pie 1 4 ## Cinderella Vs Howden 2 3 ## Cinderella Vs Pie 2 4 ## Howden Vs Pie 3 4 # In addition to this, if we also want to see actual class levels on the same matrix: for (i in 1:length(comb.matrix$V1)) { comb.matrix$CLASSi.Names[i] \u0026lt;- class.level[comb.matrix$CLASSi[i]] comb.matrix$col2j.Names[i] \u0026lt;- class.level[comb.matrix$CLASSj[i]] } comb.matrix ## CLASSi CLASSj CLASSi.Names col2j.Names ## Blue Vs Cinderella 1 2 Blue Cinderella ## Blue Vs Howden 1 3 Blue Cinderella ## Blue Vs Pie 1 4 Blue Cinderella ## Cinderella Vs Howden 2 3 Blue Cinderella ## Cinderella Vs Pie 2 4 Blue Cinderella ## Howden Vs Pie 3 4 Blue Cinderella  Exercise 3. Calculate MSW, MSB, \\(F\\) and \\(p\\) for the data from Wansink Table 1 (Homework 4, Exercise 6) where\n\\[ MSB = \\frac{\\sum_i n_i(x_i-\\bar{x})^2}{k-1} \\]\n\\[ MSW = \\frac{\\sum_i (n_i-1)s_i^2}{N-k} \\]\nStart with the strings:\nMeans \u0026lt;- \u0026quot;268.1 271.1 280.9 294.7 285.6 288.6 384.4\u0026quot; StandardDeviations \u0026lt;- \u0026quot;124.8 124.2 116.2 117.7 118.3 122.0 168.3\u0026quot; SampleSizes \u0026lt;- \u0026quot;18 18 18 18 18 18 18\u0026quot; Tokenize the strings, then convert the tokens to a create vectors of numeric values. Use these vectors to compute and print \\(MSW\\), \\(MSB\\), \\(F\\) and \\(p\\).\nMeans\u0026lt;-strsplit(Means, split = \u0026quot; \u0026quot;) x\u0026lt;-as.numeric(Means[[1]]) Sd\u0026lt;- strsplit(StandardDeviations, split = \u0026quot; \u0026quot;) s\u0026lt;-as.numeric(Sd[[1]]) n\u0026lt;-strsplit(SampleSizes, split= \u0026quot; \u0026quot;) n\u0026lt;-as.numeric(n[[1]]) k\u0026lt;-length(n) N\u0026lt;-18*k ## MSB iteration sum.x=0 for (i in 1:k) { sum.x=sum.x + x[i] } x_mean=sum.x/k ss.x=0 for (i in 1:k){ ss.x=ss.x + n[i]*(x[i] - x_mean)^2 } MSB=ss.x/(k-1) MSB ## [1] 28815.96 ## MSW iteration ss.w=0 for (i in 1:k){ ss.w=ss.w + ((n[i]-1)*(s[i])^2) } MSW=ss.w/(N-k) MSW ## [1] 16508.6 F_ratio = MSB/MSW F_ratio ## [1] 1.745512 P_value = pf(F_ratio, df1=k-1, df2=N-k, lower.tail=FALSE) P_value ## [1] 0.1163133 If you use SAS, I’ve provided macro variables that can be tokenized in either macro language or using SAS functions. You can mix and match macro, DATA, IML or SQL processing as you wish, but you must write code to convert the text into numeric tokens before processing.\nCompare your results from previous homework, or to the resource given in previous homework, to confirm that the text was correctly converted to numeric values.\n Exercise 4. Repeat the regression analysis from Homework 4, Exercise 5, but start with the text\nRate \u0026lt;- \u0026quot;Rate | 23000 | 24000 | 25000 | 26000 | 27000 | 28000 | 29000\u0026quot; Yield \u0026lt;- \u0026quot;Yield | 111.4216 | 155.0326 | 181.1176 | 227.5800 | 233.4623 | 242.1753 | 231.3890\u0026quot; Rate \u0026lt;- reformat(Rate) Yield \u0026lt;- reformat(Yield) Note that by default, strsplit in R will read split as a regular expression, and | is a special character in regular expressions. You will need to change one of the default parameters for this exercise.\nTokenize these strings and convert to numeric vectors, then use these vectors to define\n\\[ y =\\left( \\begin{array}{c} 111.4216 \\\\ 155.0326 \\\\ \\vdots \\\\ 231.3890 \\end{array}\\right) = \\left(\\begin{array}{rr} 1 \u0026amp; 23000 \\\\ 1 \u0026amp; 24000 \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; 29000 \\end{array}\\right) \\left(\\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\end{array}\\right)^t = \\mathbf{X} \\mathbf{\\beta} \\]\nSolve for and print \\(\\hat{\\beta}\\).\nIf you use SAS, I’ve provided macro variables that can be tokenized in either macro language or using SAS functions. You can mix and match macro, DATA, IML or SQL processing as you wish, but you must write code to convert the text into numeric tokens before processing.\ny \u0026lt;- as.vector( Yield) y ## [1] 111.4216 155.0326 181.1176 227.5800 233.4623 242.1753 231.3890 #creating a matrix for bias term bias=rep(1:1, length.out=length(y)) bias ## [1] 1 1 1 1 1 1 1 cx \u0026lt;- Rate X=matrix(c(bias,cx), ncol = 2) X ## [,1] [,2] ## [1,] 1 23000 ## [2,] 1 24000 ## [3,] 1 25000 ## [4,] 1 26000 ## [5,] 1 27000 ## [6,] 1 28000 ## [7,] 1 29000 #multplication of transpose of x and x tX=t(X) tX ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 1 1 1 1 1 1 ## [2,] 23000 24000 25000 26000 27000 28000 29000 Xm=tX%*%X Xm ## [,1] [,2] ## [1,] 7 1.82e+05 ## [2,] 182000 4.76e+09 A=solve(Xm) hat.beta=A%*%(tX%*%y) hat.beta ## [,1] ## [1,] -347.18307857 ## [2,] 0.02094758 Compare your results from previous homework, or to the resource given in previous homework, to confirm that the text was correctly converted to numeric values.\n Exercise 5. Use the file openmat2015.csv from D2L. These data are from https://news.theopenmat.com/high-school-wrestling/high-school-wrestling-rankings/final-2015-clinch-gear-national-high-school-wrestling-individual-rankings/57136. This is a list of top-ranked high school wrestlers in 2015, their high School, Weight class and in some cases the College where they expected to enroll and compete after high school.\nWe wish to know how many went on to compete in the national championship in 2019, so we will merge this table with the data from Homework 7, ncaa2019.csv. The openmat2015.csv data contains only a single column, Name. You will need to split the text in this column to create the columns First and Last required to merge with ncaa2019.csv.\nDo not print these tables in the submitted work Instead, print a contingency table comparing Weight for 2015 and Weight for 2019. What is the relationship between high school and college weight classes? You may instead produce a scatter plot or box-whisker plot, using high school weight class as the independent variable.\nIf you do this in SAS, use the openmat2015SAS.csv file, it will import College correctly.\nopenmat2015 \u0026lt;- read.delim2( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/openmat2015.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; ) openmat2015$Weight2015 \u0026lt;- openmat2015$Weight openmat2015$First \u0026lt;- sapply(strsplit(as.character(openmat2015$Name),\u0026#39; \u0026#39;), \u0026quot;[\u0026quot;, 1) # sapply(strsplit(as.character(openmat2015$Name),\u0026#39; \u0026#39;), function(x){x[1]}) openmat2015$Last \u0026lt;- sapply(strsplit(as.character(openmat2015$Name),\u0026#39; \u0026#39;), \u0026quot;[\u0026quot;, 2) # sapply(strsplit(as.character(openmat2015$Name),\u0026#39; \u0026#39;), function(x){x[2]}) head(openmat2015) ## Weight Rank Name Year School State College ## 1 106 1 Cade Olivas Fr. St. John Bosco CA ## 2 106 2 Roman Bravo-Young Fr. Sunnyside AZ ## 3 106 3 Gavin Teasdale Fr. Jefferson-Morgan PA ## 4 106 4 Drew Mattin So. Delta OH ## 5 106 5 Real Woods Fr. Montini Catholic IL ## 6 106 6 Jacori Teemer Fr. Long Beach NY ## Previous Weight2015 First Last ## 1 1 106 Cade Olivas ## 2 2 106 Roman Bravo-Young ## 3 3 106 Gavin Teasdale ## 4 5 106 Drew Mattin ## 5 6 106 Real Woods ## 6 7 106 Jacori Teemer ncaa2019.dat = read.table( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2019.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; ) ncaa2019.dat$Weight2019 \u0026lt;- ncaa2019.dat$Weight head(ncaa2019.dat) ## Weight Last First Finish Weight2019 ## 1 125 Lee Spencer 1 125 ## 2 125 Mueller Jack 2 125 ## 3 125 Rivera Sebastian 3 125 ## 4 125 Arujau Vitali 4 125 ## 5 125 Piccininni Nicholas 5 125 ## 6 125 Glory Pat 6 125 merged.dat \u0026lt;- merge(x = openmat2015, y = ncaa2019.dat, by = c(\u0026quot;First\u0026quot;, \u0026quot;Last\u0026quot;), all = TRUE) head(merged.dat) ## First Last Weight.x Rank Name Year School State ## 1 A.C. Headlee 132 3 A.C. Headlee Sr. Waynesburg PA ## 2 Aaron Burkett 106 18 Aaron Burkett Jr. Chesnut Ridge PA ## 3 Adam Hudson 160 17 Adam Hudson Sr. Shelbyville IL ## 4 AJ Nevills NA NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 Al Beattie 285 15 Al Beattie Sr. Burrell PA ## 6 Alan Hart 113 20 Alan Hart So. Edward OH ## College Previous Weight2015 Weight.y Finish Weight2019 ## 1 North Carolina 4 132 NA \u0026lt;NA\u0026gt; NA ## 2 NR 106 NA \u0026lt;NA\u0026gt; NA ## 3 NR 160 NA \u0026lt;NA\u0026gt; NA ## 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA 285 cons 32 285 ## 5 Pitt Johnstown 18 285 NA \u0026lt;NA\u0026gt; NA ## 6 17 113 NA \u0026lt;NA\u0026gt; NA merged.dat \u0026lt;- merged.dat[!is.na(merged.dat$Weight2015) \u0026amp; !is.na(merged.dat$Weight2019),] weight_contingency \u0026lt;- table( merged.dat$Weight2015, merged.dat$Weight2019, dnn = c(\u0026quot;Weight for 2015\u0026quot;, \u0026quot;Weight for 2019\u0026quot;) ) weight_contingency ## Weight for 2019 ## Weight for 2015 125 133 141 149 157 165 174 184 197 285 ## 106 2 1 0 0 1 0 0 0 0 0 ## 113 1 1 1 0 0 0 0 0 0 0 ## 120 5 3 1 1 0 0 0 0 0 0 ## 126 0 1 1 0 0 0 0 0 0 0 ## 132 2 2 4 1 0 0 0 0 0 0 ## 138 0 0 2 1 1 0 0 0 0 0 ## 145 0 0 1 1 4 0 0 0 0 0 ## 152 0 0 0 1 1 2 2 0 0 0 ## 160 0 0 0 0 0 4 4 0 0 0 ## 170 0 0 0 0 0 2 2 2 0 0 ## 182 0 0 0 0 0 0 2 2 3 1 ## 195 0 0 0 0 0 0 0 2 1 1 ## 220 0 0 0 0 0 0 0 0 0 5 ## 285 0 0 0 0 0 0 0 0 0 1 library(ggplot2) p \u0026lt;- ggplot(merged.dat, aes(x=Weight2015, y=Weight2019)) + geom_point( size=1, shape=21, fill=\u0026quot;white\u0026quot;) # geom_abline() # Now we paste the R-square value to our relationship plot p + geom_text(x = 150, y = 300, label = lm_eqn(merged.dat), parse = TRUE) + xlab(\u0026quot;HighSchool Weight\u0026quot;) + ylab(\u0026quot;College Weight\u0026quot;) + geom_smooth(method = \u0026#39;lm\u0026#39;) The relationship between highschool weight and college weight is almost linear. # Exercise 6\nUse file openmat2015.csv from Exercise 6, and use partial text matching to answer these questions. To show your results, print only the rows from the table that match the described text patterns, but to save space, print only Name, School and College. Each of these can be answered in a single step.\nopenmat2015 \u0026lt;- read.delim2( \u0026quot;https://raw.githubusercontent.com/achalneupane/data/master/openmat2015.csv\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; ) # head(openmat2015)  Which wrestlers come from a School with a name starting with St.?  openmat2015[grepl(\u0026quot;St.\u0026quot;, openmat2015$School), c(\u0026quot;Name\u0026quot;, \u0026quot;School\u0026quot;, \u0026quot;College\u0026quot;)] ## Name School College ## 1 Cade Olivas St. John Bosco ## 17 John Tropea St. Joseph Montvale ## 30 Mitch Moore St. Paris Graham ## 37 Joey Prata St. Christopher\u0026#39;s ## 50 Eli Stickley St. Paris Graham Wisconsin ## 64 Mitchell McKee St. Michael-Albertville Minnesota \u0026#39;16 ## 67 Eli Seipel St. Paris Graham Pittsburgh ## 76 Ben Lamantia St. Anthonys Michigan ## 82 Kaid Brock Stillwater Oklahoma State ## 94 Austin O\u0026#39;Connor St. Rita ## 99 Hunter Ladnier St. Edward ## 128 Brent Moore St. Paris Graham ## 134 Tristan Moran Stillwater Oklahoma State ## 153 Kyle Lawson St. Paris Graham ## 161 Alex Marinelli St. Paris Graham Iowa \u0026#39;16 ## 182 Anthony Valencia St. John Bosco Arizona State ## 183 Logan Massa St. Johns Michigan ## 185 Joe Smith Stillwater Oklahoma State ## 201 Zahid Valencia St. John Bosco Arizona State ## 217 Jordan Joseph St. Michael-Albertville ## 251 Christian Colucci St. Peter\u0026#39;s Prep Lehigh ## 255 Ian Butterbrodt St. Johns Prep Brown  Which wrestlers were intending to attend an Iowa College?  openmat2015[grepl(\u0026quot;Iowa\u0026quot;, openmat2015$College), c(\u0026quot;Name\u0026quot;, \u0026quot;School\u0026quot;, \u0026quot;College\u0026quot;)] ## Name School College ## 21 Justin Mejia Clovis Iowa \u0026#39;17 ## 24 Jason Renteria Oak Park-River Forest Iowa \u0026#39;17 ## 65 Markus Simmons Broken Arrow Iowa State ## 121 Michael Kemerer Franklin Regional Iowa ## 122 Max Thomsen Union Community Northern Iowa ## 155 Kaleb Young Punxsutawney Iowa \u0026#39;16 ## 161 Alex Marinelli St. Paris Graham Iowa \u0026#39;16 ## 166 Bryce Steiert Waverly-Shell Rock Northern Iowa ## 176 Paden Moore Jackson County Central Northern Iowa ## 194 Isaiah Patton Dowling Catholic Northern Iowa ## 196 Jacob Holschlag Union Northern Iowa ## 197 Colston DiBlasi Park Hill Iowa State ## 204 Taylor Lujan Carrollton Northern Iowa ## 233 Cash Wilcke OA-BCIG Iowa ## 244 Ryan Parmely Maquoketa Valley Upper Iowa  Which wrestlers were intending to start College in 2016 or 2017 (College will end with 16 or 17)?  openmat2015[grepl(\u0026quot;16$|17$\u0026quot;, openmat2015$College), c(\u0026quot;Name\u0026quot;, \u0026quot;School\u0026quot;, \u0026quot;College\u0026quot;)] ## Name School College ## 21 Justin Mejia Clovis Iowa \u0026#39;17 ## 24 Jason Renteria Oak Park-River Forest Iowa \u0026#39;17 ## 45 Kyle Norstrem Brandon Virginia Tech \u0026#39;16 ## 46 Jack Mueller Wyoming Seminary Virginia \u0026#39;16 ## 51 Ty Agaisse Delbarton Princeton \u0026#39;16 ## 64 Mitchell McKee St. Michael-Albertville Minnesota \u0026#39;16 ## 126 Hayden Hidlay Mifflin County NC State \u0026#39;16 ## 145 Jake Wentzel South Park Pitt \u0026#39;16 ## 155 Kaleb Young Punxsutawney Iowa \u0026#39;16 ## 161 Alex Marinelli St. Paris Graham Iowa \u0026#39;16 ## 186 Nick Reenan Wyoming Seminary Northwestern \u0026#39;16  Which wrestlers are intending compete in a sport other than wrestling? (look for a sport in parenthesis in the College column. Note - ( is a special character in regular expressions, so to match the exact character, it needs to be preceded by the escape character \\. However, \\ in strings is a special character, so itself must be preceded by the escape character.  # all.sports.brackets \u0026lt;- # openmat2015[grepl(\u0026quot;\\\\(\u0026quot;, openmat2015$College), ] openmat2015[grepl(\u0026quot;\\\\(\u0026quot;, openmat2015$College), ][!grepl(\u0026quot;Wrestling\u0026quot;, openmat2015[grepl(\u0026quot;\\\\(\u0026quot;, openmat2015$College), ]$College, ignore.case = TRUE), c(\u0026quot;Name\u0026quot;, \u0026quot;School\u0026quot;, \u0026quot;College\u0026quot;)] ## Name School College ## 218 Chase Osborn Penn Minnesota (Baseball) ## 225 Tevis Barlett Cheyenne East Washington (FB) ## 230 Jan Johnson Governor Mifflin Akron(FB) ## 261 Michael Johnson Montini Catholic Yale (Football) ## 264 Gage Cervenka Emerald Clemson (Football) ## 267 Jake Marnin Southeast Polk Southern Illinois (Football) ## 277 Que Overton Jenks Oklahoma (Football) ## 279 Norman Oglesby Benjamin Davis Cincinnati (Football)  ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"d203e78bf9d0f86a9885c96e6bfd6757","permalink":"/achalneupane.github.io/post/processing_text/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/processing_text/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Processing text","type":"post"},{"authors":["**Achal Neupane**"],"categories":null,"content":"  Instructions There are four exercises below. All are optional. You may solve these, one solution per exercise, and submit them for extra credit. You use R, SAS or Python as you wish.\n Exercise 1. Consider the Hidalgo data set from Homework 9. Load the data, and calculate mean and median.\nhidalgo.dat = read.table(\u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/Week_9/hidalgo.dat\u0026quot;, header = T, sep = \u0026quot;,\u0026quot; ) mean.hidalgo \u0026lt;- mean(hidalgo.dat$X.060) mean.hidalgo ## [1] 0.08607851 median.hidalgo \u0026lt;- median(hidalgo.dat$X.060) median.hidalgo ## [1] 0.08 Part a. Calculate a jackknife estimate of the mean, and jackknife standard error of the mean, of these data. Are these values what you expect?\ny \u0026lt;- hidalgo.dat$X.060 n \u0026lt;- length(y) hat.theta.rep \u0026lt;- {} for(i in 1:n){ hat.theta.rep[i] = mean(y[-i]) } bar.theta \u0026lt;- mean(hat.theta.rep) # jackknife estimate of the mean bar.theta ## [1] 0.08607851 hat.theta \u0026lt;- mean.hidalgo # bias print(bias.jack \u0026lt;- (n-1) * (bar.theta - hat.theta)) ## [1] 0 #jackknife standard error for mean would be jackknife.stderr.mean \u0026lt;- sqrt((n-1) * mean((hat.theta.rep - mean(hat.theta.rep))^2)) jackknife.stderr.mean ## [1] 0.0006787497 # which can also be calculated as: variance.of.hat.theta.rep \u0026lt;- var(hat.theta.rep) jackknife.var = ((n-1)^2/n)*variance.of.hat.theta.rep jackknife.stderr.mean = sqrt(jackknife.var) jackknife.stderr.mean ## [1] 0.0006787497 Yes, I think these values are what we should expect.\n Part b. Calculate a jackknife estimate of the median, and jackknife standard error of the median, of these data. Are these values what you expect?\nprint(hat.theta \u0026lt;- median(y)) ## [1] 0.08  n \u0026lt;- length(y) hat.theta.rep \u0026lt;- {} for(i in 1:n){ hat.theta.rep[i] = median(y[-i]) } bar.theta \u0026lt;- mean(hat.theta.rep) bar.theta ## [1] 0.08  hat.theta \u0026lt;- median.hidalgo # bias print(bias.jack \u0026lt;- (n-1) * (bar.theta - hat.theta)) ## [1] 0  #jackknife standard error for median would be jackknife.stderr.median \u0026lt;- sqrt((n-1) * mean((hat.theta.rep - mean(hat.theta.rep))^2)) jackknife.stderr.median ## [1] 0  # # Also can be calculated as # variance.of.ests \u0026lt;- var(hat.theta.rep) # jackknife.var = ((n-1)^2/n)*variance.of.ests # jackknife.stderr = sqrt(jackknife.var) # jackknife.stderr # Or as: # n \u0026lt;-length(y) # theta \u0026lt;- median(y) # jk \u0026lt;- sapply(1:n, function(i) median(y[-i])) # # thetaBar \u0026lt;- mean(jk) # thetaBar # biasEst \u0026lt;- (n - 1) * (thetaBar - theta) # biasEst # seEst \u0026lt;- sqrt((n - 1) * mean((jk - thetaBar) ^ 2)) # seEst # # To check our answer, we can check with the bootstrap package # # library(bootstrap) # out \u0026lt;- jackknife(y, median) # out$jack.se # out$jack.bias  Part c. Calculate a bootstrap estimate of the mean, and of the median, of the Hidalgo data. Use \\(B = 1000\\) samples.\n B \u0026lt;- 1000 star.theta.rep \u0026lt;- rep(0,B) star.theta \u0026lt;- mean(y) for(i in 1:B) { star.theta.rep[i] \u0026lt;- mean(sample(y,replace=TRUE)) } # bootstrap estimate of the mean star.bar.theta \u0026lt;- mean (star.theta.rep) star.bar.theta ## [1] 0.08609793  # BS estimate of standard error for mean se.boot.mean \u0026lt;- sd(star.theta.rep - mean(star.theta.rep)) se.boot.mean ## [1] 0.0006879024  bstraps \u0026lt;- c() for (i in 1:1000) { bsample \u0026lt;- sample(y, length(y), replace=T) bstraps \u0026lt;- c(median(bsample), bstraps)} star.hat.theta \u0026lt;- median(bstraps) # bootstrap estimate of median star.hat.theta ## [1] 0.08  # BS estimate of standard error for mean se.boot.median \u0026lt;- sd(star.theta.rep - median(star.theta.rep)) se.boot.median ## [1] 0.0006879024  Part d. When data are normally distributed, and for large samples, the standard error of the median can be approximated by \\[ s.e._{med} = 1.253 \\times s.e._{mean} \\] where \\(s.e._{mean} = \\sigma /\\sqrt{n}\\).\nHow do the jackknife and bootstrap estimates of standard error compare to the parametric estimates?\nstd \u0026lt;- function(x) sd(x)/sqrt(length(x)) s.e.mean \u0026lt;- std(hidalgo.dat$X.060) s.e.mean ## [1] 0.0006787497 s.e.median \u0026lt;- s.e.mean * 1.253 s.e.median ## [1] 0.0008504733 # compared to jackknife.stderr.mean ## [1] 0.0006787497 jackknife.stderr.median ## [1] 0 se.boot.mean ## [1] 0.0006879024 se.boot.median ## [1] 0.0006879024   Exercise 2. Consider the data from Homework 6.\nNATR332.DAT \u0026lt;- data.frame( Y1 = c(146,141,135,142,140,143,138,137,142,136), Y2 = c(141,143,139,139,140,141,138,140,142,138) ) Let \\(\\theta\\) be the ratio of the two population means:\n\\[ \\theta = \\frac{\\mu_{Y1}}{\\mu_{Y2}} \\] Calculate jackknife and bootstrap estimates for \\(\\widehat{\\theta}\\), and for the standard error for \\(\\widehat{\\theta}\\).\nPart a. Jacknife.  n \u0026lt;- nrow(NATR332.DAT) y \u0026lt;- NATR332.DAT$Y1 z \u0026lt;- NATR332.DAT$Y2 theta.hat \u0026lt;- mean(y)/ mean(z) # computing jackknife replicates leaving one-out estimates theta.jack \u0026lt;- numeric(n) for( i in 1:n){ theta.jack[i] \u0026lt;- mean(y[-i])/mean(z[-1]) } bias \u0026lt;- (n-1) * (mean(theta.jack)- theta.hat) bias ## [1] 0.006423983  # now, we calculate standard error for jackknife se.jack \u0026lt;- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2)) se.jack ## [1] 0.007824608  Part b. Bootstrap. # For Bootstrap # let B = 1000 theta.b \u0026lt;- numeric(B) theta.hat \u0026lt;- mean(NATR332.DAT$Y1)/mean(NATR332.DAT$Y2) n \u0026lt;- nrow(NATR332.DAT) for(b in 1:B){ i \u0026lt;- sample (1:n, size = n, replace = TRUE) y \u0026lt;- NATR332.DAT$Y1[i] z \u0026lt;- NATR332.DAT$Y2[i] theta.b[b] \u0026lt;- mean(y)/mean(z) } bias \u0026lt;- mean(theta.b) - theta.hat se.boot \u0026lt;- sd(theta.b) print(list(est = theta.hat, bias = bias, cv = bias/se.boot, standard_error_bootstrap = se.boot)) ## $est ## [1] 0.9992862 ## ## $bias ## [1] 7.696709e-05 ## ## $cv ## [1] 0.012534 ## ## $standard_error_bootstrap ## [1] 0.006140666   Exercise 3 Part a. Consider the ELO data. Subset the data to exclude non-qualifiers - NQ - then create a factor AA. This will indicate if the wrestler that as All-American (top 8 places), or did not place in the tournament. Use ActualFinish equals AA. Next, calculate an effect size \\(d\\) for the difference in ELO scores between All-American and non-All-American wrestlers; you will need to calculate means and standard deviations as necessary. Since the populations are unbalanced, you will need to use a pooled sd of the form\n\\[ s_{pooled} = \\sqrt{\\frac{(n_1-1) s_1^2 + (n_2-1) s_2^2} {n_1 + n_2 -2}} \\]\nelo.dat \u0026lt;- read.table(\u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/Week5/elo.csv\u0026quot;, header = TRUE, row.names = 1, sep = \u0026quot;,\u0026quot;) head(elo.dat) ## Weight Conference ELO ActualFinish ExpectedFinish ## 3 125 ACC 1380.84 cons 24 E[cons 16] ## 4 125 ACC 1404.51 cons 12 E[cons 12] ## 5 125 ACC 1348.79 cons 24 E[cons 24] ## 6 125 Big 12 1312.73 cons 32 E[NQ] ## 8 125 Big 12 1373.79 cons 24 E[cons 16] ## 12 125 Big 12 1398.16 cons 12 E[NQ] elo.exNQ \u0026lt;- elo.dat[!grepl(\u0026quot;NQ\u0026quot;, elo.dat$ExpectedFinish),] # elo.exNQ$ActualFinish \u0026lt;- as.factor(elo.exNQ$ActualFinish[grepl(\u0026quot;AA\u0026quot;, elo.exNQ$ActualFinish)]) # Creating a new column with AA and other elo.exNQ$AA.Other \u0026lt;- as.factor(ifelse(grepl(\u0026#39;AA\u0026#39;, elo.exNQ$ActualFinish), \u0026#39;AA\u0026#39;, ifelse(!grepl(\u0026#39;AA\u0026#39;, elo.exNQ$ActualFinish), \u0026#39;Other\u0026#39;, NA))) # # meand AA # mean.dat \u0026lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$AA.Other), mean), c(\u0026quot;group\u0026quot;, \u0026quot;mean\u0026quot;)) # sd.dat \u0026lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$AA.Other), sd), c(\u0026quot;group\u0026quot;, \u0026quot;SD\u0026quot;)) # count.dat \u0026lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$AA.Other), length), c(\u0026quot;group\u0026quot;, \u0026quot;Count\u0026quot;)) mean.dat \u0026lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$ExpectedFinish), mean), c(\u0026quot;group1\u0026quot;, \u0026quot;mean\u0026quot;)) sd.dat \u0026lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$ExpectedFinish), sd), c(\u0026quot;group1\u0026quot;, \u0026quot;SD\u0026quot;)) count.dat \u0026lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$ExpectedFinish), length), c(\u0026quot;group1\u0026quot;, \u0026quot;Count\u0026quot;)) Mean_SD_Count.dat \u0026lt;- Reduce(function(...) merge(..., by = c(\u0026quot;group1\u0026quot;), all.x = TRUE), lapply( list(mean.dat, sd.dat, count.dat), transform # grp = ave(seq_along(group), group, FUN = seq_along) )) # Here as well, we will relabel our groups as \u0026#39;AA\u0026#39; and \u0026#39;Other\u0026#39; Mean_SD_Count.dat$group \u0026lt;- as.factor(ifelse(grepl(\u0026#39;AA\u0026#39;, Mean_SD_Count.dat$group1), \u0026#39;AA\u0026#39;, ifelse(!grepl(\u0026#39;AA\u0026#39;, Mean_SD_Count.dat$group1), \u0026#39;Other\u0026#39;, NA))) sd_pooled \u0026lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$group), function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) ) sd_pooled ## $AA ## [1] 41.31208 ## ## $Other ## [1] 16.58026 # Now we get the mean of mean Mean \u0026lt;- lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$group), function(dd) mean(dd$mean)) # Now for Counts Count \u0026lt;- lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$group), function(dd) sum(dd$Count)) # combine two lists as a dataframe table.mean.sd \u0026lt;- do.call(rbind, Map(data.frame, Mean = Mean, sd_pooled = sd_pooled, Count = Count)) cohen.d \u0026lt;- function(m1, s1, m2, s2){ cohens_d \u0026lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2)) return(cohens_d) } required.replicates \u0026lt;- function (m1, s1, m2, s2, alpha=0.05, beta=0.2){ n \u0026lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) return(round(n,0)) } effect_size \u0026lt;- cohen.d( m1 = table.mean.sd$Mean[1], s1 = table.mean.sd$sd_pooled[1], m2 = table.mean.sd$Mean[2], s2 = table.mean.sd$sd_pooled[2] ) effect_size ## [1] 2.687879 Req.Replicates \u0026lt;- required.replicates( m1 = table.mean.sd$Mean[1], s1 = table.mean.sd$sd_pooled[1], m2 = table.mean.sd$Mean[2], s2 = table.mean.sd$sd_pooled[2] ) Req.Replicates ## [1] 2  Part b. Calculate jackknife and bootstrap estimates of the error of \\(d\\). Since ELO is determined by a wrestlers success within a weight class, you will need to honor this grouping (or sampling) of the data. Calculate the jackknife by excluding one Weight at a time from the data, and recalculating \\(d\\); since there are 10 weight classes there should be 10 jackknife replicates.\nFor the bootstrap, sample from the 10 weight classes (use unique or levels). Note that you will not be able simply subset the data on something like Weight %in% samples, since the bootstrap will require duplicate samples. Instead, iterate over weight class samples and merge subsets of the original data.\n Part c. Compare your estimates of standard error to the parametric estimate, approximated by\n\\[ s.e._d ~ \\sqrt{\\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2} {2(n_1 + n_2)}} \\]\nn1 = table.mean.sd$Count[1] n2 = table.mean.sd$Count[2] d = effect_size # parametric estimate of standard error s.e_d \u0026lt;- sqrt((((n1 + n2)/(n1*n2))) + ((d^2)/(2*(n1+n2)))) s.e_d ## [1] 0.1778143   Exercise 4 Consider the data for U.S. Wholesale price for pumpkins 2018 in pumpkins.csv.\nPart a. Load the data, and calculate the \\(F\\) test and the parametric \\(P(\u0026gt;F)\\) using the code below. (set eval=TRUE).\npumpkins.dat \u0026lt;- read.csv(\u0026quot;/Users/owner1/Box/sdsu/statistical_programming_course/Week5/pumpkins.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;) attach(pumpkins.dat) # print(summary(pumpkins.dat)) summary(aov(Price ~ Class, data=pumpkins.dat)) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Class 3 44687 14896 94.03 4.42e-14 *** ## Residuals 26 4119 158 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 tt \u0026lt;- summary(aov(Price ~ Class, data = pumpkins.dat)) # p value tt[[1]][[\u0026quot;Pr(\u0026gt;F)\u0026quot;]][1] ## [1] 4.421291e-14 # F test tt[[1]][[\u0026quot;F value\u0026quot;]][1] ## [1] 94.03394  Part b. Permute Price over Class - that assume create a new data set on the assumption that Class has not influence on Price. Do this 1000 times, and calculate the \\(F\\) ratio for each. Plot the distribution of \\(F\\), and calculate how many \\(F\\) are greater than the \\(F\\) from part a. How does this compare with the parametric estimate for \\(P(\u0026gt;F\\)? Do you need to increase the number of permutations?\nAnswer: I do not see any of the calculated F-statistics greater then the F-statistics calculated from part a. Perhaps I need to increase the number of permutations.\nB \u0026lt;- 1000 perm.theta.rep \u0026lt;- rep(0,B) F_ratio \u0026lt;- {} for(i in 1:B) { y.s \u0026lt;- sample(Price) gg \u0026lt;- aov(y.s ~ Class) # summary(tt)[[1]][[\u0026quot;Pr(\u0026gt;F)\u0026quot;]] F_ratio.tmp \u0026lt;- summary(gg)[[1]][[\u0026quot;F value\u0026quot;]][1] F_ratio \u0026lt;- c(F_ratio, F_ratio.tmp) } F_ratio[1:10] ## [1] 2.0607382 0.3019736 0.3944886 1.8049371 9.4249732 0.5467785 0.6190417 ## [8] 4.1575200 0.8452027 1.1795127 # Distribution of F-ratio hist(F_ratio) # how many $F$ are greater than the $F$ from part a sum(F_ratio \u0026gt; tt[[1]][[\u0026quot;F value\u0026quot;]][1]) ## [1] 0  Part c. Repeat part b, but this time, honor the Week grouping. That is, permute Price over Class only within observations grouped by Week. Compare this to\nsummary(aov(Price ~ Class + as.factor(Week), data=pumpkins.dat)) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Class 3 44687 14896 76.833 3.75e-11 *** ## as.factor(Week) 6 241 40 0.207 0.97 ## Residuals 20 3877 194 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 B \u0026lt;- 1000 perm.theta.rep \u0026lt;- rep(0,B) F_ratio \u0026lt;- {} for(i in 1:B) { y.s \u0026lt;- sample(Price) gg \u0026lt;- aov(y.s ~ Class + as.factor(Week)) # summary(tt)[[1]][[\u0026quot;Pr(\u0026gt;F)\u0026quot;]] F_ratio.tmp \u0026lt;- summary(gg)[[1]][[\u0026quot;F value\u0026quot;]][1] F_ratio \u0026lt;- c(F_ratio, F_ratio.tmp) } F_ratio[1:10] ## [1] 0.7431674 2.1164829 0.2572306 0.2782830 0.7235510 0.3795191 2.1433168 ## [8] 0.9127464 0.3869931 0.9630900 # Distribution of F-ratio hist(F_ratio) tt \u0026lt;- summary(aov(Price ~ Class + as.factor(Week), data = pumpkins.dat)) sum(F_ratio \u0026gt; tt[[1]][[\u0026quot;F value\u0026quot;]][1]) ## [1] 0 Which are more appropriate for these data?\nI think having more number of permutations help in getting better statistics for these data.\nComments: Ex 3. For the bootstrap, something like\nweights \u0026lt;- unique(elo.dat\\(Weight) for (i in 1:samples) { weight.samples \u0026lt;- sample(weights,replace=TRUE) elo.sub \u0026lt;- c() for(w in weight.samples) { elo.sub \u0026lt;- elo.dat[(elo.dat\\)Weight == w),] }\nIt is a harder exercise, because we need to respect the sampling method of the original data, and for these data, wrestlers are grouped by weight class, so we need to sample weight classes. Similarly, for Ex 4, we want to sample Price observations within Week groups, so\nweeks \u0026lt;- unique(pumpkins.dat\\(Week) for(i in 1:10000) { for(week in weeks) { mask \u0026lt;- pumpkins.dat\\)Week==week pumpkins.dat\\(Price[mask] \u0026lt;- sample(pumpkins.dat\\)Price[mask]) } pumpkins.dat\\(Price[mask] \u0026lt;-sample(pumpkins.dat\\)Price[mask]) }\nConsider if Ex 4 had been executed as a randomized complete block design, with Week as block. You would have independently randomized and applied treatment (Class) to each experimental unit within the block. We need to do the same thing with the bootstrap.\n  ","date":1565907983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565907983,"objectID":"dfe4973ae4b4460ef4f4be8fab33a869","permalink":"/achalneupane.github.io/post/resampling/","publishdate":"2019-08-15T17:26:23-05:00","relpermalink":"/achalneupane.github.io/post/resampling/","section":"post","summary":"Statistics series","tags":["R","Statistics","Statistical_programming"],"title":"Resampling","type":"post"},{"authors":["**Achal Neupane**","Chenchen Feng","Pauline K. Mochama","Huma Saleem","Shin-Yi Lee Marzano"],"categories":null,"content":"Abstract\nRNA silencing or RNA interference (RNAi) is an essential mechanism in animals, fungi, and plants that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. We previously determined that mycoviruses are triggers and targets of RNA silencing in Sclerotinia sclerotiorum. However, recent progresses in RNAi or dsRNA-based pest control requires more detailed characterization of the RNA silencing pathways in S. sclerotiorum to investigate the utility of dsRNA-based strategy for white mold control. This study elucidates the roles of argonaute enzymes, agl-2 and agl-4, in small RNA metabolism in S. sclerotiorum. Gene disruption mutants of agl-2 and agl-4 were compared for changes in phenotype, virulence, viral susceptibility, and small RNA profiles. The Δagl-2 mutant but not the Δagl-4 mutant had significantly slower growth and virulence prior to virus infection. Similarly, the Δagl-2 mutant but not the Δagl-4 mutant, showed greater debilitation under virus infection compared to uninfected strains. The responses were confirmed in complementation studies and revealed the antiviral role of agl-2. Gene disruption mutants of agl-2, agl-4, Dicer-like (dcl)-1, and dcl-2 did not change the stability of the most abundant endogenous small RNAs, which suggests the existence of alternative enzymes/pathways for small RNA biogenesis in S. sclerotiorum. Furthermore, in vitro synthesized dsRNA targeting agl-2 showed a significantly reduced average lesion diameter (P \u0026lt; 0.05) on canola leaves with agl-2 down-regulated compared to controls. This is the first report describing the effectiveness of RNA pesticides targeting S. sclerotiorum RNA silencing pathway for the control of the economically important pathogen.\n","date":1564462800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564462800,"objectID":"6f94d744b708b104fb142f701b6e1e44","permalink":"/achalneupane.github.io/publication/neupane_etal_2019_frontiers/","publishdate":"2019-07-30T00:00:00-05:00","relpermalink":"/achalneupane.github.io/publication/neupane_etal_2019_frontiers/","section":"publication","summary":"Abstract\nRNA silencing or RNA interference (RNAi) is an essential mechanism in animals, fungi, and plants that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. We previously determined that mycoviruses are triggers and targets of RNA silencing in Sclerotinia sclerotiorum. However, recent progresses in RNAi or dsRNA-based pest control requires more detailed characterization of the RNA silencing pathways in S.","tags":null,"title":"Roles of argonautes  and dicers on Sclerotinia sclerotiorum antiviral RNA silencing","type":"publication"},{"authors":["**Achal Neupane**","Chenchen Feng","Pauline K. Mochama","Huma Saleem","Shin-Yi Lee Marzano"],"categories":null,"content":"Abstract\nProfiling the classes of the RNA silencing, also known as RNA interference, is an essential mechanism in plants, animals and fungi that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. RNA-silencing- deficient fungi show increased susceptibility to virus infection. Little is known about the classes of RNA editing in virus- derived small RNA which will teach us the nature of self-nonself recognition and ways to modulate RNA modification to control fungal infections. The present study dissects the RNA silencing pathway in S. sclerotiorum by disrupting its key silencing genes using the split-marker recombination method in order to probe the contributions of these genes, specifically argonautes, to fungal virulence and viral defense mechanisms. Following gene disruption, mutants were studied for changes in phenotype, pathogenicity, viral susceptibility, and small RNA processing compared to the wild-type strain, DK3. Among the argonaute mutants, the agl-2 mutant had significantly slower growth and virulence prior to and following virus infection. Additional analyses indicated that the virus-infected wild-type strain accumulated virus-derived small RNAS (vsiRNAs) with distinct patterns of internal and terminal nucleotide mismatches. Dicer 1 mutant produced less vsiRNA compared to dicer 2 mutant and the wild type strain, suggesting the two dicers are not in the state of complete redundancy. This finding expands our overall understanding of S. sclerotiorum and has important implications for any current or future uses of dsRNA and mycoviruses as disease control agents.\n","date":1563944400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563944400,"objectID":"4bc3c665c720fc0f7cad27884a42cab3","permalink":"/achalneupane.github.io/talk/asv_2019/","publishdate":"2019-07-24T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/asv_2019/","section":"talk","summary":"Abstract\nProfiling the classes of the RNA silencing, also known as RNA interference, is an essential mechanism in plants, animals and fungi that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. RNA-silencing- deficient fungi show increased susceptibility to virus infection. Little is known about the classes of RNA editing in virus- derived small RNA which will teach us the nature of self-nonself recognition and ways to modulate RNA modification to control fungal infections.","tags":null,"title":"Profiling the Classes of the RNA Editing in Virus-derived small RNAs in White Mold Sclerotinia sclerotiorum","type":"talk"},{"authors":["Surendra Neupane","Sarah E Schweitzer","**Achal Neupane**","Ethan J Andersen","Anne Fennell","Ruanbao Zhou","Madhav P Nepal"],"categories":null,"content":"Abstract\nMitogen-Activated Protein Kinase (MAPK) genes encode proteins that regulate biotic and abiotic stresses in plants through signaling cascades comprised of three major subfamilies: MAP Kinase (MPK), MAPK Kinase (MKK), and MAPKK Kinase (MKKK). The main objectives of this research were to conduct genome-wide identification of MAPK genes in Helianthus annuus and examine functional divergence of these genes in relation to those in nine other plant species (Amborella trichopoda, Aquilegia coerulea, Arabidopsis thaliana, Daucus carota, Glycine max, Oryza sativa, Solanum lycopersicum, Sphagnum fallax, and Vitis vinifera), representing diverse taxonomic groups of the Plant Kingdom. A Hidden Markov Model (HMM) profile of the MAPK genes utilized reference sequences from A. thaliana and G. max, yielding a total of 96 MPKs and 37 MKKs in the genomes of A. trichopoda, A. coerulea, C. reinhardtii, D. carota, H. annuus, S. lycopersicum, and S. fallax. Among them, 28 MPKs and eight MKKs were confirmed in H. annuus. Phylogenetic analyses revealed four clades within each subfamily. Transcriptomic analyses showed that at least 19 HaMPK and seven HaMKK genes were induced in response to salicylic acid (SA), sodium chloride (NaCl), and polyethylene glycol (Peg) in leaves and roots. Of the seven published sunflower microRNAs, five microRNA families are involved in targeting eight MPKs. Additionally, we discussed the need for using MAP Kinase nomenclature guidelines across plant species. Our identification and characterization of MAP Kinase genes would have implications in sunflower crop improvement, and in advancing our knowledge of the diversity and evolution of MAPK genes in the Plant Kingdom.\n","date":1548136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548136800,"objectID":"5f8603dbf56d766d0adf9bd99a7adc8c","permalink":"/achalneupane.github.io/publication/surendra_et_al_2019_plants/","publishdate":"2019-01-22T00:00:00-06:00","relpermalink":"/achalneupane.github.io/publication/surendra_et_al_2019_plants/","section":"publication","summary":"Abstract\nMitogen-Activated Protein Kinase (MAPK) genes encode proteins that regulate biotic and abiotic stresses in plants through signaling cascades comprised of three major subfamilies: MAP Kinase (MPK), MAPK Kinase (MKK), and MAPKK Kinase (MKKK). The main objectives of this research were to conduct genome-wide identification of MAPK genes in Helianthus annuus and examine functional divergence of these genes in relation to those in nine other plant species (Amborella trichopoda, Aquilegia coerulea, Arabidopsis thaliana, Daucus carota, Glycine max, Oryza sativa, Solanum lycopersicum, Sphagnum fallax, and Vitis vinifera), representing diverse taxonomic groups of the Plant Kingdom.","tags":null,"title":"Identification and Characterization of Mitogen-Activated Protein Kinase (MAPK) Genes in Sunflower (Helianthus annuus L.)","type":"publication"},{"authors":["**Achal Neupane**","Pauline K. Mochama","Huma Saleem","Shin-Yi Lee Marzano"],"categories":null,"content":"Abstract\nProfilingRNA silencing, also known as RNA interference, is an essential mechanism in plants, animals and fungi that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. RNA-silencing- deficient fungi show increased susceptibility to virus infection. Plant pathogenic fungi also utilize RNA silencing to silence plant host immunity genes through the delivery of fungal small RNAs into plants. This cross-kingdom RNA silencing facilitates fungal infection of plants. Overall, these findings demonstrate the significant contributions of fungal RNA silencing pathways to fungal virulence and viral defense. This study dissects the RNA silencing pathway in S. sclerotiorum by disrupting its key silencing genes using the split-marker recombination method in order to probe the contributions of these genes, specifically argonautes, to fungal virulence and viral defense mechanisms. Following gene disruption, mutants were studied for changes in phenotype, pathogenicity, viral susceptibility, and small RNA processing compared to the wild-type strain, DK3. Among the argonaute mutants, the ∆agl-2 mutant had significantly slower growth and virulence prior to and following virus infection. Additional studies indicated that the virus-infected wild-type strain accumulated virus-derived smallRNAs(vsiRNAs) with distinct patterns of internal and terminal nucleotide mismatches. Additionally, dicer 1 mutant produced less vsiRNA compared to dicer 2 mutant and the wild type strain. These results together support that S. sclerotiorum has robust RNAsilencing mechanisms that function primarily in antiviral defense but also in endogenous gene regulation processes. This finding expands our overall understanding of S. sclerotiorum and has important implications for any current or future uses of mycoviruses as biological control agents, an emerging area of interest in fungal control research.\n","date":1547272800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547272800,"objectID":"a4f6bf78e7304328b86c1c5a819bab67","permalink":"/achalneupane.github.io/talk/pag_2019/","publishdate":"2019-01-12T00:00:00-06:00","relpermalink":"/achalneupane.github.io/talk/pag_2019/","section":"talk","summary":"Abstract\nProfilingRNA silencing, also known as RNA interference, is an essential mechanism in plants, animals and fungi that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. RNA-silencing- deficient fungi show increased susceptibility to virus infection. Plant pathogenic fungi also utilize RNA silencing to silence plant host immunity genes through the delivery of fungal small RNAs into plants.","tags":null,"title":"Roles of Dicers and Argonautes on Sclerotinia sclerotiorum Antiviral Small RNA Processing","type":"talk"},{"authors":["**Achal Neupane**","Chenchen Feng","Jiuhuan Feng","Arjun Kafle","Heike Bücking","Shin-Yi Lee Marzano"],"categories":null,"content":"Abstract\nArbuscular mycorrhizal fungi (AMF), including Rhizophagus spp., can play important roles in nutrient cycling of the rhizosphere. However, the effect of virus infection on AMF’s role in nutrient cycling cannot be determined without first knowing the diversity of the mycoviruses in AMF. Therefore, in this study, we sequenced the R. irregularis isolate-09 due to its previously demonstrated high efficiency in increasing the N/P uptake of the plant. We identified one novel mitovirus contig of 3685 bp, further confirmed by reverse transcription-PCR. Also, publicly available Rhizophagus spp. RNA-Seq data were analyzed to recover five partial virus sequences from family Narnaviridae, among which four were from R. diaphanum MUCL-43196 and one was from R. irregularis strain-C2 that was similar to members of the Mitovirus genus. These contigs coded genomes larger than the regular mitoviruses infecting pathogenic fungi and can be translated by either a mitochondrial translation code or a cytoplasmic translation code, which was also reported in previously found mitoviruses infecting mycorrhizae. The five newly identified virus sequences are comprised of functionally conserved RdRp motifs and formed two separate subclades with mitoviruses infecting Gigaspora margarita and Rhizophagus clarus, further supporting virus-host co-evolution theory. This study expands our understanding of virus diversity. Even though AMF is notably hard to investigate due to its biotrophic nature, this study demonstrates the utility of whole root metatranscriptome.\n","date":1544594400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544594400,"objectID":"58503c103ec3658f6b32818b6b2a869f","permalink":"/achalneupane.github.io/publication/neupane_etal_viruses_2018/","publishdate":"2018-12-12T00:00:00-06:00","relpermalink":"/achalneupane.github.io/publication/neupane_etal_viruses_2018/","section":"publication","summary":"Abstract\nArbuscular mycorrhizal fungi (AMF), including Rhizophagus spp., can play important roles in nutrient cycling of the rhizosphere. However, the effect of virus infection on AMF’s role in nutrient cycling cannot be determined without first knowing the diversity of the mycoviruses in AMF. Therefore, in this study, we sequenced the R. irregularis isolate-09 due to its previously demonstrated high efficiency in increasing the N/P uptake of the plant. We identified one novel mitovirus contig of 3685 bp, further confirmed by reverse transcription-PCR.","tags":null,"title":"Metatranscriptomic analysis and in silico approach identified mycoviruses in the arbuscular mycorrhizal fungus Rhizophagus spp.","type":"publication"},{"authors":["Shin-Yi Lee Marzano","**Achal Neupane**","Leslie Domier"],"categories":null,"content":"Abstract\nMycoviruses belonging to the family Hypoviridae cause persistent infection of many different host fungi. We previously determined that the white mold fungus, Sclerotinia sclerotiorum, infected with Sclerotinia sclerotiorum hypovirus 2-L (SsHV2-L) exhibits reduced virulence, delayed/reduced sclerotial formation, and enhanced production of aerial mycelia. To gain better insight into the cellular basis for these changes, we characterized changes in mRNA and small RNA (sRNA) accumulation in S. sclerotiorum to infection by SsHV2-L. A total of 958 mRNAs and 835 sRNA-producing loci were altered after infection by SsHV2-L, among which \u0026gt;100 mRNAs were predicted to encode proteins involved in the metabolism and trafficking of carbohydrates and lipids. Both S. sclerotiorum endogenous and virus-derived sRNAs were predominantly 22 nt in length suggesting one dicer-like enzyme cleaves both. Novel classes of endogenous small RNAs were predicted, including phasiRNAs and tRNA-derived small RNAs. Moreover, S. sclerotiorum phasiRNAs, which were derived from noncoding RNAs and have the potential to regulate mRNA abundance in trans, showed differential accumulation due to virus infection. tRNA fragments did not accumulate differentially after hypovirus infection. Hence, in-depth analysis showed that infection of S. sclerotiorum by a hypovirulence-inducing hypovirus produced selective, large-scale reprogramming of mRNA and sRNA production.\n","date":1544421600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544421600,"objectID":"ba5b8bea183ffc47be271213a1106625","permalink":"/achalneupane.github.io/publication/marzano_etal_viruses_2018/","publishdate":"2018-12-10T00:00:00-06:00","relpermalink":"/achalneupane.github.io/publication/marzano_etal_viruses_2018/","section":"publication","summary":"Abstract\nMycoviruses belonging to the family Hypoviridae cause persistent infection of many different host fungi. We previously determined that the white mold fungus, Sclerotinia sclerotiorum, infected with Sclerotinia sclerotiorum hypovirus 2-L (SsHV2-L) exhibits reduced virulence, delayed/reduced sclerotial formation, and enhanced production of aerial mycelia. To gain better insight into the cellular basis for these changes, we characterized changes in mRNA and small RNA (sRNA) accumulation in S. sclerotiorum to infection by SsHV2-L.","tags":null,"title":"Transcriptional and small RNA responses of the white mold fungus Sclerotinia sclerotiorum to infection by a virulence-attenuating hypovirus","type":"publication"},{"authors":["Surendra Neupane","Ethan J. Andersen","**Achal Neupane**","Madhav P. Nepal"],"categories":null,"content":"Abstract\nNucleotide Binding Site-Leucine-Rich Repeat (NBS-LRR) genes encode disease resistance proteins involved in plants\u0026rsquo; defense against their pathogens. Although sunflower is affected by many diseases, only a few molecular details have been uncovered regarding pathogenesis and resistance mechanisms. Recent availability of sunflower whole genome sequences in publicly accessible databases allowed us to accomplish a genome-wide identification of Toll-interleukin-1 receptor-like Nucleotide-binding site Leucine-rich repeat (TNL), Coiled Coil (CC)-NBS-LRR (CNL), Resistance to powdery mildew 8 (RPW8)-NBS-LRR (RNL) and NBS-LRR (NL) protein encoding genes. Hidden Markov Model (HMM) profiling of 52,243 putative protein sequences from sunflower resulted in 352 NBS-encoding genes, among which 100 genes belong to CNL group including 64 genes with RX_CC like domain, 77 to TNL, 13 to RNL, and 162 belong to NL group. We also identified signal peptides and nuclear localization signals present in the identified genes and their homologs. We found that NBS genes were located on all chromosomes and formed 75 gene clusters, one-third of which were located on chromosome 13. Phylogenetic analyses between sunflower and Arabidopsis NBS genes revealed a clade-specific nesting pattern in CNLs, with RNLs nested in the CNL-A clade, and species-specific nesting pattern for TNLs. Surprisingly, we found a moderate bootstrap support (BS = 50%) for CNL-A clade being nested within TNL clade making both the CNL and TNL clades paraphyletic. Arabidopsis and sunflower showed 87 syntenic blocks with 1049 high synteny hits between chromosome 5 of Arabidopsis and chromosome 6 of sunflower. Expression data revealed functional divergence of the NBS genes with basal level tissue-specific expression. This study represents the first genome-wide identification of NBS genes in sunflower paving avenues for functional characterization and potential crop improvement.\n","date":1530334800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530334800,"objectID":"c75a920391264e082b0e1de38e825df9","permalink":"/achalneupane.github.io/publication/surendra_et_al_2018_july_genes/","publishdate":"2018-06-30T00:00:00-05:00","relpermalink":"/achalneupane.github.io/publication/surendra_et_al_2018_july_genes/","section":"publication","summary":"Abstract\nNucleotide Binding Site-Leucine-Rich Repeat (NBS-LRR) genes encode disease resistance proteins involved in plants\u0026rsquo; defense against their pathogens. Although sunflower is affected by many diseases, only a few molecular details have been uncovered regarding pathogenesis and resistance mechanisms. Recent availability of sunflower whole genome sequences in publicly accessible databases allowed us to accomplish a genome-wide identification of Toll-interleukin-1 receptor-like Nucleotide-binding site Leucine-rich repeat (TNL), Coiled Coil (CC)-NBS-LRR (CNL), Resistance to powdery mildew 8 (RPW8)-NBS-LRR (RNL) and NBS-LRR (NL) protein encoding genes.","tags":null,"title":"Genome-Wide Identification of NBS-Encoding Resistance Genes in Sunflower (Helianthus annuus L.)","type":"publication"},{"authors":["Pauline Mochama","Prajakta Jadhav","**Achal Neupane**","Shin-Yi Lee Marzano"],"categories":null,"content":"Abstract\nThis study aimed to demonstrate the existence of antiviral RNA silencing mechanisms in Sclerotinia sclerotiorum by infecting wild-type and RNA-silencing-deficient strains of the fungus with an RNA virus and a DNA virus. Key silencing-related genes were disrupted to dissect the RNA silencing pathway. Specifically, dicer genes (dcl-1, dcl-2, and both dcl-1/dcl-2) were displaced by selective marker(s). Disruption mutants were then compared for changes in phenotype, virulence, and susceptibility to virus infections. Wild-type and mutant strains were transfected with a single-stranded RNA virus, SsHV2-L, and copies of a single-stranded DNA mycovirus, SsHADV-1, as a synthetic virus constructed in this study. Disruption of dcl-1 or dcl-2 resulted in no changes in phenotype compared to wild-type S. sclerotiorum; however, the double dicer mutant strain exhibited significantly slower growth. Furthermore, the Δdcl-1/dcl-2 double mutant, which was slow growing without virus infection, exhibited much more severe debilitation following virus infections including phenotypic changes such as slower growth, reduced pigmentation, and delayed sclerotial formation. These phenotypic changes were absent in the single mutants, Δdcl-1 and Δdcl-2. Complementation of a single dicer in the double disruption mutant reversed viral susceptibility to the wild-type state. Virus-derived small RNAs were accumulated from virus-infected wild-type strains with strand bias towards the negative sense. The findings of these studies indicate that S. sclerotiorum has robust RNA silencing mechanisms that process both DNA and RNA mycoviruses and that, when both dicers are silenced, invasive nucleic acids can greatly debilitate the virulence of this fungus.\n","date":1524027600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524027600,"objectID":"40d3beff2b1de1cc58bac67870f249e7","permalink":"/achalneupane.github.io/publication/mochama_et_al/","publishdate":"2018-04-18T00:00:00-05:00","relpermalink":"/achalneupane.github.io/publication/mochama_et_al/","section":"publication","summary":"Abstract\nThis study aimed to demonstrate the existence of antiviral RNA silencing mechanisms in Sclerotinia sclerotiorum by infecting wild-type and RNA-silencing-deficient strains of the fungus with an RNA virus and a DNA virus. Key silencing-related genes were disrupted to dissect the RNA silencing pathway. Specifically, dicer genes (dcl-1, dcl-2, and both dcl-1/dcl-2) were displaced by selective marker(s). Disruption mutants were then compared for changes in phenotype, virulence, and susceptibility to virus infections.","tags":null,"title":"Mycoviruses as Triggers and Targets of RNA Silencing in White Mold Fungus Sclerotinia sclerotiorum","type":"publication"},{"authors":["Paul Leo","Achal Neupane","Mahmoud Bassal","Zeya Kyaw Maung","Felicity Newell","Megan Ellis","Gregory Boxall","Amanda Smith","Paula Marlton","Richard D'Andrea","Thomas Gonda","Matthew Brown","Russell Saal","Andrew Moore"],"categories":null,"content":"Abstract\nBackground :\nGene and mutation discovery in cancers is typically performed by sequencing somatic and germline samples from the same individuals and subtracting germline mutations. However, in biobanked acute myeloid leukaemia (AML) samples, matched germline DNA is frequently unavailable. When germline samples are available, sequencing both germline and somatic samples for each patient significantly increases costs. This study explores whether it is possible to utilise unrelated germline controls to identify ocogenic drivers.\nMethods :\nWhole exome sequencing was performed on somatic samples from 188 adults and children with AML (n=144 and n=44 respectively) and 429 control germline samples. The discovery process includes a rigorous statistical genetics approach whereby the AML cases and controls are age and ethnically matched. Residual population stratification is controlled by appropriate covariates. Burden tests are performed to detect mutations enriched in somatic samples compared to the unrelated germline controls. Statistical significance is determined by Q-Q plots against the null hypothesis.\nResults :\nBurden tests that included damaging protein coding mutations identified genes previously described with recurrent mutations in AML (e.g. NPM1, DNMT3A, IDH2, NRAS, RUNX1, FLT3, IDH1, TET2, ASXL1) thus validating this bioinformatics approach. Furthermore, this methodology resulted in the detection of several additional novel genes not previously identified in AML. These mutations are currently undergoing conventional validation by Sanger sequencing, as well as bioinformatic validation using whole genome sequencing data from 128 of the same AML samples sequenced using the Complete Genomics platform.\nDiscussion :\nWe demonstrate that this methodology can: 1) Identify technical artefacts from sequencing and alignment. 2) Enable statistical modelling of allele frequencies that can identify low frequency clones which are undetectable using standard genotyping approaches. 3) Provide a purely statistical approach to gene discovery, agnostic to mutation type (synonymous, coding, non-coding etc).\nConclusions :\nIn summary, we have performed whole exome and whole genome sequencing on the largest cohort of AML samples in Australia. Innovative bioinformatic analysis has detected all previously identified, somatically mutated AML genes and has discovered a number of potentially significant novel mutations. The functional and prognostic impact of these events and the subsequent functional investigations will be discussed.\n","date":1471755600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471755600,"objectID":"2b845749eb8d1dda4a49c0a934dc5a5b","permalink":"/achalneupane.github.io/talk/leo_et_al/","publishdate":"2016-08-21T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/leo_et_al/","section":"talk","summary":"Abstract\nBackground :\nGene and mutation discovery in cancers is typically performed by sequencing somatic and germline samples from the same individuals and subtracting germline mutations. However, in biobanked acute myeloid leukaemia (AML) samples, matched germline DNA is frequently unavailable. When germline samples are available, sequencing both germline and somatic samples for each patient significantly increases costs. This study explores whether it is possible to utilise unrelated germline controls to identify ocogenic drivers.","tags":null,"title":"Gene Discovery in Acute Myeloid Leukaemia: somatic and germline mutations.","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"127af807fc7ac22881d4a1f91b27f308","permalink":"/achalneupane.github.io/project/statistical-programming/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/achalneupane.github.io/project/statistical-programming/","section":"project","summary":"Some Statistics","tags":null,"title":"Updates","type":"project"},{"authors":["Amanda M. Smith","**Achal Neupane**","Sadia Afrin","Rachel Burow","Caedyn L. Stinson","Andrew S. Moore"],"categories":null,"content":"Abstract\nMutations of the RAS family of genes are frequent events in AML, occurring in 10% of children and 10-20% of adults. NRASmutations promote proliferation through activation of the Ras/Raf/MEK/ERK signalling pathway. Several MEK inhibitors have shown promising pre-clinical activity in AML, with a number of compounds currently in adult phase I/II clinical trials, including AZD6244, GSK1120212 and AS703026. Given not all patients respond to MEK inhibitor treatment we undertook a comprehensive preclinical evaluation of MEK inhibitors in AML and developed a clinically relevant model of resistance. The in vitroefficacy of 7 MEK inhibitors was determined using a diverse panel of 6 paediatric and 5 adult AML cell lines (Table 1). All AML cell lines were sensitive to at least one MEK inhibitor with the exception of the Down syndrome associated AML line, CMK, and the adult erythroblastic line, HEL, that showed overt resistance (IC50 \u0026gt;20μM) to MEK inhibition (Table 1). In sensitive cell lines, the reduced proliferation was associated with apoptosis, as assessed by Annexin V+ staining. To confirm mechanism of action, inhibition of MEK phosphorylation as well as the downstream kinase, pERK, were assessed by immunoblotting. The level of basal MEK activation was variable across the cell line panel and pMEK was increased upon exposure to active MEK inhibitors. In contrast, levels of pERK were reduced suggesting that MEK inhibitors may disrupt the interaction of MEK with its downstream transducers rather than a direct inhibition of MEK phosphorylation. Molecular and clinical resistance to kinase inhibitors is well described for targets such as FLT3 and BCR-ABL1. Since clinical responses to MEK inhibitors have been variable, we investigated the potential mechanisms of resistance to MEK inhibitors in vitro. Long-term culture of THP-1 cells (MLL-rearranged, NRASmutated) with AZD6244 and AS703026, resulted in high-level resistance. Importantly, cells displayed cross-resistance not only to these two compounds but also a third MEK inhibitor, GSK1120212 (Table 2). Resistance was associated with reduced basal pMEK expression. In order to establish the mechanism of resistance we performed comprehensive mutation and gene expression analyses utilising whole-exome sequencing and RNAseq respectively. These data revealed a spectrum of acquired molecular aberrations common to both resistant cell lines compared to the parental THP1 cells. Together, these data indicate that whilst MEK inhibition is a promising strategy to treat AML, resistance to one MEK inhibitor may lead to cross-resistance to other compounds targeting MEK.\n","date":1458104400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458104400,"objectID":"539eb55424ad86f403b2c1547e80887a","permalink":"/achalneupane.github.io/talk/mek_brisbane_2016/","publishdate":"2016-03-16T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/mek_brisbane_2016/","section":"talk","summary":"Abstract\nMutations of the RAS family of genes are frequent events in AML, occurring in 10% of children and 10-20% of adults. NRASmutations promote proliferation through activation of the Ras/Raf/MEK/ERK signalling pathway. Several MEK inhibitors have shown promising pre-clinical activity in AML, with a number of compounds currently in adult phase I/II clinical trials, including AZD6244, GSK1120212 and AS703026. Given not all patients respond to MEK inhibitor treatment we undertook a comprehensive preclinical evaluation of MEK inhibitors in AML and developed a clinically relevant model of resistance.","tags":null,"title":"MEK inhibitor resistance in acute myeloid leukaemia","type":"talk"},{"authors":["Andrew S. Moore","Russell Saal","Lisa Anderson1","Gregory Boxal","Amanda M. Smith","Achal Neupane","Mhairi S. Marshall","Felicity Newell","Megan Ellis","Nik Cummings","Anna L. Brown","Ian D. Lewis","Bik To","Paula Marlton","Andrew H. Wei","Richard J. D'Andrea","Thomas J. Gonda","Matthew A. Brown","Paul J. Leo"],"categories":null,"content":"Abstract\nWhole exome sequencing (WES) can detect a high proportion of cancer causing genetic mutations, and therefore has the potential to replace targeted molecular assays and identify additional novel, patient-specific mutations. To test the ability of WES to detect variants of clinical consequence, we compared WES findings to results for FLT3-ITD and NPM1mut obtained by routine pathology testing (high-resolution fragment analysis (HRFA) and Matrix Assisted Laser Desorption/Ionization – Time of Flight Mass Spectrometry (MALDI-TOF) respectively), and to point mutation findings in DNMT3A, IDH1/2, JAK2, KRAS, NRAS and MPL, as determined by Sequenom, across a cohort of 188 AML samples from adults (n = 144) and children (n = 44). All subjects provided written informed consent, and the study protocols had been approved by institutional research ethics committees. Median age at diagnosis was 51 years, (range 1-89) and median bone marrow blast percentage was 76% (range 8-100). A discovery phase using 96 samples developed genotype algorithms and ascertained their sensitivity. NPM1mut were reliably detected using WES, with 100% (n=25 positive) concordance with MALDI-TOF. WES and Sequenom were 99.4% concordant over the selected gene panels (1594 measurements). The FLT3-ITD false negative rate initially exceeded 50%, leading us to optimize our discovery algorithms and develop a calibration curve which specified the sequence coverage needed over the FLT3-ITD region to reliably genotype at any allelic ratio (AR). We demonstrate that our algorithms can reliably detect FLT3-ITD with AR \u0026gt; 0.05 using exome capture with less than 100x coverage over the FLT3-ITD region. Additionally, we tested the algorithms over another 900 germline controls with no false positive measurements. In a validation phase using 78 samples, all NPM1mut samples (n=20 positive) were detected, including an additional NPM1mut (type A) initially missed using MALDI-TOF. Concordance with Sequenom again exceeded 99.5% (comparable to the error rate expected in this Sequenom panel). For FLT3-ITD, 18\u0026frasl;21 HRFA-positive samples were detected with all 3 false-negatives falling outside the predicted sensitivity threshold; two samples with AR \u0026lt;0.01 (which would require sequencing coverage of 500x for detection) and one with AR = 0.4 but with a sequence coverage of only 38x over the FLT3-ITD. In conclusion, we have performed WES on one of the largest cohorts of adult and paediatric AML described to date and demonstrate that clinically relevant mutations, including FLT3-ITD and NPM1mut, can be simultaneously detected with a single exome capture platform with accuracy similar to or better than current methods.\n","date":1441861200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441861200,"objectID":"4c3e271742d3183dc277147525076504","permalink":"/achalneupane.github.io/talk/esh_2015_buddapest/","publishdate":"2015-09-10T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/esh_2015_buddapest/","section":"talk","summary":"Abstract\nWhole exome sequencing (WES) can detect a high proportion of cancer causing genetic mutations, and therefore has the potential to replace targeted molecular assays and identify additional novel, patient-specific mutations. To test the ability of WES to detect variants of clinical consequence, we compared WES findings to results for FLT3-ITD and NPM1mut obtained by routine pathology testing (high-resolution fragment analysis (HRFA) and Matrix Assisted Laser Desorption/Ionization – Time of Flight Mass Spectrometry (MALDI-TOF) respectively), and to point mutation findings in DNMT3A, IDH1/2, JAK2, KRAS, NRAS and MPL, as determined by Sequenom, across a cohort of 188 AML samples from adults (n = 144) and children (n = 44).","tags":null,"title":"Sensitivity of Whole Exome Sequencing for the Detection of FLT3-ITD and NPM1 Mutations in Acute Myeloid Leukaemia","type":"talk"},{"authors":["Achal Neupane","Seong-il Eyun","Haichuan Wang","Blair D. Siegfried","Etsuko N. Moriyama"],"categories":null,"content":"Abstract\nThe western corn rootworm, Diabrotica virgifera virgifera(Coloeptera: Chrysomelidae), is one of the most devastating pests of corn causing nearly a billion dollars of financial loss both in terms of yield loss and treatment costs. Although Coleoptera is the most diverse order of insects comprising more than 400,000 species, only a few coleopteran genomes and transcriptomes (e.g., from Tribolium castaneum and Dendroctonus ponderosae) have been published to date. The genome size of haploid D. v. virgifera is estimated to be ~2.58 GB, one of the largest among beetle species. Its complete genome sequence is currently in the draft stage. In this study, in order to identify the gene sets expressed in their larval stages (when the most damage to corn is caused) and to contribute to improving the genome assembly, we have sequenced and assembled transcriptomes from egg, neonate, and third-instar larval stages of D. v. virgifera using next-generation technologies. In total ~700 gigabases were sequenced. De novo transcriptome assembly was performed using four different short read assemblers for individual and pooled sets of reads. Hybrid assembly using both Illumina and 454 reads was also performed. After examining the assembly quality based on contig length and annotation effectiveness with similarity search, we chose the Trinity assembly from the pooled dataset including 163,871 contigs (the average length: 914 bp) as the most inclusive. We identified and annotated genes encoding chemoreceptors, gamma-aminobutyric acid (GABA) type A receptor, and glycoside hydrolase families. Compared to the sequences found in the draft genome, we observed variations in sequences as well as in the number of introns. We also examined conservation of gene structures in chemoreceptors from closely related insect lineages. Our transcriptome sequences can contribute toward improved quality of the D. v. virgiferagenome assembly and annotations.\n","date":1397451600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397451600,"objectID":"202e7c475385ace7d2185edd3cffc440","permalink":"/achalneupane.github.io/talk/wcr_neupane/","publishdate":"2014-04-14T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/wcr_neupane/","section":"talk","summary":"Abstract\nThe western corn rootworm, Diabrotica virgifera virgifera(Coloeptera: Chrysomelidae), is one of the most devastating pests of corn causing nearly a billion dollars of financial loss both in terms of yield loss and treatment costs. Although Coleoptera is the most diverse order of insects comprising more than 400,000 species, only a few coleopteran genomes and transcriptomes (e.g., from Tribolium castaneum and Dendroctonus ponderosae) have been published to date. The genome size of haploid D.","tags":null,"title":"Transcriptome analysis of western corn rootworm larvae and eggs","type":"talk"},{"authors":["Sarbottam Piya","Madhav P. Nepal","Jack L. Butler","Gary E. Larson","**Achal Neupane**"],"categories":null,"content":"Abstract\nSickleweed (Falcaria vulgaris), an intro-duced species native to Europe and Asia, grows as anaggressive weed in some areas of the upper Midwest inthe United States. We are reporting genetic diversityand population structure of sickleweed populationsusing microsatellite markers and nuclear and chloro-plast DNA sequences. Populations showed highgenetic differentiation but did not show significantgeographic structure, suggesting random establish-ment of different genotypes at different sites was likelydue to human mediated multiple introductions. Threegenetic clusters revealed by microsatellite data andpresence of six chlorotypes supported our hypothesisof multiple introductions. Chloroplast DNA sequencedata revealed six chlorotypes nested into two mainlineages suggesting at least two introductions ofsickleweed in the upper Midwest. Some individualsexhibited more than two alleles at several microsatel-lite loci suggesting occurrence of polyploidy, whichcould be a post-introduction development to mitigatethe inbreeding effects. High genetic variation in theintroduced range attributable to multiple introductionsand polyploidy may be inducing the evolution ofinvasiveness in sickleweed. Results of this studyprovide valuable insights into the evolution of sickle-weed and baseline data for designing proper manage-ment practices for controlling sickleweed in the UnitedStates.\n","date":1393048800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393048800,"objectID":"d30bd74cc032791d52980d57c0edeb1d","permalink":"/achalneupane.github.io/publication/piya_et_al_biological_invasions/","publishdate":"2014-02-22T00:00:00-06:00","relpermalink":"/achalneupane.github.io/publication/piya_et_al_biological_invasions/","section":"publication","summary":"Abstract\nSickleweed (Falcaria vulgaris), an intro-duced species native to Europe and Asia, grows as anaggressive weed in some areas of the upper Midwest inthe United States. We are reporting genetic diversityand population structure of sickleweed populationsusing microsatellite markers and nuclear and chloro-plast DNA sequences. Populations showed highgenetic differentiation but did not show significantgeographic structure, suggesting random establish-ment of different genotypes at different sites was likelydue to human mediated multiple introductions. Threegenetic clusters revealed by microsatellite data andpresence of six chlorotypes supported our hypothesisof multiple introductions.","tags":null,"title":"Genetic diversity and population structure of sickleweed(Falcaria vulgaris; Apiaceae) in the upper Midwest USA","type":"publication"},{"authors":["**Achal Neupane**","Madhav P Nepal","Benjamin V Benson","Kenton J macArthur","Sarbottam Piya"],"categories":null,"content":"Abstract\nMitogen-Activated Protein Kinase (MAPK) genes encode proteins that mediate various signaling pathways associated with biotic and abiotic stress responses in eukaryotes. The MAPK genes form a 3-tier signal transduction cascade between cellular stimuli and physiological responses. Recent identification of soybean MAPKs and availability of genome sequences from other legume species allowed us to identify their MAPK genes. The main objectives of this study were to identify MAPKs in 3 legume species, Lotus japonicus, Medicago truncatula, and Phaseolus vulgaris, and to assess their phylogenetic relationships. We used approaches in comparative genomics for MAPK gene identification and named the newly identified genes following Arabidopsis MAPK nomenclature model. We identified 19, 18, and 15 MAPKs and 7, 4, and 9 MAPKKs in the genome of Lotus japonicus, Medicago truncatula, and Phaseolus vulgaris, respectively. Within clade placement of MAPKs and MAPKKs in the 3 legume species were consistent with those in soybean and Arabidopsis. Among 5 clades of MAPKs, 4 founder clades were consistent to MAPKs of other plant species and orthologs of MAPK genes in the fifth clade-\u0026ldquo;Clade E\u0026rdquo; were consistent with those in soybean. Our results also indicated that some gene duplication events might have occurred prior to eudicot-monocot divergence. Highly diversified MAPKs in soybean relative to those in 3 other legume species are attributable to the polyploidization events in soybean. The identification of the MAPK genes in the legume species is important for the legume crop improvement; and evolutionary relationships and functional divergence of these gene members provide insights into plant genome evolution.\n","date":1385964000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385964000,"objectID":"f8fce080fe9b4b2193a50dd6ba7be66f","permalink":"/achalneupane.github.io/publication/mapk_legume/","publishdate":"2013-12-02T00:00:00-06:00","relpermalink":"/achalneupane.github.io/publication/mapk_legume/","section":"publication","summary":"Abstract\nMitogen-Activated Protein Kinase (MAPK) genes encode proteins that mediate various signaling pathways associated with biotic and abiotic stress responses in eukaryotes. The MAPK genes form a 3-tier signal transduction cascade between cellular stimuli and physiological responses. Recent identification of soybean MAPKs and availability of genome sequences from other legume species allowed us to identify their MAPK genes. The main objectives of this study were to identify MAPKs in 3 legume species, Lotus japonicus, Medicago truncatula, and Phaseolus vulgaris, and to assess their phylogenetic relationships.","tags":null,"title":"Evolutionary history of mitogen-activated protein kinase (MAPK) genes in Lotus, Medicago, and Phaseolus","type":"publication"},{"authors":["**Achal Neupane**","Madhav P Nepal","Sarbottam Piya","Senthil Subramanian","Jai S Rohila","R Neil Reese","Benjamin V Benson"],"categories":null,"content":"Abstract\nMitogen-activated protein kinase (MAPK) genes in eukaryotes regulate various developmental and physiological processes including those associated with biotic and abiotic stresses. Although MAPKs in some plant species including Arabidopsis have been identified, they are yet to be identified in soybean. Major objectives of this study were to identify GmMAPKs, assess their evolutionary relationships, and analyze their functional divergence. We identified a total of 38 MAPKs, eleven MAPKKs, and 150 MAPKKKs in soybean. Within the GmMAPK family, we also identified a new clade of six genes: four genes with TEY and two genes with TQY motifs requiring further investigation into possible legume-specific functions. The results indicated the expansion of the GmMAPK families attributable to the ancestral polyploidy events followed by chromosomal rearrangements. The GmMAPK and GmMAPKKK families were substantially larger than those in other plant species. The duplicated GmMAPK members presented complex evolutionary relationships and functional divergence when compared to their counterparts in Arabidopsis. We also highlighted existing nomenclatural issues, stressing the need for nomenclatural consistency. GmMAPK identification is vital to soybean crop improvement, and novel insights into the evolutionary relationships will enhance our understanding about plant genome evolution.\n","date":1379826000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379826000,"objectID":"65a2af4d7063178ef3c02f46c3c421c0","permalink":"/achalneupane.github.io/publication/mapk_soybean/","publishdate":"2013-09-22T00:00:00-05:00","relpermalink":"/achalneupane.github.io/publication/mapk_soybean/","section":"publication","summary":"Abstract\nMitogen-activated protein kinase (MAPK) genes in eukaryotes regulate various developmental and physiological processes including those associated with biotic and abiotic stresses. Although MAPKs in some plant species including Arabidopsis have been identified, they are yet to be identified in soybean. Major objectives of this study were to identify GmMAPKs, assess their evolutionary relationships, and analyze their functional divergence. We identified a total of 38 MAPKs, eleven MAPKKs, and 150 MAPKKKs in soybean.","tags":null,"title":"Identification, Nomenclature, and Evolutionary Relationships of Mitogen-Activated Protein Kinase (MAPK) Genes in Soybean","type":"publication"},{"authors":["**Achal Neupane**","Sarbottam Piya","Neil Reese","Jai Rohila","Senthil Subramanian","Madhav P. Nepal"],"categories":null,"content":"Abstract\nMitogen Activated Protein Kinases (MAPKs) are serine/threonine specific kinases that are induced by various extracellular and intracellular stimuli, and are involved in signaling pathways in all eukaryotes. In plants, MAPKs are known to regulate various signal transduction pathways including those associated with biotic and abiotic stresses. The MAPK gene members belong to three functionally linked gene families called MAPKs, MAPKKs (MAPK Kinases) and MAPKKKs (MAPKK Kinases). Although, MAPKs of model plants such as Arabidopsis and Oryza have been identified and characterized, these genes in soybean (Glycine max) are yet to be identified. In this study, we used approaches in comparative genomics and bioinformatics for the genome-wide identification of MAPKs, MAPKKs and MAPKKKs in soybean. Arabidopsis reference sequences were used in protein BLAST to search the putative MAPKs of soybean from publicly available databases. In our in-silico analysis, the redundant sequences were removed to perform unbiased and rigorous phylogenetic analyses. We verified the presence of unique conserved domains and active sites in each putative MAPK gene member manually and by using Geneious and Pfam programs. From our three different confirmatory analyses, we identified 38 MAP Kinases, 11 MAPK Kinases and 115 MAPKK Kinases in soybean. Arabidopsis nomenclature model was followed to assign numeric subscript to MAPK sequences grouped with Arabidopsis sequences in phylogenetic tree. Lack of effective code of nomenclature lead us to a conundrum, which will be discussed in the meeting. Universal codes of gene nomenclature are crucial for understanding various signaling pathways. Therefore, an effective code of gene nomenclature is warranted. The results from this study will help us characterize soybean MAP Kinases paving avenue for the functional analyses of different cellular and physiological pathways in the order these genes function.\n","date":1341982800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341982800,"objectID":"575b5a59332ef130d008b000b1098d3b","permalink":"/achalneupane.github.io/talk/bsa_neupane/","publishdate":"2012-07-11T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/bsa_neupane/","section":"talk","summary":"Abstract\nMitogen Activated Protein Kinases (MAPKs) are serine/threonine specific kinases that are induced by various extracellular and intracellular stimuli, and are involved in signaling pathways in all eukaryotes. In plants, MAPKs are known to regulate various signal transduction pathways including those associated with biotic and abiotic stresses. The MAPK gene members belong to three functionally linked gene families called MAPKs, MAPKKs (MAPK Kinases) and MAPKKKs (MAPKK Kinases). Although, MAPKs of model plants such as Arabidopsis and Oryza have been identified and characterized, these genes in soybean (Glycine max) are yet to be identified.","tags":null,"title":"Identification of Mitogen Activated Protein Kinase Family Members in Soybean","type":"talk"},{"authors":["Sarbottam Piya","**Achal Neupane**","Jack Butler","Gary Larson","Madhav Nepal"],"categories":null,"content":"Abstract\n","date":1341982800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341982800,"objectID":"f384b4d0b7332258e189e407aa3f967b","permalink":"/achalneupane.github.io/talk/piya_bsa/","publishdate":"2012-07-11T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/piya_bsa/","section":"talk","summary":"Abstract","tags":null,"title":"Population Genetics of *Falcaria vulgaris* (Sickleweed) in North America","type":"talk"},{"authors":["Sarbottam Piya","**Achal Neupane**","Jack Butler","Gary Larson","Madhav Nepal"],"categories":null,"content":"Abstract\n","date":1334293200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1334293200,"objectID":"dc29829aa51379f91031c272068bffed","permalink":"/achalneupane.github.io/talk/piya_sd_academy/","publishdate":"2012-04-13T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/piya_sd_academy/","section":"talk","summary":"Abstract","tags":null,"title":"Introduction history and spread of Falcaria vulgaris Bernh. (Apiaceae) in the United States based on herbarium records","type":"talk"},{"authors":["**Achal Neupane**","Sarbottam Piya","Senthil Subramanian","Jai S. Rohila","Madhav Nepal"],"categories":null,"content":"Abstract\n","date":1332565200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332565200,"objectID":"723bffaa2eefa924f959362cd2ccf00c","permalink":"/achalneupane.github.io/talk/neupane_aspb/","publishdate":"2012-03-24T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/neupane_aspb/","section":"talk","summary":"Abstract","tags":null,"title":"A Nomenclatural Conundrum: Applying Existing Nomenclature to the Identification of Soybean (*Glycine max*) MAP Kinase Genes","type":"talk"},{"authors":["Sarbottam Piya","Madhav P. Nepal","**Achal Neupane**","Jack L. Butler","Gary E. Larson"],"categories":null,"content":"Abstract\nHerbarium records were studied to infer the introduction history and spread of the exotic Eurasian sickleweed (Falcaria vulgaris Bernh.) in the United States. The spread of the plant was reconstructed using the location of early collections as the possible sites of primary introduction, and the location of subsequent collections as potential pathways along which this species spread. Herbarium records indicate that sickleweed was first introduced no later than 1922, and independent introduction of this plant took place in the East Coast and in the Midwest of the United States. The species has spread to 37 counties of 15 states of the United States. No recent sickleweed record has been reported for the last 17 years in the US except Iowa, Nebraska and South Dakota. The plant has been characterized as an aggressive weed by experts in the latter two states, where it is already well established and has infested the Fort Pierre National Grassland and Buffalo Gap National Grassland in South Dakota, and is reported from several sites along Nebraska roadsides. It is essential to verify the existence of sickleweed in the areas from where the herbarium specimens were previously collected to help identify the areas at risk. Control strategies need to be implemented and policy should be developed to establish the participation of public lands managers, transportation departments and private land-owners to control and manage this species before it becomes a more widespread invader.\n","date":1329890400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1329890400,"objectID":"2f3d80b37a554c15605714e9e2f87646","permalink":"/achalneupane.github.io/publication/piya_et_al_proceedings_sd/","publishdate":"2012-02-22T00:00:00-06:00","relpermalink":"/achalneupane.github.io/publication/piya_et_al_proceedings_sd/","section":"publication","summary":"Abstract\nHerbarium records were studied to infer the introduction history and spread of the exotic Eurasian sickleweed (Falcaria vulgaris Bernh.) in the United States. The spread of the plant was reconstructed using the location of early collections as the possible sites of primary introduction, and the location of subsequent collections as potential pathways along which this species spread. Herbarium records indicate that sickleweed was first introduced no later than 1922, and independent introduction of this plant took place in the East Coast and in the Midwest of the United States.","tags":null,"title":"Inferring Introduction History and Spread of Falcaria vulgaris Bernh.(Apiaceae) in the United States Based on Herbarium Records","type":"publication"},{"authors":["**Achal Neupane**","Chezaray Anjorin","Xueqing Song","Deepak Kumar"],"categories":null,"content":"Abstract\nThe goal of this project was to study the responses of antibodies against four influenza A viruses; PR8, RV6, Swine and J1. RV6, Swine and J1 differ from PR8 by 1 amino acid (99.9% similarity), 20 amino acids, and more than 100 amino acids, respectively. Despite several attempts of early diagnosis and chemotherapy, breast cancer is one of the leading causes of cancer deaths in United States claiming almost 40610 lives per year. Breast cancer is more common in white women than in African American women but the survival rate for 5 years for African American women is comparatively less which is 77% than 90 % for white women. Higher concentration of estrogen secretion has also been associated with the risk factors of breast cancer. These viruses resemble the different viruses that circulate in people and that can have varying numbers of amino acid differences, and we wanted to examine how these difference affect the ability of antibodies to bind to different viruses using different techniques. ELISA (Enzyme Linked Immunosorbent Assay) was used to determine whether a particular antibody is present in a blood sample and ELISPOT (ELISPOT-Enzyme-linked Immunospot assay) was used for enumeration of B cells secreting specific antibody. In recent studies, metal-based anticancer drugs are found to be very effective in the death of cancer cells proving it as a very useful cancer chemotherapeutic. In this study we have synthesized ionic triorganotin compounds with increased solubility due to their partially ionic characteristic and tested their anticancer activity using MDA-MB 231 breast cancer cells. Methods: Three compounds (1) Triphenyltin Hydroxideparent compound CA11 and (3) CA 32 derivatives were tested. MDA-MB 231 cells were plated in 96-wellplate and treated with varying concentration (1ng to 100ug) of various triorganotin derivatives for 24, 48 and 72 hours. A combination of trypan blue dye exclusion and WST-1 cell proliferation reagent was used to\n","date":1256792400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1256792400,"objectID":"e7d5723ad441299c2f07dec7888ee1aa","permalink":"/achalneupane.github.io/talk/hbcu_neupane/","publishdate":"2009-10-29T00:00:00-05:00","relpermalink":"/achalneupane.github.io/talk/hbcu_neupane/","section":"talk","summary":"Abstract\nThe goal of this project was to study the responses of antibodies against four influenza A viruses; PR8, RV6, Swine and J1. RV6, Swine and J1 differ from PR8 by 1 amino acid (99.9% similarity), 20 amino acids, and more than 100 amino acids, respectively. Despite several attempts of early diagnosis and chemotherapy, breast cancer is one of the leading causes of cancer deaths in United States claiming almost 40610 lives per year.","tags":null,"title":"Anticancer Activity of Novel Ionic Triorganotin Derivatives against MDA-MB 231 Breast Cancer Cells","type":"talk"}]