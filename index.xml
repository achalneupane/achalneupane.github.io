<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Achal Neupane</title>
    <link>/achalneupane.github.io/</link>
      <atom:link href="/achalneupane.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Achal Neupane</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Mon, 18 Nov 2019 17:26:23 -0500</lastBuildDate>
    <image>
      <url>/achalneupane.github.io/img/icon-192.png</url>
      <title>Achal Neupane</title>
      <link>/achalneupane.github.io/</link>
    </image>
    
    <item>
      <title>STAT_601_Final</title>
      <link>/achalneupane.github.io/post/stat_601_final/</link>
      <pubDate>Mon, 18 Nov 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/stat_601_final/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Final Projects (Fall 2019)- Statistics 601. Due on December 18, 2019 at 5 pm central time.&lt;/p&gt;
&lt;p&gt;Work on this exam by yourself and be sure to reference any material you use on your exam. Only discuss the exam with me, Ms. Fuglsby, or Ms. Hajebi and not the other students in the class.
Do two of the following three problems. Turn in one pdf and RMD file for each problem clearly labeling which problems you have chosen to do.
1: The data shown in schizophrenia.csv were collected in a follow-up study of women patients with schizophrenia and summarized in Davis (2002), Statistical Methods for the Analysis of Repeated Measurements, Springer, New York. The binary response recorded at 0, 2, 6, 8 and 10 months after hospitalization was thought disorder (absent or present). The single covariate is the factor indicating whether a patient had suffered early or late onset of her condition (age of onset less than 20 years or age of onset 20 years or above). The question of interest is whether the course of the illness differs between patients with early and late onset? Investigate the question of interest.
i) Provide a two to three-page write-up (including graphs) explaining your analysis of the experiment and the conclusions you can draw from it.&lt;br /&gt;
ii) As a secondary component provide annotated code that replicates your analysis.
Make sure to discuss any concerns about the modeling assumptions used in your analysis.
The .csv file has the following variables.
subject
- the patient ID, a factor with levels 1 to 44.
onset
- the time of onset of the disease, a factor with levels &amp;lt; 20 yrs and &amp;gt; 20 yrs.
disorder
- whether thought disorder was absent or present, the response variable.
month
- month after hospitalization.&lt;/p&gt;
&lt;p&gt;Please note that you may have already explored this dataset in the class. Even so, please do a complete and extended analysis answering the questions, with the focus of writing and explaining the what you have found in your analysis.&lt;/p&gt;
&lt;p&gt;For this analysis, first I splitted the schizophrenia data into younger and older patients percentage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;schizophrenia2 &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/schizophrenia.csv&amp;quot;, header = TRUE, sep =&amp;quot;,&amp;quot;)

cat(&amp;quot;Calculating percentage data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calculating percentage data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;younger_percentage &amp;lt;- c( nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 0 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 0))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 2 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 2))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 6 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 6))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 8 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 8))),
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 10 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                           nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 10))))

older_percentage &amp;lt;- c( nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 0 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 0))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 2 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 2))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 6 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;lt; 20 yrs&amp;quot; &amp;amp; month == 6))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 8 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 8))),
                       nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 10 &amp;amp; disorder == &amp;quot;present&amp;quot;))) / 
                         nrow(subset(schizophrenia2, (onset == &amp;quot;&amp;gt; 20 yrs&amp;quot; &amp;amp; month == 10))))

younger_percentage %&amp;gt;% summary() %&amp;gt;% as.matrix() %&amp;gt;% t() %&amp;gt;% knitr::kable(caption = &amp;quot;Summary statistics for presence of younger onset data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary statistics for presence of younger onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.59375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;older_percentage %&amp;gt;% summary() %&amp;gt;% as.matrix() %&amp;gt;% t() %&amp;gt;% knitr::kable(caption = &amp;quot;Summary statistics for presence of older onset data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary statistics for presence of older onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0833333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2520833&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5833333&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x = seq(1:5), y=younger_percentage, type=&amp;quot;l&amp;quot;, col=1, xaxt=&amp;quot;n&amp;quot;, ylim=c(0,0.8),
     xlab=&amp;quot;Months&amp;quot;, ylab=&amp;quot;Percent present&amp;quot;, main = &amp;quot;Percent present by age at Onset&amp;quot;)
lines(older_percentage, col=4)
legend(&amp;quot;topright&amp;quot;, legend=c(&amp;quot;Percent present at Onset &amp;lt; 20 years&amp;quot;, &amp;quot;Percent Present at Onset &amp;gt; 20 years&amp;quot;), 
       col=   c(&amp;quot;1&amp;quot;, &amp;quot;4&amp;quot;), lty = c(1, 1))
axis(side=1, at=seq(1:5), labels = c(0, 2, 6, 8, 10))
axis(side=2, at=seq(1:11), labels = c(0, 0.1, .2, .3, .4, .5, .6, .7, .8, .9, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this analysis, we see that for the older-onset population, the mean and the median of the percentage of &lt;code&gt;presence of schizophrenia&lt;/code&gt; are lower. Looking at the percentage plot, it appears that at each sample date (in months) the percentage is lower in the older-onset population.
Looking at the graph of the precentages, it looks like the precentage is lower in the older-onset popullation at every sample date (in months). Both of these seem to indicate that the question is worth investigating more closely.
The mosaic plot shows an increase of ‘absence’ for both populations, through 8 months. Then, a light diminishing of absences at 10 months.&lt;/p&gt;
&lt;p&gt;We can then use GEE approach to further investigate the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GEE Independent (identity)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.7889&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GEE Exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.6867&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GEE Unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;164.011&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In exchangeable and unstructured model, the QIC score is the lowest. So for the selection of model, either model can be a good choice. However, comparing parameters such as CIC, QICu and QICC, I think the exchangeable model seems better.&lt;/p&gt;
&lt;p&gt;Additionally, we can also use linear mixed effects model (lmer) and develop different models by selecting different variables, and then compare our models.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00345048 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Without interaction term:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  179.591302394191&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  184.624563351416&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;With interaction term: month v. onset:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  184.587333241198&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  189.620892549092&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see whether which models works best in our case, we can do model comparison with ANOVA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (no intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.117087&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0774174&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Interaction Term v. No Interaction Term (fixed slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.079143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7784622&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope, No interaction Term v. Random Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.195931&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1579996&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Random Slope, No interaction term v. Fixed Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.037944&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0247979&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## No interaction term v. Interaction term (random slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0788446&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7788693&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (with intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.116788&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.077429&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model with a fixed effect and no interaction term has the lowest BIC score. This would be an indicator that the concept of contact is not likely to add much to the template if anything. The most significant difference based on ANOVA analysis is between random slope, no interaction term and fixed slope, with interaction term (fourth table). This is indicated with a p-score of 0.025 i.e., [P&amp;lt;0.05].&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;(Vole Data)- Consider the “microtus” dataset in the “Flury” library in R.
Background from Airoldi_Flury_Salvioni_JTheorBiol_1995: Discrimination Between Two Species of Microtus using both Classified and Unclassified Observations.
“1. Introduction
Microtus subterraneus and M. multiplex are now considered to be two distinct species (Niethammer, 1982; Krapp, 1982), contrary to the older view of Ellerman &amp;amp; Morrison-Scott (1951). The two species differ in the number of chromosomes: 2n=52 or 54 for M. subterraneus, and 2n=46 or 48 for M. multiplex. Hybrids from the laboratory have reduced fertility (Meylan, 1972), and hybrids from the field, whose karyotypes would be clearly recognizable, have never been found (Krapp, 1982).
The geographic ranges of distribution of M. subterraneus and M. multiplex overlap to some extent in the Alps of southern Switzerland and northern Italy (Niethammer, 1982; Krapp, 1982). M. subterraneus is smaller than M. multiplex in most measurements, and occurs at elevations from 1000 m to over 2000 m, except in the western part of its range (for example, Belgium and Brittany), where it is found in lower elevations. M. multiplex is found at similar elevations, but also at altitudes from 200–300 m south of the Alps (Ticino, Toscana).
The two chromosomal types of M. subterraneus can be crossed in the laboratory (Meylan, 1970, 1972), but no hybrids have so far been found in the field. In M. multiplex, the two chromosomal types show a distinct distribution range, but they are morphologically indistinguishable, and a hybrid has been found in the field (Storch &amp;amp; Winking, 1977).
No reliable criteria based on cranial morphology have been found to distinguish the two species. Saint Girons (1971) pointed out a difference in the sutures of the posterior parts of the premaxillary and nasal bones compared to the frontal one, but this criterion does not work well in many cases. For both paleontological and biogeographical research it would be useful to have a good rule for discriminating between the two species, because much of the data available are in form of skull remains, either fossilized or from owl pellets.
The present study was initiated by a data collection consisting of eight morphometric variables measured by one of the authors (Salvioni) using a Nikon measure-scope (accuracy 1/1000 mm) and dial calipers (accuracy 1/100 mm). The sample consists of 288 specimens collected mostly in Central Europe (Alps and Jura mountains) and in Toscana. One peculiar aspect of this data set is that the chromosomes of 89 specimens were analyzed to identify the species. Only the morphometric characteristics are available for the remaining 199 specimens…”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Develop a model from the 89 specimens that you can use to predict the group membership of the remaining 199 specimens’.
a) Explain your GLM and assess the quality of the fit with the classified observations.&lt;br /&gt;
You may want to use Cross Validation to predict the accuracy of your model.
b) Provide a two to three-page write-up (including graphs) explaining your analysis of the dataset and your recommendations on the usefulness of your predictions.
c) Provide predictions for the unclassified observations.&lt;br /&gt;
d) As a secondary component provide annotated code that replicates your analysis.&lt;/p&gt;
&lt;p&gt;The microtus data set for this final contains 288 observations. Only 89 of these obs are classified as either Subterraneus or Multiplex. The other 199 obs are unknown. The data set has 8 variables that are all measurements of the two possible species. The objective of this final project will be to predict the species of the unknown obs by modeling the 89 classified obs.&lt;/p&gt;
&lt;p&gt;The microtus dataset for this final assignement has 288 rows and 9 columns. We are using 89 of these observations which are labeled as either Subterraneus or Multiplex and the rest of the 199 observartions are unknown. We are splitting our data into known and unknown groups for prediction purposes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR2&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked _by_ &amp;#39;.GlobalEnv&amp;#39;:
## 
##     schizophrenia2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked _by_ &amp;#39;.GlobalEnv&amp;#39;:
## 
##     schizophrenia2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:HSAUR2&amp;#39;:
## 
##     agefat, aspirin, BtheB, gardenflowers, GHQ, HSAURtable, mastectomy,
##     polyps3, pottery, respiratory, schooldays, womensrole&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: splines&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: RcmdrMisc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: car&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;car&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     recode&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: sandwich&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: effects&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lattice theme set by effectsTheme()
## See ?effectsTheme for details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The Commander GUI is launched only in interactive sessions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type &amp;#39;citation(&amp;quot;pROC&amp;quot;)&amp;#39; for a citation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;pROC&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     cov, smooth, var&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;GGally&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     nasa&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, we check the relationship between the variables in our data using ggpairs plot where different classes are shown in different colours. Based on this plot (looking at the lower left points), we can say that the all of these variables are somewhat correlated which can be confirmed by their corresponding values in the upper right. Another interesting aspect of this plot is the density distribution which shows us the distribution of suberraneus, multiplex and the unknown groups, and also whether the variables shown here appear similar or different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ############################
# Now we do logistic regression of our Model followed by AIC and p value checking


model=glm(Group~M1Left+Height+Foramen,data=known,family=binomial)
AIC(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 29.09982&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Group ~ M1Left + Height + Foramen, family = binomial, 
##     data = known)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.20430  -0.01969   0.01008   0.10191   1.24456  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept) 80.578764  31.313670   2.573  0.01007 * 
## M1Left      -0.042164   0.014111  -2.988  0.00281 **
## Height      -0.026826   0.028312  -0.948  0.34336   
## Foramen      0.004990   0.003317   1.504  0.13253   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.28  on 88  degrees of freedom
## Residual deviance:  21.10  on 85  degrees of freedom
## AIC: 29.1
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# MSE of the model
MSE.glm &amp;lt;- mean((predict(model, newdata = known, type = &amp;quot;response&amp;quot;)-known$Group)^2)
MSE.glm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03622752&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Cross validation with 10 fold: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross validation with 10 fold:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;boot&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:car&amp;#39;:
## 
##     logit&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(cv.err.10 &amp;lt;- mean(cv.glm(known, model, K = 10)$delta))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04652552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My logistic regression model (GLM) with formula &lt;code&gt;Group~M1Left+Height+Foramen&lt;/code&gt; has an MSE(Mean square error) of 0.036 which was quite similar to the error values by Cross validation with 10 fold (0.051). The AIC calculated for this model was 29.09982. I then used stepwise regression below to enquire whether this model(and variables in the model) was the best model I could choose from.&lt;/p&gt;
&lt;p&gt;Now, we can also construct the classification tree by selecting all variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(party)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: modeltools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: stats4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;modeltools&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:car&amp;#39;:
## 
##     Predict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     refit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: strucchange&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: zoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;zoo&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     as.Date, as.Date.numeric&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(caret)
# model &amp;lt;- train(
#   factor(Group) ~., data = known, method = &amp;quot;ctree2&amp;quot;,
#   trControl = trainControl(&amp;quot;cv&amp;quot;, number = 10),
#   tuneGrid = expand.grid(maxdepth = 3, mincriterion = 0.95 )
#   )
# 
# plot(model$finalModel)

p1.1.tree &amp;lt;- rpart(Group~.,data=known,control = rpart.control( minsplit = 10))

library(party)
library(partykit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: libcoin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;partykit&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:party&amp;#39;:
## 
##     cforest, ctree, ctree_control, edge_simple, mob, mob_control,
##     node_barplot, node_bivplot, node_boxplot, node_inner, node_surv,
##     node_terminal, varimp&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.party(p1.1.tree),
     tp_args = list(id = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # Make predictions on the test data
# library(dplyr)
# predicted.classes &amp;lt;- model %&amp;gt;% predict(uk)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In above figure, using a tree, I want to see what variables are being chosen. These results seemed to follow along with the summary results from the model seleted below with step regression which is why the step model seems better than my GLM above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: randomForest&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MuMIn&amp;#39;:
## 
##     importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit=randomForest(factor(Group)~.,data=known)
varImpPlot(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Stat_601_FINAL_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, based on random forest model, I want to see what variables are being chosen. Based on this, Foramen seems to be the least iportant while M1left seems to the most important variables. Here, Mean DecreaseinGini is the average (mean) of a variable’s total decrease in node impurity, weighted by the proportion of samples reaching that node in each individual decision tree in the random forest. A higher Mean Decrease in Gini indicates higher variable importance.&lt;/p&gt;
&lt;p&gt;In an attempt to improve selection of variables in my model above, I performed a stepwise selection of all variables. This process determined that the lowest AIC (27.70) was found when using &lt;code&gt;M1Left + M3Left + Foramen + Length + Height&lt;/code&gt;. This model indicates that the omission of Foramen and Length as in my original model is not a good idea. Nonetheless the error for this model is even smaller (0.027) than what we had for original model, and with a much improved cross validated error of .048.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#use stepwise regression for the glm
step_known &amp;lt;- step(glm(Group ~., data = known, family = &amp;quot;binomial&amp;quot;), direction=&amp;quot;both&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start:  AIC=32.96
## Group ~ M1Left + M2Left + M3Left + Foramen + Pbone + Length + 
##     Height + Rostrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## - M2Left   1   14.965 30.965
## - Pbone    1   15.288 31.288
## - Rostrum  1   15.627 31.627
## &amp;lt;none&amp;gt;         14.962 32.962
## - Length   1   17.330 33.330
## - Height   1   18.744 34.744
## - Foramen  1   19.434 35.434
## - M3Left   1   20.654 36.654
## - M1Left   1   40.753 56.753&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Step:  AIC=30.97
## Group ~ M1Left + M3Left + Foramen + Pbone + Length + Height + 
##     Rostrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## - Pbone    1   15.306 29.306
## - Rostrum  1   15.627 29.627
## &amp;lt;none&amp;gt;         14.965 30.965
## - Length   1   18.268 32.268
## - Height   1   18.945 32.945
## + M2Left   1   14.962 32.962
## - Foramen  1   19.965 33.965
## - M3Left   1   20.763 34.763
## - M1Left   1   42.436 56.436&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Step:  AIC=29.31
## Group ~ M1Left + M3Left + Foramen + Length + Height + Rostrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## - Rostrum  1   15.703 27.703
## &amp;lt;none&amp;gt;         15.306 29.306
## - Length   1   18.625 30.625
## - Height   1   18.951 30.951
## + Pbone    1   14.965 30.965
## + M2Left   1   15.288 31.288
## - M3Left   1   20.855 32.855
## - Foramen  1   21.418 33.418
## - M1Left   1   42.970 54.970&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Step:  AIC=27.7
## Group ~ M1Left + M3Left + Foramen + Length + Height&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Df Deviance    AIC
## &amp;lt;none&amp;gt;         15.703 27.703
## - Length   1   18.960 28.960
## - Height   1   19.019 29.019
## + Rostrum  1   15.306 29.306
## + Pbone    1   15.627 29.627
## + M2Left   1   15.694 29.694
## - M3Left   1   21.039 31.039
## - Foramen  1   21.463 31.463
## - M1Left   1   46.843 56.843&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# step_known &amp;lt;- step(glm(Group ~., data = known, family = &amp;quot;binomial&amp;quot;),trace=F, direction=&amp;quot;both&amp;quot;)

#extract the formula with the lowest aic
form_known &amp;lt;- formula(step_known)
form_known&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Group ~ M1Left + M3Left + Foramen + Length + Height&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_step &amp;lt;- glm(Group ~ M1Left + M3Left + Foramen + Length + Height, data=known,family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(model_step)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27.70264&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model_step)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Group ~ M1Left + M3Left + Foramen + Length + Height, 
##     family = binomial, data = known)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.26335  -0.00138   0.00013   0.05223   1.14144  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept) 187.830585 101.914533   1.843   0.0653 .
## M1Left       -0.058382   0.026760  -2.182   0.0291 *
## M3Left        0.024869   0.016656   1.493   0.1354  
## Foramen       0.011898   0.007164   1.661   0.0968 .
## Length       -0.041467   0.029516  -1.405   0.1600  
## Height       -0.092972   0.071107  -1.307   0.1910  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.279  on 88  degrees of freedom
## Residual deviance:  15.703  on 83  degrees of freedom
## AIC: 27.703
## 
## Number of Fisher Scoring iterations: 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# MSE from step regression best model
MSE.glm_step &amp;lt;- mean((predict(model_step, newdata = known, type = &amp;quot;response&amp;quot;)-known$Group)^2)
MSE.glm_step&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02740134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Cross validation with 10 fold: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross validation with 10 fold:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(cv.err_step.10 &amp;lt;- mean(cv.glm(known, model_step, K = 10)$delta))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: algorithm did not converge&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06012331&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now finally, the best model that was selected after step regression will be used to predict the rest of the data set that is unclassified (the unknown data: uk). The AIC for this model was 27.70 which is smaller (thus better) than our original model. Also, the MSE for this model was only 0.027 which is smaller (thus better) than our original model. Additionally, the cross validation error was 048 which is a bit higher than our MSE for this model and, but still better than the CV of our orignal model. Anyway, I will be selcting this model with AIC 0.27 from obtained step regression for the prediction purpose.&lt;/p&gt;
&lt;p&gt;The specific observation after predictions using the model will be shown below. I will also show a table of count of each species class at the bottom.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##################################################
# ### ROC curve
# # source: https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/
# model=glm(Group~M1Left+Height+Foramen, data=known, family=binomial)
# pred=predict(model,data=known, type=&amp;quot;response&amp;quot;)
# (roc(known$Group,pred))$auc ## this gives the area under curve
# plot(roc(known$Group,pred))

#######################
## Final Model
uk=subset(mc,Group==2)
known=subset(mc,Group!=2)

# Using chosen model from stepwise regression
Final_formula=Group~M1Left+M2Left+M3Left+Foramen+Height
model=glm(Final_formula,data=known,family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred=predict(model,newdata=uk,type=&amp;quot;response&amp;quot;)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Final_formula, family = binomial, data = known)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.46039  -0.00606   0.00410   0.09077   1.56483  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept) 121.314960  49.465349   2.453  0.01419 * 
## M1Left       -0.049655   0.017862  -2.780  0.00544 **
## M2Left       -0.017500   0.017327  -1.010  0.31251   
## M3Left        0.012076   0.007616   1.586  0.11283   
## Foramen       0.006361   0.003937   1.615  0.10620   
## Height       -0.060146   0.044815  -1.342  0.17957   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.279  on 88  degrees of freedom
## Residual deviance:  17.754  on 83  degrees of freedom
## AIC: 29.754
## 
## Number of Fisher Scoring iterations: 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(j in 1:nrow(uk)){
  if(pred[j]&amp;lt;0.5)
  {uk[j,1]=&amp;quot;multiplex&amp;quot;}
  if(pred[j]&amp;gt;=0.5){uk[j,1]=&amp;quot;subterraneous&amp;quot;}
}

cat(&amp;quot;This is the first 6 rows of the predictions for unclassified observations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is the first 6 rows of the predictions for unclassified observations&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(uk,6) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;M1Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;M2Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;M3Left&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Foramen&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pbone&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Height&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rostrum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1841&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1562&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1585&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5024&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;821&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;91&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;subterraneous&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1770&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1459&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1542&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3856&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4542&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;755&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;405&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;subterraneous&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1785&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1573&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4165&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3928&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;767&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;425&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;93&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2095&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1660&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1870&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3937&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5218&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2355&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;842&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;490&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1976&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1666&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1704&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4058&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;814&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;481&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;multiplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1980&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1643&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1950&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3569&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6020&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2355&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;815&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;460&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Count of predicted class:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Count of predicted class:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(uk$Group)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;multiplex&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;subterraneous&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;128&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ## decouple the predictions 
# uk_multi=subset(uk, Group==&amp;quot;multiplex&amp;quot;)
# uk_subten=subset(uk, Group==&amp;quot;subterraneous&amp;quot;)
# 
# nrow(uk_multi) ## number of samples predicted as multiplex
# nrow(uk_subten) ## number of samples predicted as subterraneous
# 
# rownames(uk_multi) ## sample numbers of the multiplex classified 
# rownames(uk_subten) ## sample numbers of the subterraneous classified 
############################################################################&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Data Analysis and Mixed Models II</title>
      <link>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_ii/</link>
      <pubDate>Fri, 08 Nov 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_ii/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the  data from the  package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Investigate the use of other correlational structures than the independence and exchangeable structures used in the text for the respiratory data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which model is the best? Compare the following models: independent, exchangable, and what ever models you tried in part (a). Justify your answer. (Hint: use QIC (in ), MSE, misclassification rate, comparison of naive vs robust Z-score, or another method, be sure to state your method)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1a.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gee)
library(geepack)
library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MuMIn)
library(multcomp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: TH.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:HSAUR3&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;respiratory&amp;quot;, package=&amp;quot;HSAUR3&amp;quot;)
head(respiratory)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     centre treatment gender age status month subject
## 1        1   placebo female  46   poor     0       1
## 112      1   placebo female  46   poor     1       1
## 223      1   placebo female  46   poor     2       1
## 334      1   placebo female  46   poor     3       1
## 445      1   placebo female  46   poor     4       1
## 2        1   placebo female  28   poor     0       2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(respiratory)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 555   7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub &amp;lt;- subset(respiratory, month &amp;gt; &amp;quot;0&amp;quot;)
resp_sub$baseline &amp;lt;- rep(subset(respiratory, month == &amp;quot;0&amp;quot;)$status,rep(4, 111))

resp_sub$nstat &amp;lt;- as.numeric(resp_sub$status == &amp;quot;good&amp;quot;)
resp_sub$month &amp;lt;- resp_sub$month[, drop = TRUE]
head(resp_sub, n = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     centre treatment gender age status month subject baseline nstat
## 112      1   placebo female  46   poor     1       1     poor     0
## 223      1   placebo female  46   poor     2       1     poor     0
## 334      1   placebo female  46   poor     3       1     poor     0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(resp_sub)[names(resp_sub) == &amp;quot;treatment&amp;quot;] &amp;lt;- &amp;quot;trt&amp;quot;
levels(resp_sub$trt)[2] &amp;lt;- &amp;quot;trt&amp;quot;

cat(&amp;quot;Now, fitting the models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Now, fitting the models&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_glm &amp;lt;- glm(status ~ centre + trt + gender + baseline + age, data = resp_sub, family = &amp;quot;binomial&amp;quot;)

resp_sub_gee1 &amp;lt;- gee(nstat ~ centre + trt+ gender + baseline + age, data = resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;independence&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_gee2 &amp;lt;- gee(nstat ~ centre + trt + gender + baseline + age, data =resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;exchangeable&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_gee3 &amp;lt;- gee(nstat ~ centre + trt + gender + baseline + age, data =resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;unstructured&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resp_sub_gee4 &amp;lt;- gee(nstat ~ centre + trt + gender + baseline + age, data =resp_sub, family = &amp;quot;binomial&amp;quot;, id = subject, corstr = &amp;quot;AR-M&amp;quot;, scale.fix = TRUE, scale.value = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
## running glm to get initial regression estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      centre2       trttrt   gendermale baselinegood          age 
##  -0.90017133   0.67160098   1.29921589   0.11924365   1.88202860  -0.01816588&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot; Summary&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_glm)$coeff, caption = &amp;quot;Summary coeff of GLM binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GLM binomial&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;z value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|z|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9001713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3376530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.6659658&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0076767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6716010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2395666&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.8034004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0050567&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2992159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2368410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.4856047&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1192436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2946710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4046671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6857223&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8820286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2412902&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.7998563&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0181659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0088644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.0493065&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0404322&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee1)$coef, caption = &amp;quot;Summary coeff of GEE binomial independent&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial independent&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9001713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3376531&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.665965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4603270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9555041&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6716010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2395666&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.803400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3568191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8821889&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2992159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2368410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.485603&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3507780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.7038127&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1192436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2946710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.404667&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4432023&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2690501&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8820286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2412902&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.799855&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3500515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.3764332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0181659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0088644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.049306&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0130043&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3969169&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee2)$coef, caption = &amp;quot;Summary coeff of GEE binomial exchangeable&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9001713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4784634&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.8813796&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4603270&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9555041&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6716010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3394723&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9783676&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3568191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8821889&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2992159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3356101&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8712064&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3507780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.7038127&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1192436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4175568&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2855747&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4432023&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2690501&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8820286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3419147&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5043802&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3500515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.3764332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0181659&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0125611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.4462014&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0130043&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3969169&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee3)$coef, caption = &amp;quot;Summary coeff of GEE binomial unstructured&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9312798&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4791852&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9434655&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4612499&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.0190352&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6727947&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3390779&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9841895&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3548202&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8961568&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2789154&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3354409&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8126404&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3494500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.6597956&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0946735&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4172964&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2268736&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4436295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2134068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9346252&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3428184&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.6432949&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3480468&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5585200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0168892&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0125574&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3449620&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0129054&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3086948&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(summary(resp_sub_gee4)$coef, caption = &amp;quot;Summary coeff of GEE binomial AR-M&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Summary coeff of GEE binomial AR-M&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Naive z&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust S.E.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Robust z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9629448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4455161&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.1614142&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4611607&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.0880894&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;centre2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7427015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3146448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.3604438&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3562300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.0848932&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;trttrt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2472824&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3112665&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.0071210&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3518974&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.5444492&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;gendermale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1132323&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3883671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2915599&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4494506&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2519348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;baselinegood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9113953&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3167921&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0335952&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3501873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.4582088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0169164&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0116652&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.4501525&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0129273&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.3085816&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the difference between Naive z and Robust z with relevant variables for the above models, resp_sub_gee3 with unstructured comparison correlation structure model tends to have a minimal difference between Naive z and Robust z than others. Therefore, in this case, unstructured model fits best.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;b. Which model is the best? Compare the following models: independent, exchangable, and what ever models you tried in part (a). Justify your answer. (Hint: use QIC (in \textbf{MESS}), MSE, misclassification rate, comparison of naive vs robust Z-score, or another method, be sure to state your method)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MESS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MuMIn&amp;#39;:
## 
##     QIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE Independent&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE independent&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;508.5300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.2182&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.6091&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.6559&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;508.7222&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE Exchangeable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.795035&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.218168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.609084&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.288434&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;496.051916&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE Unstructured&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE Unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.741778&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.336132&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.668066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.202823&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;496.465676&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fitting GEE ar1&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;QIC from GGE ar1&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;496.834891&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.642425&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-241.821212&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.596233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;497.091771&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As mentioned above in 1a., resp_sub_gee3 with unstructured correlation structure model has slightly minimal difference of Naive z and Robust z than others. I used the QIC value to compare the result, it looks like the third model with unstructured correlation structure model has a lower QIC value of 495.7418 than the other three models. So, the model with a corr unstructured. So, the model with unstructured correlation structure is the best among four.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The data set  from  package were collected in a follow-up study of women patients with schizophrenia (Davis, 2002). The binary response recorded at 0, 2, 6, 8 and 10 months after hospitalization was ``thought disorder’’ (absent or present). The single covariate is the factor indicating whether a patient had suffered early or late onset of her condition (age of onset less than 20 years or age of onset 20 years or above). The question of interest is whether the course of the illness differs between patients with early and late onset schizophrenia. (&lt;a href=&#34;https://www.rdocumentation.org/packages/HSAUR3/versions/1.0-9/topics/schizophrenia2&#34; class=&#34;uri&#34;&gt;https://www.rdocumentation.org/packages/HSAUR3/versions/1.0-9/topics/schizophrenia2&lt;/a&gt;)
Investigate this question using&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;plots and summary statistics&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the GEE approach&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;mixed effects model (lmer) from previous chapter&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is there a difference? What model(s) work best? Describe your results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2a.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calculating percentage data&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 3: &lt;/span&gt;Summary statistics for presence of younger onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.34375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.59375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 3: &lt;/span&gt;Summary statistics for presence of older onset data&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Min.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1st Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3rd Qu.&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0833333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2520833&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5833333&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_II_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;For the older-onset population, the mean and the median of the percentage of’ presence of schizophrenia’ are lower. Looking at the percentage plot, it appears that at each sample date (in months) the percentage is lower in the older-onset population.&lt;/p&gt;
&lt;p&gt;2b. the GEE approach&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 4: &lt;/span&gt;QIC from GEE Independent (identity)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.788881&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;159.831542&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-75.915771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.978669&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;164.046945&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 4: &lt;/span&gt;QIC from GEE Exchangeable&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;159.800970&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.156189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-76.078094&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.822391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.190581&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 4: &lt;/span&gt;QIC from GEE Unstructured&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;QIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.480447&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICu&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.652702&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Quasi Lik&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-76.326351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;CIC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.913873&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;params&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;QICC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.956957&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;In exchangeable and unstructured model, the QIC score is the lowest. So for the selection of model, either model can be a good choice. However, comparing parameters such as CIC, QICu and QICC, I think the exchangeable model seems better.&lt;/p&gt;
&lt;p&gt;2c. mixed effects model (lmer) from previous chapter&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00345048 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Without interaction term:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  179.591302394191&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  184.624563351416&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;With interaction term: month v. onset:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Fixed Slope :  184.587333241198&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BIC: Random Slope:  189.620892549092&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Answer: Please see the outputs from lmer&lt;/p&gt;
&lt;p&gt;2d
&lt;strong&gt;Answer to 2d: Model comparison with ANOVA&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (no intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.117087&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0774174&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Interaction Term v. No Interaction Term (fixed slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.079143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7784622&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope, No interaction Term v. Random Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.5913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.57013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.195931&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1579996&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Random Slope, No interaction term v. Fixed Slope, With interaction term&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.037944&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0247979&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## No interaction term v. Interaction term (random slope)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.6246&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-72.01159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.0232&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0788446&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7788693&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;## Fixed Slope v. Random Slope (with intaction term)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BIC&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logLik&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;deviance&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chisq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Chi Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;Chisq)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;schiz_lmer3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;163.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184.5873&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-74.53056&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.0611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;schiz_lmer4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;189.6209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.97216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.9443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.116788&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.077429&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;The model with a fixed effect and no interaction term has the lowest BIC score. This would be an indicator that the concept of contact is not likely to add much to the template if anything. The most significant difference based on ANOVA analysis is between random slope, no interaction term and fixed slope, with interaction term (fourth table). This is indicated with a p-score of 0.025 i.e., [P&amp;lt;0.05].&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Data Analysis and Mixed Models I</title>
      <link>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_i/</link>
      <pubDate>Mon, 28 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/longitudinal_data_analysis_and_mixed_models_i/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Following up with the Beat the Blues data from the video (package HSAUR3) do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Construct boxplots to compare the factor variable  in an analogous way to how we constructed boxplots in the video for the treatment variable. Discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat (a) for the  variable. Discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the  function to fit a model to the Beat the Blues data that assumes that the repeated measurements are independent. Compare the results to those from fitting the random intercept model  from the video.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Investigate and discuss whether there is any evidence of an interaction between treatment and time for the Beat the Blues data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Construct a plot of the mean profiles of both treatment groups in the Beat the Blues data, showing also standard deviation bars at each time point.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;gee&amp;quot;)
library(&amp;quot;lme4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Matrix&amp;quot;)
library(&amp;quot;multcomp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: TH.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library (HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:TH.data&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(plyr)
library(tidyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;tidyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# 1.a&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1.a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a. Construct boxplots to compare the factor variable \textbf{drug} in an
# analogous way to how we constructed boxplots in the video for the treatment
# variable. Discuss the results.
data(&amp;quot;BtheB&amp;quot;)

layout(matrix(1:2, nrow=1))

ylim=range(BtheB[ , grep(&amp;quot;bdi&amp;quot;, names(BtheB))],
           na.rm=TRUE)

yes=subset(BtheB, drug==&amp;quot;Yes&amp;quot;)[ , grep(&amp;quot;bdi&amp;quot;, names(BtheB))]
boxplot(yes, main=&amp;quot;Drugs Used&amp;quot;, ylab=&amp;quot;BDI&amp;quot;,
        xlab=&amp;quot;Time (in Months)&amp;quot;, names=c(0,2,3,5,8), 
        ylim=ylim)

no=subset(BtheB, drug==&amp;quot;No&amp;quot;)[ , grep(&amp;quot;bdi&amp;quot;, names(BtheB))]
boxplot(no, main=&amp;quot;Drugs Not Used&amp;quot;, ylab=&amp;quot;BDI&amp;quot;,
        xlab=&amp;quot;Time (in Months)&amp;quot;, names=c(0,2,3,5,8), 
        ylim=ylim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# b. Repeat (a) for the \textbf{length} variable. Discuss the results.

layout(matrix(1:2, nrow = 1))
lengthlow&amp;lt;-subset(BtheB,length==&amp;quot;&amp;lt;6m&amp;quot;)[,grep(&amp;quot;bdi&amp;quot;,names(BtheB))]
boxplot(lengthlow, main =&amp;quot;Length&amp;lt;6m&amp;quot;, ylab= &amp;quot;BDI&amp;quot;, xlab = &amp;quot;Time (in months)&amp;quot;,names=c(0,2,3,5,8))

lengthhigh&amp;lt;-subset(BtheB,length==&amp;quot;&amp;gt;6m&amp;quot;)[,grep(&amp;quot;bdi&amp;quot;,names(BtheB))]
boxplot(lengthhigh, main =&amp;quot;Length&amp;gt;6m&amp;quot;, ylab= &amp;quot;BDI&amp;quot;, xlab = &amp;quot;Time (in months)&amp;quot;,names=c(0,2,3,5,8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.c&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# c. Use the \textit{lm} function to fit a model to the Beat the Blues
# data that assumes that the repeated measurements are independent.
# Compare the results to those from fitting the random intercept model
# \textit{BtheB\_lmer1} from the video.

BtheB$subject=factor(rownames(BtheB))
nobs=nrow(BtheB)
cat(&amp;quot;Let change the data in wide format&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Let change the data in wide format&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_long=reshape(BtheB, idvar = &amp;quot;subject&amp;quot;,
                   varying = c(&amp;quot;bdi.2m&amp;quot;, &amp;quot;bdi.3m&amp;quot;, &amp;quot;bdi.5m&amp;quot;, &amp;quot;bdi.8m&amp;quot;),
                   direction = &amp;quot;long&amp;quot;)
BtheB_long$time=rep(c(2, 3, 5, 8), rep(nobs, 4))


BtheB_lm=lm(bdi ~ bdi.pre + time + treatment + drug + 
              length, data=BtheB_long, na.action=na.omit)

summary(BtheB_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bdi ~ bdi.pre + time + treatment + drug + length, 
##     data = BtheB_long, na.action = na.omit)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -23.6779  -5.4177   0.0151   5.3268  27.2688 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     7.32290    1.72765   4.239 3.08e-05 ***
## bdi.pre         0.57396    0.05497  10.440  &amp;lt; 2e-16 ***
## time           -0.93784    0.23650  -3.965 9.36e-05 ***
## treatmentBtheB -3.32254    1.10069  -3.019  0.00278 ** 
## drugYes        -3.56866    1.14717  -3.111  0.00206 ** 
## length&amp;gt;6m       1.71067    1.11056   1.540  0.12463    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 8.674 on 274 degrees of freedom
##   (120 observations deleted due to missingness)
## Multiple R-squared:  0.395,  Adjusted R-squared:  0.384 
## F-statistic: 35.78 on 5 and 274 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_lmer1=lmer(bdi ~ bdi.pre + time + treatment + drug +
                   length + (1 | subject), data=BtheB_long,
                 REML=FALSE, na.action=na.omit)

cftest(BtheB_lmer1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lmer(formula = bdi ~ bdi.pre + time + treatment + drug + length + 
##     (1 | subject), data = BtheB_long, REML = FALSE, na.action = na.omit)
## 
## Linear Hypotheses:
##                     Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) == 0     5.59239    2.24244   2.494   0.0126 *  
## bdi.pre == 0         0.63968    0.07789   8.212 2.22e-16 ***
## time == 0           -0.70476    0.14639  -4.814 1.48e-06 ***
## treatmentBtheB == 0 -2.32908    1.67036  -1.394   0.1632    
## drugYes == 0        -2.82495    1.72684  -1.636   0.1019    
## length&amp;gt;6m == 0       0.19708    1.63832   0.120   0.9043    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Univariate p values reported)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Comparison of lm and lmer&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Comparison of lm and lmer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(BtheB_lmer1,BtheB_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: BtheB_long
## Models:
## BtheB_lm: bdi ~ bdi.pre + time + treatment + drug + length
## BtheB_lmer1: bdi ~ bdi.pre + time + treatment + drug + length + (1 | subject)
##             Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&amp;gt;Chisq)    
## BtheB_lm     7 2012.3 2037.7 -999.15   1998.3                            
## BtheB_lmer1  8 1887.5 1916.6 -935.75   1871.5 126.8      1  &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;We can plot the residuals from Linear model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## We can plot the residuals from Linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(BtheB_lm, which=2)
# Assumptions for Mixed-model

residuals &amp;lt;- function(object, obs) obs-predict(object) 
cat(&amp;quot;We can also plot the residuals from linear mixed-model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## We can also plot the residuals from linear mixed-model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:2, ncol = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qint &amp;lt;- ranef(BtheB_lmer1)$subject[[&amp;quot;(Intercept)&amp;quot;]]
qres &amp;lt;- residuals(BtheB_lmer1,BtheB_long$bdi.pre)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in obs - predict(object): longer object length is not a multiple of
## shorter object length&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqnorm(qint, ylab = &amp;quot;Estimated random intercepts&amp;quot;,
     xlim = c(-3, 3), ylim = c(-20, 20),
     main = &amp;quot;Random intercepts&amp;quot;)
qqline(qint, col=&amp;quot;red&amp;quot;, lwd=3)
qqnorm(qres, xlim = c(-3, 3), ylim = c(-20, 20),
     ylab = &amp;quot;Estimated residuals&amp;quot;,
     main = &amp;quot;Residuals&amp;quot;)
qqline(qres, col=&amp;quot;red&amp;quot;, lwd=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.d&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# d. Investigate and discuss whether there is any evidence of an interaction
# between treatment and time for the Beat the Blues data.

cat(&amp;quot;Testing fitted linear model with lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Testing fitted linear model with lm&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_lm2=lm(bdi ~ bdi.pre + time + treatment + drug +
                   length + time*treatment + subject, 
                 data=BtheB_long, REML=FALSE, na.action=na.omit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
##  extra argument &amp;#39;REML&amp;#39; will be disregarded&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(BtheB_lm2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bdi ~ bdi.pre + time + treatment + drug + length + 
##     time * treatment + subject, data = BtheB_long, na.action = na.omit, 
##     REML = FALSE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.142  -2.135   0.000   1.475  18.851 
## 
## Coefficients: (4 not defined because of singularities)
##                      Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)          29.76770    9.56265   3.113 0.002154 ** 
## bdi.pre              -1.65000    0.70622  -2.336 0.020565 *  
## time                 -0.95949    0.21348  -4.494 1.24e-05 ***
## treatmentBtheB      -19.42712   11.68602  -1.662 0.098158 .  
## drugYes               6.00000    3.53108   1.699 0.091000 .  
## length&amp;gt;6m            22.48102   12.10631   1.857 0.064941 .  
## subject10             3.10000    4.52199   0.686 0.493881    
## subject11            19.06898    4.40270   4.331 2.45e-05 ***
## subject12           100.14125   35.32423   2.835 0.005105 ** 
## subject13            18.05000    5.42455   3.327 0.001061 ** 
## subject14            23.31898    5.41836   4.304 2.75e-05 ***
## subject15            15.80000    6.07510   2.601 0.010070 *  
## subject16             5.45000    4.11791   1.323 0.187343    
## subject17            46.96818   12.31014   3.815 0.000186 ***
## subject18            19.78102   12.16794   1.626 0.105760    
## subject19            42.11898    7.12772   5.909 1.67e-08 ***
## subject2             34.65000   11.83833   2.927 0.003862 ** 
## subject20            52.83102   25.34385   2.085 0.038512 *  
## subject21             8.46022    7.50845   1.127 0.261336    
## subject22            33.95000    9.83645   3.451 0.000694 ***
## subject23            14.00000    4.99370   2.804 0.005606 ** 
## subject24             3.67026    8.82128   0.416 0.677853    
## subject25            27.12026    6.73771   4.025 8.36e-05 ***
## subject26            11.81818    5.55378   2.128 0.034694 *  
## subject27             0.95128    6.84960   0.139 0.889699    
## subject28            62.16624   12.24689   5.076 9.51e-07 ***
## subject29            24.00000   14.55901   1.648 0.100992    
## subject3             27.40128    9.26385   2.958 0.003511 ** 
## subject30            12.58102   10.30362   1.221 0.223663    
## subject31            28.60000   10.49865   2.724 0.007078 ** 
## subject32            29.28102   15.42181   1.899 0.059198 .  
## subject33            10.23102    9.57607   1.068 0.286765    
## subject34             1.19125    8.26315   0.144 0.885531    
## subject35            56.95000   20.08684   2.835 0.005101 ** 
## subject36            72.85128   18.52482   3.933 0.000120 ***
## subject37            19.95000    6.66242   2.994 0.003135 ** 
## subject38            13.91898    5.59943   2.486 0.013833 *  
## subject39             3.29920    8.84068   0.373 0.709448    
## subject4             16.25000    6.11601   2.657 0.008590 ** 
## subject40            32.76898    7.02198   4.667 5.94e-06 ***
## subject41           100.19125   33.25146   3.013 0.002956 ** 
## subject42            26.05000    6.07510   4.288 2.93e-05 ***
## subject43           -18.13102   11.53675  -1.572 0.117793    
## subject44            17.51022    6.19857   2.825 0.005261 ** 
## subject45            38.15000   18.69805   2.040 0.042771 *  
## subject46            69.79125   23.95835   2.913 0.004031 ** 
## subject47            61.53102   23.96835   2.567 0.011061 *  
## subject48            10.84579   11.62898   0.933 0.352242    
## subject49            -7.40875    7.23325  -1.024 0.307078    
## subject5             27.71022    9.01742   3.073 0.002447 ** 
## subject50            55.96898    6.58205   8.503 6.75e-15 ***
## subject51            59.90128   16.51807   3.626 0.000374 ***
## subject52            48.24125   19.27469   2.503 0.013206 *  
## subject53            43.46898    4.56947   9.513  &amp;lt; 2e-16 ***
## subject54             3.64920    8.34740   0.437 0.662512    
## subject55            28.08102    7.50271   3.743 0.000244 ***
## subject56             0.63102    7.46940   0.084 0.932767    
## subject57             9.23102    4.40270   2.097 0.037412 *  
## subject58            73.13102   19.55676   3.739 0.000247 ***
## subject59            52.24125   17.72452   2.947 0.003627 ** 
## subject6             -3.36898    7.46940  -0.451 0.652502    
## subject60            -6.08978    5.33361  -1.142 0.255056    
## subject61             7.36898    6.58205   1.120 0.264386    
## subject62            10.01898    6.06957   1.651 0.100536    
## subject63             9.18060    4.07475   2.253 0.025457 *  
## subject64           -19.51709   15.51635  -1.258 0.210070    
## subject65             5.61022    5.42631   1.034 0.302566    
## subject66           -34.50043   16.19268  -2.131 0.034471 *  
## subject67            -0.15000    4.78979  -0.031 0.975052    
## subject68            55.89920   24.10162   2.319 0.021495 *  
## subject69            36.70128   11.36703   3.229 0.001476 ** 
## subject7              2.60000    4.78979   0.543 0.587920    
## subject70            64.84125   21.93711   2.956 0.003534 ** 
## subject71            10.18102    9.70541   1.049 0.295574    
## subject72            54.96818   19.13108   2.873 0.004549 ** 
## subject73           -24.20000   13.65756  -1.772 0.078092 .  
## subject74            -4.99872    6.27981  -0.796 0.427075    
## subject75            -3.33102   11.44996  -0.291 0.771446    
## subject76            23.80000    9.18080   2.592 0.010310 *  
## subject77             4.85000    3.23629   1.499 0.135711    
## subject78             6.90000    5.51572   1.251 0.212560    
## subject79            19.10128    9.01832   2.118 0.035536 *  
## subject8              3.31898    7.69955   0.431 0.666937    
## subject80             9.46898    7.12772   1.328 0.185695    
## subject81            20.36898    4.56947   4.458 1.45e-05 ***
## subject82            -0.08978    5.78228  -0.016 0.987630    
## subject83           -18.18102   14.10427  -1.289 0.199028    
## subject84             8.95000    6.07510   1.473 0.142427    
## subject85            82.36818   21.62732   3.809 0.000191 ***
## subject86            40.75000   16.18143   2.518 0.012659 *  
## subject87            16.76022    7.47517   2.242 0.026168 *  
## subject88             5.56898    5.86055   0.950 0.343253    
## subject89            50.53102   23.96835   2.108 0.036387 *  
## subject9             29.28102   13.89048   2.108 0.036408 *  
## subject90            30.93102   16.08662   1.923 0.056078 .  
## subject92            60.94920   22.05961   2.763 0.006320 ** 
## subject93            21.76246   13.32765   1.633 0.104232    
## subject94             3.20000    4.94351   0.647 0.518249    
## subject95                  NA         NA      NA       NA    
## subject96                  NA         NA      NA       NA    
## subject98                  NA         NA      NA       NA    
## subject99                  NA         NA      NA       NA    
## time:treatmentBtheB   0.64358    0.29757   2.163 0.031867 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.994 on 181 degrees of freedom
##   (120 observations deleted due to missingness)
## Multiple R-squared:  0.8675, Adjusted R-squared:  0.7958 
## F-statistic:  12.1 on 98 and 181 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Testing fitted linear mixed-model with lmer subject as a random effect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Testing fitted linear mixed-model with lmer subject as a random effect&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BtheB_lmer2=lmer(bdi ~ bdi.pre + time + treatment + drug +
                   length + time*treatment + (1 | subject), 
                 data=BtheB_long, REML=FALSE, na.action=na.omit)

cftest(BtheB_lmer2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lmer(formula = bdi ~ bdi.pre + time + treatment + drug + length + 
##     time * treatment + (1 | subject), data = BtheB_long, REML = FALSE, 
##     na.action = na.omit)
## 
## Linear Hypotheses:
##                          Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) == 0          6.47824    2.31905   2.793  0.00521 ** 
## bdi.pre == 0              0.64046    0.07843   8.166 2.22e-16 ***
## time == 0                -0.95555    0.20831  -4.587 4.49e-06 ***
## treatmentBtheB == 0      -4.09804    1.98490  -2.065  0.03896 *  
## drugYes == 0             -2.79209    1.73987  -1.605  0.10854    
## length&amp;gt;6m == 0            0.21905    1.65044   0.133  0.89441    
## time:treatmentBtheB == 0  0.49000    0.28959   1.692  0.09064 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Univariate p values reported)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#1.e&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1.e&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# e. Construct a plot of the mean profiles of both treatment groups in the Beat
# the Blues data, showing also standard deviation bars at each time point.

data_sum=ddply(BtheB_long, c(&amp;quot;time&amp;quot;, &amp;quot;treatment&amp;quot;), summarise, 
            N=sum(!is.na(bdi)),
            mean=mean(bdi, na.rm=TRUE),
            sd=sd(bdi, na.rm=TRUE),
            se=sd/sqrt(N))

pd &amp;lt;- position_dodge(0.1)

ggplot(data_sum, aes(x=time, y=mean, colour=treatment)) +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=0.1, position=pd) +
  geom_line(position=pd) +
  geom_point(position=pd) + 
  theme_classic() +
  labs(title = &amp;quot;Mean profiles of both treatment groups&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;1a.&lt;/p&gt;
&lt;p&gt;The plots demonstrate that BDI (median or average) declines at a faster rate they do not take the medication.&lt;/p&gt;
&lt;p&gt;1b.&lt;/p&gt;
&lt;p&gt;The plots demonstrate that when patients live for less than 6 months, BDI (median or average) declines at a higher pace than when patients survive for more than months.&lt;/p&gt;
&lt;p&gt;1c.&lt;/p&gt;
&lt;p&gt;In linear model, Bdi.pre, time, treatmentBtheB, and drugYes are significant, but in the lmer model, only bdi.pre and time are significant. This means that the linear model understates the standard errors because it does not account for the correlation over time or the repeated measures.&lt;/p&gt;
&lt;p&gt;1d.&lt;/p&gt;
&lt;p&gt;In the first output, the linear model shows that there is interation between time and treatment with P &amp;lt; 0.05. However, mixed effect model (lmer) indicates that there is no time and treatment effect because p is 0.09.&lt;/p&gt;
&lt;p&gt;1e.&lt;/p&gt;
&lt;p&gt;The mean profile shows that the results are very different between the two categories, but the standard deviation bars converge.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the  data from the package HSAUR3. This data shows the plasma inorganic phosphate levels for 33 subjects, 20 of whom are controls and 13 of whom have been classified as obese (Davis, 2002). Perform the following on this dataset&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Construct boxplots by group and discuss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce separate plots of the profiles of the individuals in each group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Guided by how these plots fit, which linear mixed effects models do you think might be sensible? (Hint: Discuss intercept and slope, intercept and interaction).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convert the data to long version and fit the model of your choice and discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;phosphate&amp;quot;, package = &amp;quot;HSAUR3&amp;quot;)
library(lme4)
library(Matrix)
library(multcomp)
attach(phosphate)
par(mfrow = c(1,2))
ylim &amp;lt;-range(phosphate[,-1], na.rm = TRUE)
cat(&amp;quot;#2.a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2.a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a. Construct boxplots by group and discuss. 
boxplot(phosphate[group == &amp;quot;control&amp;quot;, -1], 
        xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Phosphate level&amp;quot;,
        main = &amp;quot;base R: Control group&amp;quot;, ylim = ylim)
boxplot(phosphate[group == &amp;quot;obese&amp;quot;,-1],
        xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Phosphate level&amp;quot;,
        main = &amp;quot;Obese group&amp;quot;, ylim = ylim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version:

phosphate %&amp;gt;%
  gather(time,phosphate_level, 2:9) %&amp;gt;%
  mutate(time = factor(gsub(time,pattern=&amp;quot;[[:alpha:]]&amp;quot;,replacement=&amp;#39;&amp;#39;))) %&amp;gt;%
ggplot(aes(x=time,y=phosphate_level)) +
  geom_boxplot() +
  facet_grid(.~group) +
  labs(title=&amp;#39;ggplot: Phosphate Data by Groups&amp;#39;,
       x=&amp;#39;Time (hours)&amp;#39;,
       y=&amp;#39;Phosphate level&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2.b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2.b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# b. Produce separate plots of the profiles of the individuals in each group
layout(matrix(1:2, nrow=1))

rg=range(phosphate[ ,-1])
plot(c(1, 8), c(rg), type=&amp;quot;n&amp;quot;, xlab=&amp;quot;Time (hours)&amp;quot;, 
     ylab=&amp;quot;Phosphate Levels&amp;quot;, main=&amp;quot;base R: Control Group&amp;quot;)
for(i in 1:sum(phosphate$group==&amp;quot;control&amp;quot;)){
  lines(1:8, phosphate[i, -1])
}

plot(c(1, 8), c(rg), type=&amp;quot;n&amp;quot;, xlab=&amp;quot;Time (hours)&amp;quot;, 
     ylab=&amp;quot;Phosphate Levels&amp;quot;, main=&amp;quot;Obese Group&amp;quot;)
for(i in 1:sum(phosphate$group==&amp;quot;obese&amp;quot;)){
  lines(1:8, phosphate[nrow(phosphate)-i, -1])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phosphate_t &amp;lt;- phosphate
phosphate_t$subject &amp;lt;- factor(rownames(phosphate_t))

phosphate_t %&amp;gt;%
  gather(time,phosphate_level, 2:9) %&amp;gt;%
  mutate(time = factor(gsub(time,pattern=&amp;quot;[[:alpha:]]&amp;quot;,replacement=&amp;#39;&amp;#39;))) %&amp;gt;%
ggplot(aes(x=time,y=phosphate_level)) +
  geom_line(aes(group=subject,color=subject),size=1) +
  facet_grid(.~group) +
  labs(title=&amp;#39;ggplot: Phosphate by group&amp;#39;,
       x=&amp;#39;Time (in hours)&amp;#39;,
       y=&amp;#39;Phosphate Level&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Longitudinal_Data_Analysis_and_Mixed_Models_I_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2.d&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#d. Convert the data to long version and fit the model of your choice and
#discuss the results.
data(phosphate)
ph=phosphate
ph$subject=as.factor(rownames(ph))
##LOng form
a=ph[,c(3:9)]

ph_long=reshape(ph,idvar=&amp;quot;subject&amp;quot;,v.names=&amp;quot;p_level&amp;quot;,
                varying=list(names(a)),direction=&amp;quot;long&amp;quot;)

timer=seq(1:7)
timer[1]=0.5
timer[2]=1
timer[3]=1.5
for(i in 4:7){
  timer[i]=i-2}

for(i in 1:7){
  for(j in 1:33){
    ph_long[(33*(i-1)+j),4]=timer[i]
  }
}

ph_non_inter=lmer(p_level~time+group+t0+(1|subject),data=ph_long,
                  REML=FALSE,na.actio=na.omit)

ph_inter=lmer(p_level~time+group+t0+time*group+(1|subject),
              data=ph_long,REML=FALSE,na.actio=na.omit)

ph_cept_slope=lmer(p_level~time+group+t0+
                     (time|subject),
              data=ph_long,REML=FALSE,na.actio=na.omit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## unable to evaluate scaled gradient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge: degenerate Hessian with 1 negative eigenvalues&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ph_cept_slope_inter=lmer(p_level~time+group+t0+time*group+
                           (time|subject),
                         data=ph_long,REML=FALSE,na.actio=na.omit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.0119258 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(ph_cept_slope_inter))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: p_level ~ time + group + t0 + time * group + (time | subject)
##    Data: ph_long
## 
##      AIC      BIC   logLik deviance df.resid 
##    391.4    422.4   -186.7    373.4      222 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4111 -0.5836 -0.0419  0.5861  3.5259 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  subject  (Intercept) 0.009286 0.09636      
##           time        0.004514 0.06718  0.99
##  Residual             0.250017 0.50002      
## Number of obs: 231, groups:  subject, 33
## 
## Fixed effects:
##                  Estimate Std. Error t value
## (Intercept)      0.001779   0.339884   0.005
## time             0.174229   0.031570   5.519
## groupobese       0.636604   0.137868   4.617
## t0               0.691589   0.079454   8.704
## time:groupobese -0.211030   0.050299  -4.196
## 
## Correlation of Fixed Effects:
##             (Intr) time   gropbs t0    
## time        -0.144                     
## groupobese   0.150  0.356              
## t0          -0.970  0.000 -0.304       
## time:gropbs  0.091 -0.628 -0.568  0.000
## convergence code: 0
## Model failed to converge with max|grad| = 0.0119258 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(cftest(ph_cept_slope_inter))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lmer(formula = p_level ~ time + group + t0 + time * group + (time | 
##     subject), data = ph_long, REML = FALSE, na.action = na.omit)
## 
## Linear Hypotheses:
##                       Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) == 0      0.001779   0.339884   0.005    0.996    
## time == 0             0.174229   0.031570   5.519 3.41e-08 ***
## groupobese == 0       0.636604   0.137868   4.617 3.88e-06 ***
## t0 == 0               0.691589   0.079454   8.704  &amp;lt; 2e-16 ***
## time:groupobese == 0 -0.211030   0.050299  -4.196 2.72e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Univariate p values reported)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(ph_inter,ph_non_inter,ph_cept_slope,ph_cept_slope_inter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: ph_long
## Models:
## ph_non_inter: p_level ~ time + group + t0 + (1 | subject)
## ph_inter: p_level ~ time + group + t0 + time * group + (1 | subject)
## ph_cept_slope: p_level ~ time + group + t0 + (time | subject)
## ph_cept_slope_inter: p_level ~ time + group + t0 + time * group + (time | subject)
##                     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)
## ph_non_inter         6 411.77 432.43 -199.89   399.77                         
## ph_inter             7 393.19 417.29 -189.59   379.19 20.585      1  5.705e-06
## ph_cept_slope        8 404.23 431.76 -194.11   388.23  0.000      1  1.0000000
## ph_cept_slope_inter  9 391.45 422.43 -186.72   373.45 14.775      1  0.0001211
##                        
## ph_non_inter           
## ph_inter            ***
## ph_cept_slope          
## ph_cept_slope_inter ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred=predict(ph_cept_slope_inter)
MSE=mean((pred-ph_long$p_level)^2)
message(&amp;quot;Mean Squared Error: &amp;quot;,MSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Mean Squared Error: 0.225552916473035&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;Mean Phosphate Level: &amp;quot;,mean(ph_long$p_level))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Mean Phosphate Level: 3.48744588744589&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.a&lt;/p&gt;
&lt;p&gt;Based on the plot, the control group has the median level of phosphate falling rapidly from a higher level of 4.3 to midway between t1 and t2 and starts to rise more rapidly over time. The obese group reached a relatively higher median phosphate rate close to 5 and dropped and grew steadily over time. The Obese group also has two outliers.&lt;/p&gt;
&lt;p&gt;2.c&lt;/p&gt;
&lt;p&gt;I think linear mixed model would be suitable because the slopes and intercepts in each group vary widely. Additionally, there is an interaction between control and obese groups based on time. So fitting something of the form lmer(phosphatelevel t0+time∗group+(1= phos) would be suitable.&lt;/p&gt;
&lt;p&gt;2.d&lt;/p&gt;
&lt;p&gt;Based on the model I used:
In the fixed effect we see that there is a positive relation between the time and the phosphate level. It makes sense sinse we saw that after decreasing for a while the phosphate level increases for longer than decreasing time period. Based on the f-test with cftest , time, group, baseline phosphate level and the interaction between time and group all are significant.&lt;/p&gt;
&lt;p&gt;Comparing this with three other models, (ph_non_inter=random intercept, ph_inter=random intercept with interction, ph_cept_slope=random intercept and random slope) using anova, the model I used has the smallest AIC (about 391) indicating a better fit of among four models. However, considering BIC value, we also see that random intercept with interaction is also a good model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantile Regression</title>
      <link>/achalneupane.github.io/post/quantile_regression/</link>
      <pubDate>Fri, 18 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/quantile_regression/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the {} data from the {} package&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Review the linear model fitted to this data in Chapter 6 of the text book and report the model and findings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a median regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare the two results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1     ✔ purrr   0.3.3
## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mboost)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: parallel&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: stabs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mboost 2.9-1. See &amp;#39;package?mboost&amp;#39; and &amp;#39;news(package  = &amp;quot;mboost&amp;quot;)&amp;#39;
## for a complete list of changes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;mboost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     extract&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;quantreg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: SparseM&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;SparseM&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:base&amp;#39;:
## 
##     backsolve&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;TH.data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;survival&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:quantreg&amp;#39;:
## 
##     untangle.specials&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:HSAUR3&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamlss.data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gamlss.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:HSAUR3&amp;#39;:
## 
##     plasma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:datasets&amp;#39;:
## 
##     sleep&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lattice)
data(clouds)
head(clouds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   seeding time  sne cloudcover prewetness echomotion rainfall
## 1      no    0 1.75       13.4      0.274 stationary    12.85
## 2     yes    1 2.70       37.9      1.267     moving     5.52
## 3     yes    3 4.10        3.9      0.198 stationary     6.29
## 4      no    4 2.35        5.3      0.526     moving     6.11
## 5     yes    6 4.25        7.1      0.250     moving     2.45
## 6      no    9 1.60        6.9      0.018 stationary     3.61&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1a. Review the linear model fitted to this data in Chapter 6&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1a. Review the linear model fitted to this data in Chapter 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clouds_formula &amp;lt;- rainfall ~ seeding + seeding:(sne + cloudcover + prewetness + echomotion) + time
clouds.lm &amp;lt;- glm(clouds_formula, data = clouds)
summary(clouds.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = clouds_formula, data = clouds)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5259  -1.1486  -0.2704   1.0401   4.3913  
## 
## Coefficients:
##                                 Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)                     -0.34624    2.78773  -0.124  0.90306   
## seedingyes                      15.68293    4.44627   3.527  0.00372 **
## time                            -0.04497    0.02505  -1.795  0.09590 . 
## seedingno:sne                    0.41981    0.84453   0.497  0.62742   
## seedingyes:sne                  -2.77738    0.92837  -2.992  0.01040 * 
## seedingno:cloudcover             0.38786    0.21786   1.780  0.09839 . 
## seedingyes:cloudcover           -0.09839    0.11029  -0.892  0.38854   
## seedingno:prewetness             4.10834    3.60101   1.141  0.27450   
## seedingyes:prewetness            1.55127    2.69287   0.576  0.57441   
## seedingno:echomotionstationary   3.15281    1.93253   1.631  0.12677   
## seedingyes:echomotionstationary  2.59060    1.81726   1.426  0.17757   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 4.860684)
## 
##     Null deviance: 222.335  on 23  degrees of freedom
## Residual deviance:  63.189  on 13  degrees of freedom
## AIC: 115.34
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;It looks like the seedingyes variable is the most significant variable in the model followed by seedingyes:sne&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It looks like the seedingyes variable is the most significant variable in the model followed by seedingyes:sne&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Now we choose continous variable sne to fit our linear model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Now we choose continous variable sne to fit our linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clouds.lm &amp;lt;- glm(rainfall ~ sne, data = clouds)
summary(clouds.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = rainfall ~ sne, data = clouds)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -6.4927  -2.1116   0.0556   1.2295   6.5036  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   8.7430     2.1508   4.065 0.000515 ***
## sne          -1.3695     0.6524  -2.099 0.047512 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 8.419897)
## 
##     Null deviance: 222.33  on 23  degrees of freedom
## Residual deviance: 185.24  on 22  degrees of freedom
## AIC: 123.16
## 
## Number of Fisher Scoring iterations: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSE.lm &amp;lt;- mean((predict(clouds.lm, data = clouds)-clouds$rainfall)^2)
cat(&amp;quot;Linear model MSE: &amp;quot;,MSE.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear model MSE:  7.718239&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=clouds, aes(x=sne, y=rainfall, col=seeding)) + geom_point() + geom_smooth(method=&amp;#39;lm&amp;#39;) + labs(title=&amp;#39;Rainfall determined by suitability criterion&amp;#39;,x=&amp;#39;S-NE Criterion&amp;#39;, y=&amp;#39;Rainfall&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1b. Fit a median regression (quantile regression) model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1b. Fit a median regression (quantile regression) model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median.reg &amp;lt;- rq(rainfall ~ sne, data = clouds, tau = 0.5)
cat(&amp;quot;Summary of the model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Summary of the model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(median.reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = rainfall ~ sne, tau = 0.5, data = clouds)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  8.86133      3.14768 14.86666
## sne         -1.38667     -2.46926  0.13118&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Summary of model with bootstrapped standard error&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Summary of model with bootstrapped standard error&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(median.reg, se = &amp;quot;boot&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = rainfall ~ sne, tau = 0.5, data = clouds)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             Value    Std. Error t value  Pr(&amp;gt;|t|)
## (Intercept)  8.86133  3.62423    2.44502  0.02295
## sne         -1.38667  1.06195   -1.30577  0.20512&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSE.mrm &amp;lt;- mean((predict(median.reg, data = clouds, type = &amp;quot;response&amp;quot;)-clouds$rainfall)^2)
cat(&amp;quot;MSE of median regression Model: &amp;quot;,MSE.mrm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MSE of median regression Model:  7.722559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;1c. We can also plot this model to compare with previous linear model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1c. We can also plot this model to compare with previous linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_linear &amp;lt;- ggplot(data=clouds, aes(x=sne, y=rainfall, col=seeding)) + geom_point() + geom_smooth(method=&amp;#39;lm&amp;#39;,se=FALSE) + labs(title=&amp;#39;Linear regression model: Rainfall\n determined by suitability criterion&amp;#39;,x=&amp;#39;S-NE Criterion&amp;#39;, y=&amp;#39;Rainfall&amp;#39;) + theme_classic()


plot_median &amp;lt;- ggplot(data=clouds, aes(x=sne, y=rainfall, col=seeding)) + geom_point()  + labs(title=&amp;#39;Median regression model: Rainfall\n determined by suitability criterion&amp;#39;,x=&amp;#39;S-NE Criterion&amp;#39;, y=&amp;#39;Rainfall&amp;#39;) + stat_quantile(quantiles=c(0.50), method=&amp;#39;rq&amp;#39;) + theme_classic()

grid.arrange(plot_linear, plot_median, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;In addition to fitting the models asked in 1a and 1b, I used graphical methods to compare a linear regression fit vs median regression fit for 1c. As shown in the plots, a median regression model on the data splits the absence of seeding in such a way that the median regression line has a positive slope, whereas, the simple linear regression model seems to have the negative slope. This indicates that there is higher variability in the rainfall data when cloud seeding is absent. Additionally, in presence of seeding, the median regression line is not weighted by the outliers, therefore seems better at explaining the overall data due to the high variability of the rainfall variable.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Reanalyze the {} data from the {} package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Compare the regression tree approach from chapter 9 of the textbook to median regression and summarize the different findings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose one independent variable. For the relationship between this variable and DEXfat, create linear regression quantile models for the 25%, 50% and 75% quantiles. Plot DEXfat vs that independent variable and plot the lines from the models on the graph.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;bodyfat&amp;quot;)
head(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    age DEXfat waistcirc hipcirc elbowbreadth kneebreadth anthro3a anthro3b
## 47  57  41.68     100.0   112.0          7.1         9.4     4.42     4.95
## 48  65  43.29      99.5   116.5          6.5         8.9     4.63     5.01
## 49  59  35.41      96.0   108.5          6.2         8.9     4.12     4.74
## 50  58  22.79      72.0    96.5          6.1         9.2     4.03     4.48
## 51  60  36.42      89.5   100.5          7.1        10.0     4.24     4.68
## 52  61  24.13      83.5    97.0          6.5         8.8     3.55     4.06
##    anthro3c anthro4
## 47     4.50    6.13
## 48     4.48    6.37
## 49     4.60    5.82
## 50     3.91    5.66
## 51     4.15    5.91
## 52     3.64    5.14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncol(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;rpart&amp;quot;)
# ?bodyfat

cat(&amp;quot;First, fit regression tree model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## First, fit regression tree model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bodyfat_formula &amp;lt;- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth
bodyfat_rp  &amp;lt;- rpart(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data = bodyfat, control = rpart.control(minsplit=10))
summary(bodyfat_rp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## rpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + 
##     kneebreadth, data = bodyfat, control = rpart.control(minsplit = 10))
##   n= 71 
## 
##           CP nsplit  rel error    xerror       xstd
## 1 0.66289544      0 1.00000000 1.0149614 0.16976190
## 2 0.09376252      1 0.33710456 0.4071906 0.09251759
## 3 0.07703606      2 0.24334204 0.4135264 0.08910155
## 4 0.04507506      3 0.16630598 0.2974999 0.05920879
## 5 0.01844561      4 0.12123092 0.2640916 0.05859875
## 6 0.01818982      5 0.10278532 0.2665689 0.06079305
## 7 0.01000000      6 0.08459549 0.2558127 0.06188322
## 
## Variable importance
##    waistcirc      hipcirc  kneebreadth elbowbreadth          age 
##           34           30           24            7            4 
## 
## Node number 1: 71 observations,    complexity param=0.6628954
##   mean=30.78282, MSE=120.2251 
##   left son=2 (40 obs) right son=3 (31 obs)
##   Primary splits:
##       waistcirc    &amp;lt; 88.4   to the left,  improve=0.6628954, (0 missing)
##       hipcirc      &amp;lt; 108.25 to the left,  improve=0.6254333, (0 missing)
##       kneebreadth  &amp;lt; 9.35   to the left,  improve=0.5142133, (0 missing)
##       age          &amp;lt; 40.5   to the left,  improve=0.1570344, (0 missing)
##       elbowbreadth &amp;lt; 6.55   to the left,  improve=0.1169918, (0 missing)
##   Surrogate splits:
##       hipcirc      &amp;lt; 107.85 to the left,  agree=0.915, adj=0.806, (0 split)
##       kneebreadth  &amp;lt; 9.35   to the left,  agree=0.831, adj=0.613, (0 split)
##       elbowbreadth &amp;lt; 6.55   to the left,  agree=0.648, adj=0.194, (0 split)
##       age          &amp;lt; 47     to the left,  agree=0.592, adj=0.065, (0 split)
## 
## Node number 2: 40 observations,    complexity param=0.07703606
##   mean=22.92375, MSE=32.88394 
##   left son=4 (17 obs) right son=5 (23 obs)
##   Primary splits:
##       hipcirc      &amp;lt; 96.25  to the left,  improve=0.4999238, (0 missing)
##       waistcirc    &amp;lt; 71.5   to the left,  improve=0.4408508, (0 missing)
##       kneebreadth  &amp;lt; 9.15   to the left,  improve=0.3123752, (0 missing)
##       age          &amp;lt; 41     to the left,  improve=0.2212005, (0 missing)
##       elbowbreadth &amp;lt; 6.65   to the left,  improve=0.0757275, (0 missing)
##   Surrogate splits:
##       waistcirc    &amp;lt; 71.5   to the left,  agree=0.775, adj=0.471, (0 split)
##       age          &amp;lt; 41     to the left,  agree=0.700, adj=0.294, (0 split)
##       kneebreadth  &amp;lt; 8.25   to the left,  agree=0.675, adj=0.235, (0 split)
##       elbowbreadth &amp;lt; 5.75   to the left,  agree=0.600, adj=0.059, (0 split)
## 
## Node number 3: 31 observations,    complexity param=0.09376252
##   mean=40.92355, MSE=50.39231 
##   left son=6 (28 obs) right son=7 (3 obs)
##   Primary splits:
##       kneebreadth  &amp;lt; 11.15  to the left,  improve=0.51233840, (0 missing)
##       hipcirc      &amp;lt; 109.9  to the left,  improve=0.45671770, (0 missing)
##       waistcirc    &amp;lt; 106    to the left,  improve=0.44843720, (0 missing)
##       elbowbreadth &amp;lt; 6.35   to the left,  improve=0.16017880, (0 missing)
##       age          &amp;lt; 45.5   to the right, improve=0.06131694, (0 missing)
## 
## Node number 4: 17 observations,    complexity param=0.01844561
##   mean=18.20765, MSE=16.81845 
##   left son=8 (11 obs) right son=9 (6 obs)
##   Primary splits:
##       age          &amp;lt; 59.5   to the left,  improve=0.55069560, (0 missing)
##       waistcirc    &amp;lt; 70.35  to the left,  improve=0.39973880, (0 missing)
##       elbowbreadth &amp;lt; 6.65   to the left,  improve=0.22215850, (0 missing)
##       hipcirc      &amp;lt; 92.6   to the left,  improve=0.16823720, (0 missing)
##       kneebreadth  &amp;lt; 8.55   to the left,  improve=0.08112073, (0 missing)
##   Surrogate splits:
##       elbowbreadth &amp;lt; 6.55   to the left,  agree=0.824, adj=0.500, (0 split)
##       waistcirc    &amp;lt; 71.5   to the left,  agree=0.765, adj=0.333, (0 split)
## 
## Node number 5: 23 observations,    complexity param=0.01818982
##   mean=26.40957, MSE=16.16806 
##   left son=10 (13 obs) right son=11 (10 obs)
##   Primary splits:
##       waistcirc    &amp;lt; 80.75  to the left,  improve=0.41753840, (0 missing)
##       hipcirc      &amp;lt; 101.35 to the left,  improve=0.34272770, (0 missing)
##       kneebreadth  &amp;lt; 9.5    to the left,  improve=0.30544320, (0 missing)
##       elbowbreadth &amp;lt; 7.1    to the right, improve=0.06644785, (0 missing)
##       age          &amp;lt; 57     to the right, improve=0.03572739, (0 missing)
##   Surrogate splits:
##       hipcirc      &amp;lt; 101.75 to the left,  agree=0.783, adj=0.5, (0 split)
##       kneebreadth  &amp;lt; 9.5    to the left,  agree=0.696, adj=0.3, (0 split)
##       age          &amp;lt; 66     to the left,  agree=0.652, adj=0.2, (0 split)
##       elbowbreadth &amp;lt; 6.25   to the left,  agree=0.652, adj=0.2, (0 split)
## 
## Node number 6: 28 observations,    complexity param=0.04507506
##   mean=39.26036, MSE=21.98307 
##   left son=12 (13 obs) right son=13 (15 obs)
##   Primary splits:
##       hipcirc      &amp;lt; 109.9  to the left,  improve=0.62509140, (0 missing)
##       waistcirc    &amp;lt; 99     to the left,  improve=0.47879840, (0 missing)
##       kneebreadth  &amp;lt; 9.85   to the left,  improve=0.28389460, (0 missing)
##       elbowbreadth &amp;lt; 6.35   to the left,  improve=0.18101920, (0 missing)
##       age          &amp;lt; 49.5   to the right, improve=0.04758482, (0 missing)
##   Surrogate splits:
##       waistcirc    &amp;lt; 99     to the left,  agree=0.821, adj=0.615, (0 split)
##       elbowbreadth &amp;lt; 6.45   to the left,  agree=0.714, adj=0.385, (0 split)
##       kneebreadth  &amp;lt; 9.95   to the left,  agree=0.714, adj=0.385, (0 split)
##       age          &amp;lt; 49.5   to the right, agree=0.607, adj=0.154, (0 split)
## 
## Node number 7: 3 observations
##   mean=56.44667, MSE=48.76009 
## 
## Node number 8: 11 observations
##   mean=15.96, MSE=8.818582 
## 
## Node number 9: 6 observations
##   mean=22.32833, MSE=5.242981 
## 
## Node number 10: 13 observations
##   mean=24.13077, MSE=9.046699 
## 
## Node number 11: 10 observations
##   mean=29.372, MSE=9.899016 
## 
## Node number 12: 13 observations
##   mean=35.27846, MSE=10.48431 
## 
## Node number 13: 15 observations
##   mean=42.71133, MSE=6.297998&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Plot the regression tree model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plot the regression tree model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(partykit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: libcoin&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;partykit&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:mboost&amp;#39;:
## 
##     varimp&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.party(bodyfat_rp), tp_args = list(id=FALSE), main = &amp;quot;Regression tree of the model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Print CP-table&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Print CP-table&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(bodyfat_rp$cptable)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           CP nsplit  rel error    xerror       xstd
## 1 0.66289544      0 1.00000000 1.0149614 0.16976190
## 2 0.09376252      1 0.33710456 0.4071906 0.09251759
## 3 0.07703606      2 0.24334204 0.4135264 0.08910155
## 4 0.04507506      3 0.16630598 0.2974999 0.05920879
## 5 0.01844561      4 0.12123092 0.2640916 0.05859875
## 6 0.01818982      5 0.10278532 0.2665689 0.06079305
## 7 0.01000000      6 0.08459549 0.2558127 0.06188322&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;CP value with lowest xerror&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CP value with lowest xerror&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min.cp &amp;lt;- which.min(bodyfat_rp$cptable[,&amp;#39;xerror&amp;#39;])
min.cp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 7 
## 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(
  &amp;quot;Following the methods in the book, we can fit the model\n using lowest xerror rate from CP for prunning tree model&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Following the methods in the book, we can fit the model
##  using lowest xerror rate from CP for prunning tree model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#extract the lowest CP
cp &amp;lt;- bodyfat_rp$cptable[min.cp, &amp;#39;CP&amp;#39;]
bodyfat_prune &amp;lt;- prune(bodyfat_rp, cp=cp)

cat(&amp;quot;summary of the median regression model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## summary of the median regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Median Regression model
bodyfat_rpart_qrm &amp;lt;- rq(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data=bodyfat, tau = 0.50)
summary(bodyfat_rpart_qrm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + 
##     kneebreadth, tau = 0.5, data = bodyfat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##              coefficients lower bd  upper bd 
## (Intercept)  -57.30032    -87.22119 -36.39320
## age            0.06839     -0.04338   0.14943
## waistcirc      0.28332      0.07991   0.48638
## hipcirc        0.51073      0.21307   0.75030
## elbowbreadth  -0.11982     -3.62882   2.18220
## kneebreadth    0.76453     -2.30145   2.33329&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_rpart_qrm_summary &amp;lt;- summary(bodyfat_rpart_qrm) 


#Predict Pruned regression tree model on the bodyfat data set
RegressionTree &amp;lt;- predict(bodyfat_prune, data=bodyfat)

#Create observed value and the predicted value
observed &amp;lt;- bodyfat$DEXfat
predict &amp;lt;- RegressionTree



#Regression.Tree MSE
RegressionTree.MSE &amp;lt;- mean((observed - predict)^2)

#Median Regression MSE
MedianRegression.MSE &amp;lt;- mean(bodyfat_rpart_qrm_summary$residuals^2)

df &amp;lt;- data.frame(
  RegressionTree.MSE,
  MedianRegression.MSE
)

cat(&amp;quot;MSE of both models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MSE of both models&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   RegressionTree.MSE MedianRegression.MSE
## 1            10.1705              15.0245&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Based on this, pruned regression tree has lower MSE than median regression model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Based on this, pruned regression tree has lower MSE than median regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Plot based on the regression tree prunning&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plot based on the regression tree prunning&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(as.party(bodyfat_prune), tp_args = list(id=FALSE), main = &amp;quot;Plot based on the regression tree prunning&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(
  &amp;quot;Based on the above pruned tree, the variables waist circumference and hip circumference splits explain the majority of the data and I will be choosing one of these variables for quantile regression.&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Based on the above pruned tree, the variables waist circumference and hip circumference splits explain the majority of the data and I will be choosing one of these variables for quantile regression.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Additionally, I will check with linear regression for the variable with significant effect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Additionally, I will check with linear regression for the variable with significant effect&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check.lm &amp;lt;- lm(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data = bodyfat)

summary(check.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + 
##     kneebreadth, data = bodyfat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.1782 -2.4973  0.2089  2.5496 11.6504 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -59.57320    8.45359  -7.047 1.43e-09 ***
## age            0.06381    0.03740   1.706   0.0928 .  
## waistcirc      0.32044    0.07372   4.347 4.96e-05 ***
## hipcirc        0.43395    0.09566   4.536 2.53e-05 ***
## elbowbreadth  -0.30117    1.21731  -0.247   0.8054    
## kneebreadth    1.65381    0.86235   1.918   0.0595 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.988 on 65 degrees of freedom
## Multiple R-squared:  0.8789, Adjusted R-squared:  0.8696 
## F-statistic: 94.34 on 5 and 65 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Looks like waistcirc is the most significant, so we will choose this variable&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Looks like waistcirc is the most significant, so we will choose this variable&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Now we can run a median quantile regression&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Now we can run a median quantile regression&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_qrm_25 &amp;lt;- rq(DEXfat ~ age + waistcirc, data = bodyfat, tau = 0.25)
summary(bodyfat_qrm_25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc, tau = 0.25, data = bodyfat)
## 
## tau: [1] 0.25
## 
## Coefficients:
##             coefficients lower bd  upper bd 
## (Intercept) -34.95268    -41.72329 -30.15887
## age           0.07777      0.01124   0.14609
## waistcirc     0.67033      0.60935   0.74444&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_qrm_50 &amp;lt;- rq(DEXfat ~ age + waistcirc, data = bodyfat, tau = 0.50)
summary(bodyfat_qrm_50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc, tau = 0.5, data = bodyfat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd  upper bd 
## (Intercept) -28.39189    -39.03217 -17.59073
## age           0.05514     -0.06704   0.15053
## waistcirc     0.63962      0.54298   0.76936&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_qrm_75 &amp;lt;- rq(DEXfat ~ age + waistcirc, data = bodyfat, tau = 0.75)
summary(bodyfat_qrm_50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call: rq(formula = DEXfat ~ age + waistcirc, tau = 0.5, data = bodyfat)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd  upper bd 
## (Intercept) -28.39189    -39.03217 -17.59073
## age           0.05514     -0.06704   0.15053
## waistcirc     0.63962      0.54298   0.76936&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;DEXfat explained by waist circumference at quantile 25%, 50%, and 75% regression lines&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## DEXfat explained by waist circumference at quantile 25%, 50%, and 75% regression lines&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(data=bodyfat, DEXfat~waistcirc, main=&amp;quot;baseR: Quantile regression- DEXfat Explained by waist circumference&amp;quot;, xlab=&amp;#39;Waist circumference&amp;#39;)
abline(rq(DEXfat ~ waistcirc, data=bodyfat, tau = 0.25), col=&amp;#39;blue&amp;#39;)
abline(rq(DEXfat ~ waistcirc, data=bodyfat, tau = 0.50), col=&amp;#39;green&amp;#39;)
abline(rq(DEXfat ~ waistcirc, data=bodyfat, tau = 0.75), col=&amp;#39;black&amp;#39;)
legend(&amp;#39;topleft&amp;#39;, legend = c(&amp;#39;25% Quantile&amp;#39;, &amp;#39;50% Quantile&amp;#39;, &amp;#39;75% Quantile&amp;#39;),
       fill=c(&amp;#39;blue&amp;#39;,&amp;#39;green&amp;#39;,&amp;#39;black&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=bodyfat, aes(x=waistcirc, y=DEXfat)) + geom_point() + stat_quantile(quantiles=c(0.25), method=&amp;#39;rq&amp;#39;, aes(colour=&amp;#39;25%&amp;#39;)) + stat_quantile(quantiles=c(0.50), method=&amp;#39;rq&amp;#39;, aes(colour=&amp;#39;50%&amp;#39;)) + 
  stat_quantile(quantiles=c(0.75), method=&amp;#39;rq&amp;#39;, aes(colour=&amp;#39;75%&amp;#39;)) +
  labs(title=&amp;quot;ggplot: Quantile regression- DEXfat Explained by waist circumference&amp;quot;, x=&amp;#39;Waist circumference&amp;#39;, y=&amp;#39;DEXfat&amp;#39;) +
scale_color_manual(name=&amp;quot;Quantile Percent&amp;quot;, values = c(&amp;#39;25%&amp;#39; = &amp;quot;blue&amp;quot;, &amp;#39;50%&amp;#39; = &amp;quot;green&amp;quot;, &amp;#39;75%&amp;#39; = &amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Smoothing formula not specified. Using: y ~ x
## Smoothing formula not specified. Using: y ~ x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;Based on the above pruned tree, the variables waist circumference and hip circumference splits explain the majority of the data and I chose waist circumference for quantile regression. Based on this analysis, pruned regression tree has lower MSE than median regression model. The relationship of Dexfat to Age by Waist Circumference, all three quantiles regression lines have a positive, and seemingly similar slopes.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Consider {} data from the lecture notes (package {}). Refit the additive quantile regression models presented ({}) with varying values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (lambda) in {}. How do the estimated quantile curves change?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Q3 - db data from gamlss.data package
data(db)
head(db)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   head  age
## 1 33.6 0.03
## 2 33.6 0.04
## 3 33.7 0.04
## 4 35.0 0.04
## 5 36.1 0.04
## 6 36.6 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(db)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7040    2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tau &amp;lt;- c(.03, .15, .5, .85, .97)


cat(&amp;quot;Parameters: lambda = 0&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parameters: lambda = 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rqssmod &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(tau))
db$lage &amp;lt;- with(db, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] &amp;lt;- rqss(head ~ qss(lage, lambda = 0), data = db, tau = tau[i])

gage &amp;lt;- seq(from = min(db$age), to = max(db$age), length = 50)
p &amp;lt;- sapply(1:length(tau), function(i) { predict(rqssmod[[i]], newdata = data.frame(lage = gage^(1/3)))
})

pfun &amp;lt;- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
}

xyplot(head ~ age, data = db, main = &amp;quot;Head circumference curve with lambda = 0&amp;quot;,
       xlab = &amp;quot;Age (years)&amp;quot;, ylab = &amp;quot;Head circumference (cm)&amp;quot;, pch = 19,
       scales = list(x = list(relation = &amp;quot;free&amp;quot;)),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Parameters: lambda = 1; and tau same as before&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parameters: lambda = 1; and tau same as before&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rqssmod &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(tau))
db$lage &amp;lt;- with(db, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] &amp;lt;- rqss(head ~ qss(lage, lambda = 1), data = db, tau = tau[i])

gage &amp;lt;- seq(from = min(db$age), to = max(db$age), length = 50)
p &amp;lt;- sapply(1:length(tau), function(i) { predict(rqssmod[[i]], newdata = data.frame(lage = gage^(1/3)))
})

pfun &amp;lt;- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
}

xyplot(head ~ age, data = db, main = &amp;quot;Head circumference curve with lambda=1&amp;quot;,
       xlab = &amp;quot;Age (years)&amp;quot;, ylab = &amp;quot;Head circumference (cm)&amp;quot;, pch = 19,
       scales = list(x = list(relation = &amp;quot;free&amp;quot;)),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Parameters: lambda = 20; and tau same as before&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parameters: lambda = 20; and tau same as before&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rqssmod &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(tau))
db$lage &amp;lt;- with(db, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] &amp;lt;- rqss(head ~ qss(lage, lambda = 20), data = db, tau = tau[i])

gage &amp;lt;- seq(from = min(db$age), to = max(db$age), length = 50)
p &amp;lt;- sapply(1:length(tau), function(i) { predict(rqssmod[[i]], newdata = data.frame(lage = gage^(1/3)))
})

pfun &amp;lt;- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
}

xyplot(head ~ age, data = db, main = &amp;quot;Head circumference curve with lambda=20&amp;quot;,
       xlab = &amp;quot;Age (years)&amp;quot;, ylab = &amp;quot;Head circumference (cm)&amp;quot;, pch = 19,
       scales = list(x = list(relation = &amp;quot;free&amp;quot;)),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Quantile_Regression_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;Here, lambda acts as a shinkage factor which causes the quantiles to become smoother (at higher lambda) rather than becoming wavy or rough with lower values of labda (lambda = 0). So, I found that by increasing the penalty term lambda, which is assigned to the slope of the coefficients, the overall fit of the additive quantile regression model can be smoothen.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Read the paper by Koenker and Hallock (2001), posted on D2L. Write a one page summary of the paper. This should include but not be limited to introduction, motivation, case study considered and findings.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Answer:&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;In Koenker and Hallock (2001), the authors discuss the utility of quantile regression. In this study, they made a comparison on linear regression and quantile regression analyses.The quantile regression allows large sample groups or population size to into fractals distribution or smaller quantiles represented by a parameter &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; (tau) and maintaining the same &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; for observations above and below the quantile and minimizing the sum of weighted absolute residuals. They show the utility of quantile regression by looking at various types of data sets such as household food spending on household income and child birthweight.&lt;/p&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;p&gt;The reason to look at these data sets was to illustrate and explain that in some situations, quantile regression is more useful than linear regression. Quantile regression, which measures the median rather than the mean, will yield more suitable fits to results when a distribution includes outliers that could force the mean in a specific direction. This was addressed in the paper when referring to the’ Quantile Engel Curves’ where two different points of high income and low food consumption influenced the least square estimate. Quantile regression could eliminate this type of bias by minimizing the sum of absolute residuals.&lt;/p&gt;
&lt;p&gt;Case study&lt;/p&gt;
&lt;p&gt;The Infant Birth weight case study aimed to analyze the association between low birth weights of children and several variables including public policy measures. Infants weighing less than 5 pounds, 9 ounces at birth (2500 grams) were defined as low birth weight. Quantile regression was relevant in this analysis because due to the lower tail of the birth weight distribution, all the least square estimates had been skewed.&lt;/p&gt;
&lt;p&gt;Findings&lt;/p&gt;
&lt;p&gt;The most notable finding was that, based on the least square calculation, boys were usually born heavier than girls by 100 grams. The figure, however, changes with the quantile of 0.05, where boys were only 45 grams larger and even greater at the quantile of 095, where boys were 130 grams larger. The least squares (linear regression) in this case does a poor job of explaining the distribution variability. Additionally, infants born to black and white mothers with 5th percentile birth weights differed by 1/3 of a kilogram. Furthermore, the age of mother, education beyond high school, prenatal care, marital status, and smoking all contribute to the birth weight of an infant.&lt;/p&gt;
&lt;p&gt;Conclusion&lt;/p&gt;
&lt;p&gt;In conclusion, some distributions with longer tails can weigh the mean so that the regression of the least squares can provide a false representation of the results. A more precise description of the data and underlying patterns can be obtained using quantile regression. This is particularly useful in econometrics where there are often large outliers that can have a significant impact on a least squares model. Basically, we need to assess our needs when dealing with the datasets:
i. Are we concerned about percentiles or median value?
ii. Whether we are interested in average values and minimizing the residuals errors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Survival Analysis</title>
      <link>/achalneupane.github.io/post/survival_analysis/</link>
      <pubDate>Fri, 11 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/survival_analysis/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;An investigator collected data on survival of patients with lung cancer at Mayo Clinic. The investigator would like you, the statistician, to answer the following questions and provide some graphs. Use the  data located in the  package.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the probability that someone will survive past 300 days?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Provide a graph, including 95% confidence limits, of the Kaplan-Meier estimate of the entire study.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is there a difference in the survival rates between males and females? Provide a formal statistical test with a p-value and visual evidence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is there a difference in the survival rates for the older half of the group versus the younger half? Provide a formal statistical test with a p-value and visual evidence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survminer&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggpubr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: magrittr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package &amp;#39;mclust&amp;#39; version 5.4.5
## Type &amp;#39;citation(&amp;quot;mclust&amp;quot;)&amp;#39; for citing this R package in publications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1a.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call: survfit(formula = Surv(time, status == 2) ~ 1, data = cancer)
## 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##   300     92     101    0.531  0.0346        0.467        0.603&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1b.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 1c.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## SEX values: male=1, female=2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = Surv(time, status == 2) ~ sex, data = cancer)
## 
##         N Observed Expected (O-E)^2/E (O-E)^2/V
## sex=1 138      112     91.6      4.55      10.3
## sex=2  90       53     73.4      5.68      10.3
## 
##  Chisq= 10.3  on 1 degrees of freedom, p= 0.001&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = Surv(time, status == 2) ~ Age_Group, data = dup.cancer)
## 
##               N Observed Expected (O-E)^2/E (O-E)^2/V
## Age_Group=1 117       80     88.8     0.865      1.88
## Age_Group=2 111       85     76.2     1.007      1.88
## 
##  Chisq= 1.9  on 1 degrees of freedom, p= 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;1.a&lt;/p&gt;
&lt;p&gt;The survival probability after 300 days is 53.1% determined from the dataset.&lt;/p&gt;
&lt;p&gt;1.b&lt;/p&gt;
&lt;p&gt;The plot based on the Kaplan-Meier estimator for the survival function shows that the survivability is in downward direction. The plot function generates the 95% confidence interval, and I have h and v lines drawn for survival probability at 300 days as well . The plot also indicates that within this timeline, the interval is higher for later days of survival time indicating larger variability in survival uncertainty (after 400 days of survivability).&lt;/p&gt;
&lt;p&gt;1.c&lt;/p&gt;
&lt;p&gt;Yes Males and Females have different survival probabilities which can be determined based on Kaplan-Maier estimator on male and female groups. Second plot was also generated by using &lt;code&gt;plot(survfit(Surv(time,status==2)~sex, data=cancer)&lt;/code&gt;. In this plot, the black line (males) lies below the red line (female) throughout the interval. This tells us that women have higher survival compared to males. The log rank test also indicates significant differece between sex groups with p-value of 0.001 to reject the NULL hypothesis (Null hypothesis: no difference in male and female survivability) with 95% confidence.&lt;/p&gt;
&lt;p&gt;1.d&lt;/p&gt;
&lt;p&gt;Here, data was divided into younger and older groups for the analysis based on median age. After properly splitting the data, a plot was generated based on Kaplan-Meir estimator for the two groups. Based on this plot, we can tell that the younger group (black line) of patients have longer survivability than the older patient groups(red line).However, based on the log rank test, the difference is not significant (P=0.17) to reject the NULL hypothesis ( NULL hypothesis: no difference in terms of survivability between the two age groups).&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A healthcare group has asked you to analyse the  data from the  package, which is the survival times (in months) after a mastectomy of women with breast cancer. The cancers are classified as having metastasized or not based on a histochemical marker. The healthcare group requests that your report should not be longer than one page, and must only consist of one plot, one table, and one paragraph. Do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Plot the survivor functions of each group only using GGPlot, estimated using the Kaplan-Meier estimate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use a log-rank test to compare the survival experience of each group more formally. Only present a formal table of your results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write one paragraph summarizing your findings and conclusions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## Call: survfit(formula = Surv(time, event) ~ metastasized, data = mt)
## 
##                 metastasized=no 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##    23     12       1    0.917  0.0798        0.773        1.000
##    47     11       1    0.833  0.1076        0.647        1.000
##    69     10       1    0.750  0.1250        0.541        1.000
##   148      6       1    0.625  0.1545        0.385        1.000
##   181      5       1    0.500  0.1667        0.260        0.961
## 
##                 metastasized=yes 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##     5     32       1    0.969  0.0308        0.910        1.000
##     8     31       1    0.938  0.0428        0.857        1.000
##    10     30       1    0.906  0.0515        0.811        1.000
##    13     29       1    0.875  0.0585        0.768        0.997
##    18     28       1    0.844  0.0642        0.727        0.979
##    24     27       1    0.812  0.0690        0.688        0.960
##    26     26       2    0.750  0.0765        0.614        0.916
##    31     24       1    0.719  0.0795        0.579        0.893
##    35     23       1    0.688  0.0819        0.544        0.868
##    40     22       1    0.656  0.0840        0.511        0.843
##    41     21       1    0.625  0.0856        0.478        0.817
##    48     20       1    0.594  0.0868        0.446        0.791
##    50     19       1    0.562  0.0877        0.414        0.764
##    59     18       1    0.531  0.0882        0.384        0.736
##    61     17       1    0.500  0.0884        0.354        0.707
##    68     16       1    0.469  0.0882        0.324        0.678
##    71     15       1    0.438  0.0877        0.295        0.648
##   113     10       1    0.394  0.0892        0.253        0.614
##   118      8       1    0.345  0.0906        0.206        0.577
##   143      7       1    0.295  0.0900        0.162        0.537&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Survival_Analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## log rank test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = Surv(time, event == 1) ~ metastasized, data = mt, 
##     rho = 0)
## 
##                   N Observed Expected (O-E)^2/E (O-E)^2/V
## metastasized=no  12        5      9.2      1.91      3.04
## metastasized=yes 32       21     16.8      1.05      3.04
## 
##  Chisq= 3  on 1 degrees of freedom, p= 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cox&amp;#39;s regression test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(time, event) ~ metastasized, data = mt)
## 
##                   coef exp(coef) se(coef)     z    p
## metastasizedyes 0.8516    2.3434   0.5022 1.696 0.09
## 
## Likelihood ratio test=3.35  on 1 df, p=0.06704
## n= 44, number of events= 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Question 2c:&lt;/p&gt;
&lt;p&gt;Explanation for part 2a
The figure in part 1, shows the survival probability of women with breast cancer under two categories- metastasized and not-metastasized. The red line shows the survival probability of those that have not undergone metastasis and the brown line shows the survival probability of those that have cancer metastasized elsewhere in the organs. Based on this plot we can tell that those with mestasized cancer have lower survival probability (brown line). After about 143 weeks, metastasized patient’s (brown line) survival drops down to only 29.5 % whereas for non-metastasized (red line) pateint’s the survival is still above 60%.&lt;/p&gt;
&lt;p&gt;Explanation for part 2b.&lt;/p&gt;
&lt;p&gt;To determine whether the difference is statistically significant, I have performed log-rank test and got p-value of 0.061 which is not significant at P&amp;lt;0.05 whereas for Cox regression test, the p-value was even higher.&lt;/p&gt;
&lt;p&gt;To summarize answer for part 2c:&lt;/p&gt;
&lt;p&gt;Although we saw from the figure that the the survival probability of women with matastasized breast cancer is lower than those without metastasized breast cancer, the statistical test shows this difference to be statistically not siginificant.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generalized Additive Models and Spline Models</title>
      <link>/achalneupane.github.io/post/generalized_additive_models_and_spline_models/</link>
      <pubDate>Tue, 01 Oct 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/generalized_additive_models_and_spline_models/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Consider the body fat data introduced in Chapter 9 ( data from  package).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Explore the data graphically. What variables do you think need to be included for predicting bodyfat? (Hint: Are there correlated predictors).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a generalised additive model assuming normal errors using the following code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assess the  and  of the model (don’t need GGPLOT). Are all covariates informative? Should all covariates be smoothed or should some be included as a linear effect?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Report GCV, AIC, adj-R&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;, and total model degrees of freedom.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use  function to look at the diagnostic plot. Does it appear that the normality assumption is violated?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a discussion on all of the above points.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now remove insignificant variables and remove smoothing for some variables. Report the summary, plot, GCV, AIC, adj-R&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Again fit an additive model to the body fat data, but this time for a log-transformed response. Compare the three models, which one is more appropriate? (Hint: use Adj-R&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;, residual plots, etc. to compare models).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Fit a generalised additive model that underwent AIC-based variable selection (fitted using function  function). What variable was removed by using AIC?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(mgcv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nlme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mgcv 1.8-31. For overview type &amp;#39;help(&amp;quot;mgcv-package&amp;quot;)&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mboost)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: parallel&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: stabs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mboost 2.9-1. See &amp;#39;package?mboost&amp;#39; and &amp;#39;news(package  = &amp;quot;mboost&amp;quot;)&amp;#39;
## for a complete list of changes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;mboost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamclass)
library (TH.data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;survival&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:caret&amp;#39;:
## 
##     cluster&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;TH.data&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data (&amp;quot;bodyfat&amp;quot;)
head(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    age DEXfat waistcirc hipcirc elbowbreadth kneebreadth anthro3a anthro3b
## 47  57  41.68     100.0   112.0          7.1         9.4     4.42     4.95
## 48  65  43.29      99.5   116.5          6.5         8.9     4.63     5.01
## 49  59  35.41      96.0   108.5          6.2         8.9     4.12     4.74
## 50  58  22.79      72.0    96.5          6.1         9.2     4.03     4.48
## 51  60  36.42      89.5   100.5          7.1        10.0     4.24     4.68
## 52  61  24.13      83.5    97.0          6.5         8.8     3.55     4.06
##    anthro3c anthro4
## 47     4.50    6.13
## 48     4.48    6.37
## 49     4.60    5.82
## 50     3.91    5.66
## 51     4.15    5.91
## 52     3.64    5.14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(bodyfat) # 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DescTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;DescTools&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:caret&amp;#39;:
## 
##     MAE, RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:mboost&amp;#39;:
## 
##     AUC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(bodyfat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;age&amp;quot;          &amp;quot;DEXfat&amp;quot;       &amp;quot;waistcirc&amp;quot;    &amp;quot;hipcirc&amp;quot;      &amp;quot;elbowbreadth&amp;quot;
##  [6] &amp;quot;kneebreadth&amp;quot;  &amp;quot;anthro3a&amp;quot;     &amp;quot;anthro3b&amp;quot;     &amp;quot;anthro3c&amp;quot;     &amp;quot;anthro4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
cat(&amp;quot;# 1A&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1A&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

# plot the variables to see which variables are correlated with which
library(ggplot2)
# base plot
pairs(bodyfat, main = &amp;quot;base R: plots showing correlation of variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggplot
library(GGally)
library(knitr)
p &amp;lt;- ggpairs(bodyfat)
p + labs(title = &amp;quot;ggplot: plots showing correlation of variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;all of the anthros correlate with each other&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## all of the anthros correlate with each other&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;body_f &amp;lt;- glm(DEXfat ~ ., data=bodyfat)
bf_step &amp;lt;- step(body_f, trace = 0)

cat(&amp;quot;step recommends: waistcirc, hipcirc, kneebreadth and anthro3b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## step recommends: waistcirc, hipcirc, kneebreadth and anthro3b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_step&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  glm(formula = DEXfat ~ waistcirc + hipcirc + kneebreadth + anthro3b, 
##     data = bodyfat)
## 
## Coefficients:
## (Intercept)    waistcirc      hipcirc  kneebreadth     anthro3b  
##    -71.7197       0.2037       0.3546       1.8047       7.1264  
## 
## Degrees of Freedom: 70 Total (i.e. Null);  66 Residual
## Null Deviance:       8536 
## Residual Deviance: 676.9     AIC: 373.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(summary(bf_step)$coefficients, caption = &amp;quot;Coefficients from step function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Coefficients from step function&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.7196776&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.0140437&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-14.303760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;waistcirc&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2037314&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0609069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.344966&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0013605&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;hipcirc&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3546222&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0767978&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.617607&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000185&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;kneebreadth&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8047490&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6611504&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.729710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0081187&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;anthro3b&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.1264238&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1295927&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.308844&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
cat(&amp;quot;# 1B&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1B&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

bodyfat_gam &amp;lt;- gam(DEXfat ~ s(age) + s(waistcirc) + s(hipcirc) +
                     s(elbowbreadth) + s(kneebreadth) + s(anthro3a) + s(anthro3c),      
                   data = bodyfat)

# Assess Summary
summary(bodyfat_gam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## DEXfat ~ s(age) + s(waistcirc) + s(hipcirc) + s(elbowbreadth) + 
##     s(kneebreadth) + s(anthro3a) + s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  30.7828     0.2847   108.1   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                   edf Ref.df      F  p-value    
## s(age)          1.000  1.000  0.956 0.332964    
## s(waistcirc)    1.000  1.000 10.821 0.001844 ** 
## s(hipcirc)      1.775  2.235  9.917 0.000152 ***
## s(elbowbreadth) 1.000  1.000  0.001 0.972242    
## s(kneebreadth)  8.754  8.960  6.180 3.59e-06 ***
## s(anthro3a)     1.000  1.000 12.966 0.000725 ***
## s(anthro3c)     7.042  8.041  1.798 0.100242    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.953   Deviance explained = 96.7%
## GCV = 8.4354  Scale est. = 5.7538    n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Assess plot
cat (&amp;quot;Plot of a generalized additive model for question 1b:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plot of a generalized additive model for question 1b:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:9, ncol = 3))
plot(bodyfat_gam)

# plot looks like we don&amp;#39;t need to smooth all variables. These appear linear:
# age, waistcirc, elbowbreadth &amp;amp; anthro3a don&amp;#39;t need smoothing


# Question: How do we evaluate a covariate&amp;#39;s &amp;#39;informative&amp;#39;-ness?
# Annwer: Look at the p-values of the covariates.


# Report model attributes
cat(&amp;quot;GCV&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam$gcv.ubre            # GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GCV.Cp 
## 8.435412&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam$aic                 # AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 345.708&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Adjusted R-squared&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted R-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bodyfat_gam)$r.sq       # Adjusted r-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9528156&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Total degrees of Freedom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Total degrees of Freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(bodyfat_gam$edf)            # Total degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22.57091&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Some diagnostics for a fitted gam model using cam.check&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Some diagnostics for a fitted gam model using cam.check&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gam.check(bodyfat_gam)  # Response Vs Fitted values&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 41 iterations.
## The RMS GCV score gradient at convergence was 2.767255e-07 .
## The Hessian was positive definite.
## Model rank =  64 / 64 
## 
## Basis dimension (k) checking results. Low p-value (k-index&amp;lt;1) may
## indicate that k is too low, especially if edf is close to k&amp;#39;.
## 
##                   k&amp;#39;  edf k-index p-value  
## s(age)          9.00 1.00    0.81   0.040 *
## s(waistcirc)    9.00 1.00    0.94   0.260  
## s(hipcirc)      9.00 1.78    1.02   0.560  
## s(elbowbreadth) 9.00 1.00    0.81   0.035 *
## s(kneebreadth)  9.00 8.75    1.08   0.690  
## s(anthro3a)     9.00 1.00    1.09   0.770  
## s(anthro3c)     9.00 7.04    0.89   0.185  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Additionally we can also check for the train() method. Please note that
# train() won&amp;#39;t allow for smoothing the variables; it evidently decides which to
# smooth on its own.
bf_train_gam &amp;lt;- train(DEXfat ~ age +  waistcirc + hipcirc + elbowbreadth + 
                        kneebreadth + anthro3a + anthro3c,      
                      data = bodyfat, method = &amp;quot;gam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: model fit failed for Resample22: select= TRUE, method=GCV.Cp Error in magic(G$y, G$X, msp, G$S, G$off, L = G$L, lsp0 = G$lsp0, G$rank,  : 
##   magic, the gcv/ubre optimizer, failed to converge after 400 iterations.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bf_train_gam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## .outcome ~ s(elbowbreadth) + s(kneebreadth) + s(age) + s(hipcirc) + 
##     s(waistcirc) + s(anthro3a) + s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  30.7828     0.2968   103.7   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                       edf Ref.df     F  p-value    
## s(elbowbreadth) 3.186e-07      9 0.000   0.8204    
## s(kneebreadth)  7.825e+00      9 6.978 5.34e-08 ***
## s(age)          6.695e-01      9 0.374   0.0271 *  
## s(hipcirc)      1.843e+00      9 3.740 1.35e-08 ***
## s(waistcirc)    7.598e-01      9 2.512 2.53e-07 ***
## s(anthro3a)     6.742e-01      9 1.446 5.61e-06 ***
## s(anthro3c)     5.959e-01      9 0.975 4.41e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.949   Deviance explained = 95.8%
## GCV = 7.7062  Scale est. = 6.2553    n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# It looks like train() decided to smooth all the variables.
# This resulted in a less effective model.

#
cat(&amp;quot;# 1C&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1C&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
# Removing insignificant elbowbreadth including age variables
bodyfat_gam2 &amp;lt;- gam(DEXfat ~ waistcirc + s(hipcirc) +
                      s(kneebreadth) + s(anthro3a) + s(anthro3c),      
                    data = bodyfat)
# Assess Summary
summary(bodyfat_gam2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## DEXfat ~ waistcirc + s(hipcirc) + s(kneebreadth) + s(anthro3a) + 
##     s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 13.60862    4.74887   2.866 0.006052 ** 
## waistcirc    0.19654    0.05425   3.623 0.000676 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                  edf Ref.df      F  p-value    
## s(hipcirc)     1.610  2.010 10.910 0.000103 ***
## s(kneebreadth) 8.793  8.970  6.780 2.48e-06 ***
## s(anthro3a)    1.000  1.000 18.035 8.73e-05 ***
## s(anthro3c)    7.117  8.103  2.126 0.048737 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.954   Deviance explained = 96.7%
## GCV = 7.9464  Scale est. = 5.6498    n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wastcirc and anthro3a are no longer the most signifcant parameter.

# Assess plot
cat(&amp;quot;Plots for the model after removing insignificant variables&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plots for the model after removing insignificant variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:4, ncol = 2))
plot(bodyfat_gam2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot looks like we don&amp;#39;t need to smooth all variables.  These appear linear:
# anthro3a and (possibly) hipcirc


# Report model attributes
cat(&amp;quot;GCV of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GCV of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam2$gcv.ubre            # GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GCV.Cp 
## 7.946447&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam2$aic                 # AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 343.2562&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Adjusted r-squared of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted r-squared of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bodyfat_gam2)$r.sq       # Adjusted r-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9536683&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Total Degrees of freedom of bodyfat_gam2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Total Degrees of freedom of bodyfat_gam2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(bodyfat_gam2$edf)            # Total degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20.52&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run gam.check: is the normality assumption violated?
gam.check(bodyfat_gam2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 31 iterations.
## The RMS GCV score gradient at convergence was 7.054782e-07 .
## The Hessian was positive definite.
## Model rank =  38 / 38 
## 
## Basis dimension (k) checking results. Low p-value (k-index&amp;lt;1) may
## indicate that k is too low, especially if edf is close to k&amp;#39;.
## 
##                  k&amp;#39;  edf k-index p-value
## s(hipcirc)     9.00 1.61    1.01    0.48
## s(kneebreadth) 9.00 8.79    1.06    0.70
## s(anthro3a)    9.00 1.00    1.11    0.80
## s(anthro3c)    9.00 7.12    0.91    0.17&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#
cat(&amp;quot;# 1D&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1D&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

log_transferred_DEX &amp;lt;- log(bodyfat$DEXfat)
df &amp;lt;- cbind (bodyfat, log_transferred_DEX)

bodyfat_gam3 &amp;lt;- gam(log_transferred_DEX ~ waistcirc + s(hipcirc) +
                      s(kneebreadth) + s(anthro3a) + s(anthro3c),      
                    data = df)
# Assess Summary
summary(bodyfat_gam3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## log_transferred_DEX ~ waistcirc + s(hipcirc) + s(kneebreadth) + 
##     s(anthro3a) + s(anthro3c)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 2.973535   0.158102  18.808   &amp;lt;2e-16 ***
## waistcirc   0.004418   0.001806   2.447   0.0176 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                  edf Ref.df      F  p-value    
## s(hipcirc)     2.909  3.616 11.828  8.8e-07 ***
## s(kneebreadth) 2.325  2.962  2.027 0.128320    
## s(anthro3a)    1.000  1.000 15.576 0.000217 ***
## s(anthro3c)    7.358  8.263  4.678 0.000144 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.952   Deviance explained = 96.2%
## GCV = 0.0088137  Scale est. = 0.006878  n = 71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hipcirc, anthro3a and anthro3c are the most significant smoothed terms and
# Kneebreadth is not significant here. Similarly, waistcirc is barely signifcant
# parameter (&amp;lt;0.005).

# Assess plot
#layout(matrix(1:4, ncol = 2))
#plot(bodyfat_gam3)

# plot looks like we don&amp;#39;t need to smooth all variables.  These appear linear:
# anthro3a.  Hipcirc is prominantly NOT linear in this light.


# Report model attributes
cat(&amp;quot;GCV of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GCV of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam3$gcv.ubre            # GCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      GCV.Cp 
## 0.008813659&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_gam3$aic                 # AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -136.47&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Adjusted R-square of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted R-square of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(bodyfat_gam3)$r.sq       # Adjusted r-squared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9522733&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Total degrees of freedom of bodyfat_gam3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Total degrees of freedom of bodyfat_gam3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(bodyfat_gam3$edf)            # Total degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15.59274&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run gam.check: is the normality assumption violated?
gam.check(bodyfat_gam3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 11 iterations.
## The RMS GCV score gradient at convergence was 5.573874e-08 .
## The Hessian was positive definite.
## Model rank =  38 / 38 
## 
## Basis dimension (k) checking results. Low p-value (k-index&amp;lt;1) may
## indicate that k is too low, especially if edf is close to k&amp;#39;.
## 
##                  k&amp;#39;  edf k-index p-value  
## s(hipcirc)     9.00 2.91    0.86    0.10 .
## s(kneebreadth) 9.00 2.33    0.83    0.09 .
## s(anthro3a)    9.00 1.00    1.00    0.48  
## s(anthro3c)    9.00 7.36    0.99    0.42  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Plots for the model with log transferred response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Plots for the model with log transferred response&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,4))
plot(bodyfat_gam3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot(bodyfat_gam,select=1)
# plot(bodyfat_gam2,select=1)
# plot(bodyfat_gam3,select=1)
# plot(bodyfat_gam3,select=4)
# #compare with plots
# ggplot(bodyfat) + geom_line(aes(x=seq(1:nrow(bodyfat)), y=DEXfat), lty=1, col=1) +
#   geom_line(aes(x=seq(1:nrow(bodyfat)), y=bodyfat_gam$fitted.values), lty=2, col=2) +
#   geom_line(aes(x=seq(1:nrow(bodyfat)), y=bodyfat_gam2$fitted.values), lty=3, col=3) +
#   geom_line(aes(x=seq(1:nrow(bodyfat)), y=exp(bodyfat_gam2$fitted.values)), lty=4, col=4)
# 
# ggplot(bodyfat, aes(x=seq(1:nrow(bodyfat)), y=bodyfat_gam$fitted.values)) + geom_line(lty=2, col=6)
#
cat(&amp;quot;# 1E&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1E&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#

bodyfat_boost &amp;lt;- gamboost(DEXfat ~ ., data=bodyfat)
bodyfat_aic &amp;lt;- AIC(bodyfat_boost)
cat(&amp;quot;printing AIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## printing AIC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bodyfat_aic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.268173
## Optimal number of boosting iterations: 51 
## Degrees of freedom (for mstop = 51): 7.637287&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_gam &amp;lt;- bodyfat_boost[mstop(bodyfat_aic)]

cat(&amp;quot;plots for model fitted using gamboost:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## plots for model fitted using gamboost:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:9, ncol = 3))
plot(bf_gam)

cat(&amp;quot;extract variable names&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## extract variable names&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract(bf_gam,what=&amp;#39;variable.names&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bbs(waistcirc, df = dfbase)      bbs(hipcirc, df = dfbase) 
##                    &amp;quot;waistcirc&amp;quot;                      &amp;quot;hipcirc&amp;quot; 
## bbs(elbowbreadth, df = dfbase)  bbs(kneebreadth, df = dfbase) 
##                 &amp;quot;elbowbreadth&amp;quot;                  &amp;quot;kneebreadth&amp;quot; 
##     bbs(anthro3a, df = dfbase)     bbs(anthro3b, df = dfbase) 
##                     &amp;quot;anthro3a&amp;quot;                     &amp;quot;anthro3b&amp;quot; 
##     bbs(anthro3c, df = dfbase)      bbs(anthro4, df = dfbase) 
##                     &amp;quot;anthro3c&amp;quot;                      &amp;quot;anthro4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Here, variable &amp;#39;age&amp;#39; was removed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Here, variable &amp;#39;age&amp;#39; was removed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;#1a&lt;/p&gt;
&lt;p&gt;From the plot, it appears that age and elbowbreadth don’t correlate with DEXfat.
However, waistcirc, hipcirc, and all four anthro variables appear to correlate with DEXfat. Based on the plot, I would select waistcirc, hipcirc, anthro3a and anthro3c. However, I would also test the model using anthro3b rather than anthro3a to see which yields a better model.
Additionally, stepwise regression using step() function recommended: kneebreadth, waistcirc, hipcirc and anthro3b. This also confirm that the anthro variables are sufficiently correlated to be represented by one variable which in this case is anthro3b.&lt;/p&gt;
&lt;p&gt;#1b.&lt;/p&gt;
&lt;p&gt;Not all covariates should be smoothed. Looking at the plot, the covariates that appear linear are: age, elbowbreadth, waistcirc and anthro3a. That leaves anthro3c, kneebreadth and hipcirc to be smoothed.&lt;/p&gt;
&lt;p&gt;Report model attributes:
GCV
8.435412
AIC
345.708
Adjusted R-squared
0.9528156
Total degrees of Freedom
22.57091&lt;/p&gt;
&lt;p&gt;gam.check:
While the residuals plot appears random, the histogram makes the data appear skewed to the left, so the assumption of normality doesn’t entirely hold.&lt;/p&gt;
&lt;p&gt;This summary shows that age, elbowbreadth, and anthro3c are not significant at
the significance level of 0.05. The variables age, waistcirc, elbowbreadth and
anthro3a all have linear relationships as shown in the plots. The model seems
to give a moderate GCV and high AIC which could possibily be adjusted by
variable selection and using smoothing functions on the variables mentioned
above. The adjusted R2 does indicate that the model explains most of the
variance, but as stated previously the model can improve.&lt;/p&gt;
&lt;div id=&#34;c.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1c.&lt;/h1&gt;
&lt;p&gt;GCV of bodyfat_gam2:
7.946447
AIC of bodyfat_gam2:
343.2562
Adjusted r-squared of bodyfat_gam2:
0.9536683
Total Degrees of freedom of bodyfat_gam2:
20.52&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1d.&lt;/h1&gt;
&lt;p&gt;In this case, the log-transformed model is slightly better in that it accounts for slightly more of the deviation.
Report GCV, AIC, adj-R2, and the total model degrees of freedom.
GCV: 0.0088
AIC: -136.4700
adj-R2: 0.9523
Total DF: 15.5927&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1e.&lt;/h1&gt;
&lt;p&gt;Age variable was removed by gamboost() function.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit a logistic additive model to the glaucoma data. (Here use family = “binomial”). Which covariates should enter the model and how is their influence on the probability of suffering from glaucoma? (Hint: since there are many covariates, use  to fit the GAM model.)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;TH.data&amp;quot;)
data(&amp;quot;GlaucomaM&amp;quot;)
# cat(&amp;quot;head(GlaucomaM)&amp;quot;)
# head(GlaucomaM)
# nrow(GlaucomaM) # 196
# names(GlaucomaM)

glau_gamb &amp;lt;- gamboost(Class ~., data = GlaucomaM, family = Binomial())

summary(glau_gamb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Model-based Boosting
## 
## Call:
## gamboost(formula = Class ~ ., data = GlaucomaM, family = Binomial())
## 
## 
##   Negative Binomial Likelihood (logit link) 
## 
## Loss function: { 
##      f &amp;lt;- pmin(abs(f), 36) * sign(f) 
##      p &amp;lt;- exp(f)/(exp(f) + exp(-f)) 
##      y &amp;lt;- (y + 1)/2 
##      -y * log(p) - (1 - y) * log(1 - p) 
##  } 
##  
## 
## Number of boosting iterations: mstop = 100 
## Step size:  0.1 
## Offset:  0 
## Number of baselearners:  62 
## 
## Selection frequencies:
##  bbs(tmi, df = dfbase) bbs(mhcg, df = dfbase) bbs(vars, df = dfbase) 
##                   0.17                   0.11                   0.11 
## bbs(mhci, df = dfbase)  bbs(hvc, df = dfbase) bbs(vass, df = dfbase) 
##                   0.10                   0.08                   0.08 
##   bbs(as, df = dfbase) bbs(vari, df = dfbase)   bbs(mv, df = dfbase) 
##                   0.07                   0.06                   0.04 
## bbs(abrs, df = dfbase) bbs(mhcn, df = dfbase) bbs(phcn, df = dfbase) 
##                   0.03                   0.03                   0.03 
##  bbs(mdn, df = dfbase) bbs(phci, df = dfbase)  bbs(hic, df = dfbase) 
##                   0.03                   0.02                   0.01 
## bbs(phcg, df = dfbase)  bbs(mdi, df = dfbase)  bbs(tms, df = dfbase) 
##                   0.01                   0.01                   0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:9, ncol = 3))
plot(glau_gamb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Covariates that should enter the model as determined by gamboost:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Covariates that should enter the model as determined by gamboost:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract(glau_gamb,what=&amp;#39;variable.names&amp;#39;)
var_names  &amp;lt;- unname(extract(glau_gamb,what=&amp;#39;variable.names&amp;#39;))
var_names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;as&amp;quot;   &amp;quot;abrs&amp;quot; &amp;quot;hic&amp;quot;  &amp;quot;mhcg&amp;quot; &amp;quot;mhcn&amp;quot; &amp;quot;mhci&amp;quot; &amp;quot;phcg&amp;quot; &amp;quot;phcn&amp;quot; &amp;quot;phci&amp;quot; &amp;quot;hvc&amp;quot; 
## [11] &amp;quot;vass&amp;quot; &amp;quot;vars&amp;quot; &amp;quot;vari&amp;quot; &amp;quot;mdn&amp;quot;  &amp;quot;mdi&amp;quot;  &amp;quot;tms&amp;quot;  &amp;quot;tmi&amp;quot;  &amp;quot;mv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Using the variables indicated by gamboost, run a gam model to get summary data
# phcg and phci seem smooth already
glau_gam &amp;lt;- gam(Class ~ s(as) + s(abrs)   + s(hic)  + s(mhcg)  + s(mhcn) + s(mhci)             +   phcg +  s(phcn)  + phci + s(hvc) + s(vass) + s(vars) + s(vari) +                   s(mdn) + s(mdi) + s(tms) +  s(tmi) + s(mv), 
                data=GlaucomaM, family=binomial)

summary(glau_gam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: binomial 
## Link function: logit 
## 
## Formula:
## Class ~ s(as) + s(abrs) + s(hic) + s(mhcg) + s(mhcn) + s(mhci) + 
##     phcg + s(phcn) + phci + s(hvc) + s(vass) + s(vars) + s(vari) + 
##     s(mdn) + s(mdi) + s(tms) + s(tmi) + s(mv)
## 
## Parametric coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    22.55    1581.67   0.014    0.989
## phcg          512.85    7725.61   0.066    0.947
## phci         -290.45    7534.66  -0.039    0.969
## 
## Approximate significance of smooth terms:
##           edf Ref.df Chi.sq p-value
## s(as)   2.095  2.179  0.019   0.995
## s(abrs) 1.000  1.000  0.002   0.963
## s(hic)  1.000  1.000  0.003   0.957
## s(mhcg) 1.522  1.562  0.001   0.997
## s(mhcn) 1.278  1.306  0.001   0.994
## s(mhci) 2.684  2.771  0.002   1.000
## s(phcn) 4.000  4.058  0.006   1.000
## s(hvc)  5.579  5.643  0.020   1.000
## s(vass) 1.000  1.000  0.031   0.861
## s(vars) 1.000  1.000  0.002   0.961
## s(vari) 2.761  2.828  0.013   0.999
## s(mdn)  1.000  1.000  0.001   0.980
## s(mdi)  1.000  1.000  0.003   0.957
## s(tms)  1.000  1.000  0.000   0.995
## s(tmi)  2.618  2.695  0.027   0.997
## s(mv)   1.000  1.000  0.007   0.935
## 
## R-sq.(adj) =      1   Deviance explained =  100%
## UBRE = -0.65774  Scale est. = 1         n = 196&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;Covariates indicated by gamboost() were used to generate a gam model. The names are listed in the summary printed above. The summary also shows the probability of their influence (somewhat high) for suffering from glaucoma. I think with 100 percent of the deviance explained, there should be a concern that this model is extremely over-fitting.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Investigate the use of different types of scatterplot smoothers on the Hubble data from Chapter 6. (Hint: follow the example on men1500m data scattersmoothers page 199 of Handbook).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## GAM model
library(&amp;quot;HSAUR3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;HSAUR3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:TH.data&amp;#39;:
## 
##     birds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;mgcv&amp;quot;)
library(&amp;quot;GGally&amp;quot;)
library(&amp;quot;mboost&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;wordcloud&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: RColorBrewer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data(&amp;quot;bodyfat&amp;quot;, package = &amp;quot;TH.data&amp;quot;)
data(&amp;quot;hubble&amp;quot;, package = &amp;quot;gamair&amp;quot;)
head(hubble)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Galaxy    y     x
## 1  NGC0300  133  2.00
## 2  NGC0925  664  9.16
## 3 NGC1326A 1794 16.14
## 4  NGC1365 1594 17.95
## 5  NGC1425 1473 21.88
## 6  NGC2403  278  3.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sorted_value&amp;lt;-hubble[order(hubble$x),]
x &amp;lt;- sorted_value$x
y &amp;lt;- sorted_value$y

lowess_value &amp;lt;- lowess(x, y)
plot(y~x, data = sorted_value, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;,main=&amp;quot;base R: Lowess scatterplot smoother&amp;quot;)
lines(lowess_value, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x=x, y=y) + 
  geom_point()+
  geom_line(aes(x = lowess_value$x, y = lowess_value$y)) +
    labs(title = &amp;quot;ggplot: Lowess scatterplot smoother&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(y~x, data = sorted_value, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;,main=&amp;quot;base R: Cubic scatterplot smoother&amp;quot;)
cubic_value = gam(y ~ s(x, bs = &amp;quot;cr&amp;quot;), data = sorted_value)
lines(sorted_value$x, predict(cubic_value), lty=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x=x, y=y) + 
  geom_point()+
  geom_line(aes(x = sorted_value$x, y = predict(cubic_value))) +
    labs(title = &amp;quot;ggplot: Cubic scatterplot smoother&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cubic_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ s(x, bs = &amp;quot;cr&amp;quot;)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   924.38      51.87   17.82 4.27e-14 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##        edf Ref.df    F  p-value    
## s(x) 2.148  2.648 26.8 2.88e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.754   Deviance explained = 77.7%
## GCV =  74317  Scale est. = 64570     n = 24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(y~x, data = sorted_value, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;,main=&amp;quot;base R: Quadratic model scatterplot smoother&amp;quot;)
lm_value = lm(y~x+I(x^2), data = sorted_value)
lines(sorted_value$x, predict(lm_value))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x=x, y=y) + 
  geom_point()+
  geom_line(aes(x = sorted_value$x, y = predict(lm_value))) +
    labs(title = &amp;quot;ggplot: Quadratic scatterplot smoother&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Generalized_Additive_Models_and_Spline_Models_files/figure-html/unnamed-chunk-3-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussions:&lt;/p&gt;
&lt;p&gt;The 3 different smoothers line shows that quadratic is higher than cubic and lowess, while cubic is higher than lowess. However, I am not sure if it would be logical to explain the smoother one fits better or not.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression&amp;GLM-II</title>
      <link>/achalneupane.github.io/post/logistic_regression_glm_ii/</link>
      <pubDate>Sat, 21 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/logistic_regression_glm_ii/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the  data from the  library to answer the following questions&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Construct graphical and numerical summaries that will show the relationship between tumor size and the number of recurrent tumors. Discuss your discovery. (Hint: mosaic plot may be a great way to assess this)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build a Poisson regression that estimates the effect of size of tumor on the number of recurrent tumors. Discuss your results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;bladdercancer&amp;quot;, package = &amp;quot;HSAUR3&amp;quot;)
# base R plot version
# head(bladdercancer)
cat(&amp;quot;#1a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #1a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mosaicplot(xtabs( ~ number + tumorsize, data = bladdercancer),
           main = &amp;quot;base R: The Number of recurrent tumors compared with tumor size&amp;quot;,
           shade = TRUE)

# ggplot version:
# install.packages(&amp;#39;ggmosaic&amp;#39;)
library(ggmosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = bladdercancer) +
  geom_mosaic(aes(x = product(tumorsize, number), fill = tumorsize), na.rm =
                FALSE) +
  labs(x = &amp;quot;Number&amp;quot;, x = &amp;quot;Tumour Size&amp;quot;, title = &amp;#39;ggplot: The Number of recurrent tumors compared with tumor size&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can also visualize this by creating percentage table using `prop.table`
# function.
table_rows_percentage &amp;lt;-
  table(bladdercancer$tumorsize, bladdercancer$number)
colnames(table_rows_percentage) &amp;lt;-
  c(&amp;quot;Tumour_1 (counts)&amp;quot;,
    &amp;quot;Tumour_2 (counts)&amp;quot;,
    &amp;quot;Tumour_3 (counts)&amp;quot;,
    &amp;quot;Tumour_4 (counts)&amp;quot;)
cat(&amp;quot;Table of tumour number and frequency:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Table of tumour number and frequency:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table_rows_percentage&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Tumour_1 (counts) Tumour_2 (counts) Tumour_3 (counts) Tumour_4 (counts)
##   &amp;lt;=3cm                15                 5                 1                 1
##   &amp;gt;3cm                  5                 2                 1                 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bladdercancer %&amp;gt;%
#   group_by(tumorsize,number) %&amp;gt;%
#   summarize(freq = n()) %&amp;gt;%
#   spread(number,freq,sep=&amp;#39;_of_tumors_&amp;#39;)
tt &amp;lt;- prop.table(table_rows_percentage, 1)
colnames(tt) &amp;lt;-
  c(&amp;quot;Tumour_1(%)&amp;quot;, &amp;quot;Tumour_2(%)&amp;quot;, &amp;quot;Tumour_3(%)&amp;quot;, &amp;quot;Tumour_4(%)&amp;quot;)
cat(&amp;quot;Table of tumour number and frequency in %:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Table of tumour number and frequency in %:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Tumour_1(%) Tumour_2(%) Tumour_3(%) Tumour_4(%)
##   &amp;lt;=3cm  0.68181818  0.22727273  0.04545455  0.04545455
##   &amp;gt;3cm   0.55555556  0.22222222  0.11111111  0.11111111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1a. &lt;strong&gt;Discussion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Based on the mosaic plot, frequency table or the percentage table above, we can tell that the observed frequency for 1 or 2 tumors greater than 3cm (&amp;gt;3cm) is lower than expected and the observed frequency for 3 or 4 tumors less than or equal to 3 cm (&amp;lt;=3cm) is also lower than what we would expect for this data.&lt;/p&gt;
&lt;p&gt;1b.&lt;/p&gt;
&lt;p&gt;Building a Poisson regression that estimates the effect of size of tumor on
the number of recurrent tumors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- glm(number ~ tumorsize,data=bladdercancer,family=poisson())
summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = number ~ tumorsize, family = poisson(), data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6363  -0.3996  -0.3996   0.4277   1.7326  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)     0.3747     0.1768   2.120    0.034 *
## tumorsize&amp;gt;3cm   0.2007     0.3062   0.655    0.512  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.80  on 30  degrees of freedom
## Residual deviance: 12.38  on 29  degrees of freedom
## AIC: 87.191
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;model1 (mod1): If we test the model dropping the time variable. It shows that the intercept is significant (P&amp;lt;0.05), but the tumour size is not significant.&lt;/p&gt;
&lt;p&gt;Additionally, we can also test models considering the time interaction&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- glm(number ~time + tumorsize + tumorsize*time,data=bladdercancer,family=poisson(link=log))
summary(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = number ~ time + tumorsize + tumorsize * time, family = poisson(link = log), 
##     data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6943  -0.5581  -0.2413   0.2932   1.4644  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)         0.03957    0.43088   0.092    0.927
## time                0.02138    0.02418   0.884    0.377
## tumorsize&amp;gt;3cm       0.46717    0.66713   0.700    0.484
## time:tumorsize&amp;gt;3cm -0.01676    0.03821  -0.439    0.661
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.800  on 30  degrees of freedom
## Residual deviance: 11.566  on 27  degrees of freedom
## AIC: 90.377
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;model2 (mod2): If we consider time interaction with the tumour size, we can clearly see that none of the variables are significant.&lt;/p&gt;
&lt;p&gt;If we remove time interaction from above model, we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod3 &amp;lt;- glm(number ~ time + tumorsize,data=bladdercancer,family=poisson())
summary(mod3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = number ~ time + tumorsize, family = poisson(), 
##     data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8183  -0.4753  -0.2923   0.3319   1.5446  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    0.14568    0.34766   0.419    0.675
## time           0.01478    0.01883   0.785    0.433
## tumorsize&amp;gt;3cm  0.20511    0.30620   0.670    0.503
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.800  on 30  degrees of freedom
## Residual deviance: 11.757  on 28  degrees of freedom
## AIC: 88.568
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;model3 (mod3): If we drop time interaction from previous model (mod2), we still do not get anything significant with the time or tumour size. However, the AIC value drops to 88.56.&lt;/p&gt;
&lt;p&gt;In all three models we compared, we can also see that the residual and null deviance values are low compared to the degrees of freedom. If our Null Deviance is really small, it means that the Null Model explains the data pretty well. Likewise with your Residual Deviance.&lt;/p&gt;
&lt;p&gt;Additionaly, we can perform a Chi-squared test for the Null deviance to check
whether any of the predictors have an influence on the response variables in
our three models using function &lt;code&gt;pchisq&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Source: https://stat.ethz.ch/education/semesters/as2015/asr/Uebungen/Uebungen/solution8.pdf
pchisq((mod1$null.deviance-mod1$deviance), df = (mod1$df.null-mod1$df.residual), lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5171827&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pchisq((mod2$null.deviance-mod2$deviance), df = (mod2$df.null-mod2$df.residual), lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7448414&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pchisq((mod3$null.deviance-mod3$deviance), df  = (mod3$df.null-mod3$df.residual), lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5935891&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-values in all three models are larger than 0.05, which tells us that there is no significant predictor in our model.&lt;/p&gt;
&lt;p&gt;Additionally, if we can compare all three models we built above for analysis of deviance using ANOVA:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod1,mod2,mod3,test=&amp;#39;Chisq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: number ~ tumorsize
## Model 2: number ~ time + tumorsize + tumorsize * time
## Model 3: number ~ time + tumorsize
##   Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)
## 1        29     12.380                     
## 2        27     11.566  2  0.81458   0.6655
## 3        28     11.757 -1 -0.19095   0.6621&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1b. &lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here as well, we do not find any of these models to be significant.&lt;/p&gt;
&lt;p&gt;Final conclusion: Based on these analysis we can tell that the acceptance of the null hypothesis is evident in this case because there is nothing within the data to explain an increment in the number of tumors. Since we tested both tumour size and time variables, we can tell that &lt;strong&gt;neither time&lt;/strong&gt; nor the &lt;strong&gt;tumour size&lt;/strong&gt; have any effect on increasing &lt;strong&gt;number&lt;/strong&gt; of tumours.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The following data is the number of new AIDS cases in Belgium between the years 1981-1993. Let &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; denote time
&lt;p&gt;Do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Plot the relationship between AIDS cases against time. Comment on the plot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a Poisson regression model &lt;span class=&#34;math inline&#34;&gt;\(log(\mu_i)=\beta_0+\beta_1t_i\)&lt;/span&gt;. Comment on the model parameters and residuals (deviance) vs Fitted plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now add a quadratic term in time ( ) and fit the model. Comment on the model parameters and assess the residual plots.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare the two models using AIC. Which model is better?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use -function to perform &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; test for model selection. Did adding the quadratic term improve model?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240)
t = 1:13

data &amp;lt;- as.data.frame(cbind(t, y))

cat(&amp;quot;#2a (base R plot version)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2a (base R plot version)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(y ~ t,
     main = &amp;quot;base R: Number of AIDs cases from 1981-1993&amp;quot;,
     xlab = &amp;quot;Time in Years from 1981&amp;quot;,
     ylab = &amp;quot;Number of Aids cases&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2a ggplot version&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2a ggplot version&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(x = t, y = y) + geom_point() + labs(title = &amp;quot;ggplot: Number of AIDs cases from 1981-1993&amp;quot;, x = &amp;quot;Time in Years from 1981&amp;quot;, y = &amp;quot;Number of Aids cases&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Poisson model
aids.pois &amp;lt;- glm(y ~ t, data = data, family = &amp;quot;poisson&amp;quot;)
summary(aids.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ t, family = &amp;quot;poisson&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.6784  -1.5013  -0.2636   2.1760   2.7306  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) 3.140590   0.078247   40.14   &amp;lt;2e-16 ***
## t           0.202121   0.007771   26.01   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 872.206  on 12  degrees of freedom
## Residual deviance:  80.686  on 11  degrees of freedom
## AIC: 166.37
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coefficients
exp(coef(aids.pois)) # coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           t 
##   23.117491    1.223996&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(confint(aids.pois)) # confidence interval&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 2.5 %    97.5 %
## (Intercept) 19.789547 26.894433
## t            1.205624  1.242922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#use code below for residual plots
plot(aids.pois, which = 1, main = &amp;quot;base R: Residual Vs fitted plot for y ~ t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version
# https://stackoverflow.com/questions/36731027/how-can-i-plot-the-residuals-of-lm-with-ggplot

# cc &amp;lt;- data.frame(aids.pois$residuals, aids.pois$fitted.values)
#
# ggplot(cc, aes(x = aids.pois.fitted.values, y = aids.pois.residuals)) +
#   geom_point() +
#   geom_abline()

# ggplot(cc, aes(x = aids.pois.fitted.values, y = aids.pois.residuals)) +
#   geom_smooth(method=&amp;quot;loess&amp;quot;, color=&amp;quot;red&amp;quot;, se=FALSE) +
#   geom_hline(yintercept = 0, linetype=2, color=&amp;quot;darkgrey&amp;quot;) +
#   geom_point()+ labs(title = &amp;quot;ggplot: Residual Vs fitted plot&amp;quot;)

# ggplot version
ggplot(aids.pois, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth(group = 1, formula = y ~ x) + labs(title = &amp;quot;ggplot: Residual Vs fitted plot for y ~ t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2c&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$t2 &amp;lt;- data$t ^ 2
aids2.pois &amp;lt;- glm(y ~ t + t2, data = data, family = &amp;quot;poisson&amp;quot;)
summary(aids2.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ t + t2, family = &amp;quot;poisson&amp;quot;, data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.45903  -0.64491   0.08927   0.67117   1.54596  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  1.901459   0.186877  10.175  &amp;lt; 2e-16 ***
## t            0.556003   0.045780  12.145  &amp;lt; 2e-16 ***
## t2          -0.021346   0.002659  -8.029 9.82e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 872.2058  on 12  degrees of freedom
## Residual deviance:   9.2402  on 10  degrees of freedom
## AIC: 96.924
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coefficients
exp(coef(aids2.pois)) # coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           t          t2 
##   6.6956535   1.7436895   0.9788799&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(confint(aids2.pois)) # confidence interval&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 2.5 %    97.5 %
## (Intercept) 4.5976982 9.5678396
## t           1.5965138 1.9104525
## t2          0.9737254 0.9839292&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#use code below for residual plots
plot(aids2.pois, which = 1, main = &amp;quot;base R: Residual Vs fitted plot for y ~ t + t2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version
ggplot(aids2.pois, aes(x = .fitted, y = .resid)) + geom_point() + labs(title = &amp;quot;ggplot: Residuals Vs fitted plot for y ~ t + t2&amp;quot;) + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_II_files/figure-html/unnamed-chunk-7-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2d&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2d&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(aids.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 166.3698&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(aids2.pois)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 96.92358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#2e&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #2e&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(aids.pois, aids2.pois, test = &amp;quot;Chisq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: y ~ t
## Model 2: y ~ t + t2
##   Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    
## 1        11     80.686                          
## 2        10      9.240  1   71.446 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2a. The number of new AIDS cases has an increasing trend over time and seems to be leveling off between 1981-1991 then it remains somewhat unchanged until 1993. The maximum number of new AIDS cases occurs in 1991.&lt;/p&gt;
&lt;p&gt;2b. Both (b0) and (b1) are statistically significant from zero.&lt;/p&gt;
&lt;p&gt;Interpretation of the coefficients calculated by exponentiating the estimates:&lt;/p&gt;
&lt;p&gt;exp(b1) =1.22 : A one year increase will result in a 22% increase in the mean number of new AIDs cases.&lt;/p&gt;
&lt;p&gt;exp(b0) =23.1 : When t=0, the average number of AID cases is 23.1.&lt;/p&gt;
&lt;p&gt;Likewise, comparing the residual deviance of the model, we can tell that the model is over-spread by 7.80 times on 11 degrees of freedom. Based on the residual plot, we can tell that at time 1, 2, and 13 the residual values are further away from zero indicating they are outliers. Additionally, there is a clear pattern to the residual plot which indicates that mean does not increase as the variance increase because there is not a constant spread in the residuals.&lt;/p&gt;
&lt;p&gt;Additionally, we can see a curved pattern in the Residual vs. Fitted plot. This could tell us that a transformation or adding a quadratic term to the model would be suitable.&lt;/p&gt;
&lt;p&gt;2c. All the model parameters are statistically significant from zero.&lt;br /&gt;
Interpretation of the coefficients calculated by exponentiating the estimates:&lt;/p&gt;
&lt;p&gt;exp(b1) =1.74: Taking all other parameters constant, a one year increase will result in a 74% increase in the mean number of new AID cases.&lt;/p&gt;
&lt;p&gt;exp(b2) =0.98 : Taking all other parameters constant, a one year increase will result in a 2% decrease in the mean number of new AID cases.&lt;/p&gt;
&lt;p&gt;exp(b0) =6.7 : When t=0 and t^2=0, the average number of AID cases is 6.7.&lt;/p&gt;
&lt;p&gt;Additionally, the residuals vs. fitted values plot looks much better than model one. The residuals seems randomly distributed around 0.&lt;/p&gt;
&lt;p&gt;2d. Based on the AIC values and the residual plots, model 2 is a better fit for this data.&lt;/p&gt;
&lt;p&gt;2e.Based on the chi-square test statistic and p-value—in this case we reject the null hypothesis at the  = 0.05 level that model 1 is true. We can tell that the larger model is better, which in this case, adding the quadratic term did improve the model.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the  dataset from  library. The dataset contains information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt. It is a 4 dimensional dataset with 10000 observations. You had developed a logistic regression model on HW #2. Now consider the following two models
&lt;p&gt;For the two competing models do the following&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;With the whole data compare the two models (Use AIC and/or error rate)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use validation set approach and choose the best model. Be aware that we have few people who defaulted in the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use LOOCV approach and choose the best model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use 10-fold cross-validation approach and choose the best model&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Report validation misclassification (error) rate for both models in each of the three assessment methods. Discuss your results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;Default&amp;quot;, package = &amp;quot;ISLR&amp;quot;)
Default$default&amp;lt;-as.numeric(Default$default==&amp;quot;Yes&amp;quot;)

mod.log1&amp;lt;-glm(default ~ student + balance , data = Default, family = binomial())
# summary(mod.log1)


cat (&amp;quot;#3a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.log2&amp;lt;-glm(default ~ balance , data = Default, family = binomial())
cat(&amp;quot;AIC for mod.log1:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC for mod.log1:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cat(&amp;quot;AIC(mod.log1) =&amp;quot;, AIC(mod.log1))
AIC(mod.log1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1577.682&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;AIC for mod.log2:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AIC for mod.log2:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod.log2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1600.452&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cat(&amp;quot;AIC(mod.log2) =&amp;quot;, AIC(mod.log2))
anova(mod.log1, mod.log2, test=&amp;quot;Chisq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: default ~ student + balance
## Model 2: default ~ balance
##   Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    
## 1      9997     1571.7                          
## 2      9998     1596.5 -1   -24.77 6.459e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#3b Validation approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3b Validation approach&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index&amp;lt;-sample(1:nrow(Default), size=0.6*nrow(Default))
train&amp;lt;- Default[index, ]
val&amp;lt;- Default[-index, ]


mod.train1&amp;lt;-glm(default ~ student + balance , data = train, family = binomial())
# summary(mod.train1)
mod.train2&amp;lt;-glm(default ~ balance , data = train, family = binomial())
# summary(mod.train2)
pred1&amp;lt;-predict(mod.train1, val, type = &amp;quot;response&amp;quot;)
pred2&amp;lt;-predict(mod.train2, val, type = &amp;quot;response&amp;quot;)

cat(&amp;quot;Error rate: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate1&amp;lt;- mean((pred1&amp;gt;0.5 &amp;amp; val$default==0) | (pred1&amp;lt;0.5 &amp;amp; val$default==1))
err.rate2&amp;lt;- mean((pred2&amp;gt;0.5 &amp;amp; val$default==0) | (pred2&amp;lt;0.5 &amp;amp; val$default==1))

cat(&amp;quot;Error rate of model1 =&amp;quot;, err.rate1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate of model1 = 0.027&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Error rate of model2 =&amp;quot;, err.rate2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate of model2 = 0.0265&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#3c LOOCV&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3c LOOCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)

cost &amp;lt;- function(r, pi = 0) mean(abs(r-pi) &amp;gt; 0.5)
cv.err &amp;lt;- cv.glm(Default,mod.log1, cost)$delta
cv.err2 &amp;lt;- cv.glm(Default, mod.log2, cost)$delta
cat(&amp;quot;LOOCV of model1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOOCV of model1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0267 0.0267&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;LOOCV of model2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOOCV of model2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02750000 0.02749994&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#3d 10-fold cross validation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #3d 10-fold cross validation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err1.10 &amp;lt;- cv.glm(Default, mod.log1, cost ,K=10)$delta
cv.err2.10 &amp;lt;- cv.glm(Default, mod.log2, cost ,K=10)$delta

cat(&amp;quot;10-fold cross validation of Model1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10-fold cross validation of Model1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err1.10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02670 0.02675&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;10-fold cross validation of Model2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10-fold cross validation of Model2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.err2.10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02740 0.02735&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3a. The first model with both student and balance has the smaller AIC. The anova-function was also used to perform a chi-square test for model selection and again concluded the first model was better.&lt;/p&gt;
&lt;p&gt;3b. Splitted the data into 60/40 between the training and validation data sets and made sure the default rate was similar between the two dataset, then fitted the models to the training data and then used the validation set to calculate the error rate using 0.5 as out threshold.&lt;/p&gt;
&lt;p&gt;For model 1, the MSE is 0.023.&lt;/p&gt;
&lt;p&gt;For model 2, the MSE is 0.024.&lt;/p&gt;
&lt;p&gt;Based on these values we would chose model 1 as our best model. We will also examine other validation techniques below.&lt;/p&gt;
&lt;p&gt;3c. LOOCV prediction error is adjusted for bias and we still want the smallest prediction errors.
For model 1, the adjusted prediction error is 0.0267.&lt;/p&gt;
&lt;p&gt;For model 2, the adjusted prediction error is 0.02749994.&lt;/p&gt;
&lt;p&gt;Therefore, we choose model 1 as the best model because it has the smaller adjusted prediction rate using the LOOCV approach.&lt;/p&gt;
&lt;p&gt;3d. Using K=10 for the 10-fold cross-validation approach, we obtain the following error rates:&lt;/p&gt;
&lt;p&gt;For model 1, the CV error rate is 0.02667
For model 2, the CV error rate is 0.0278&lt;/p&gt;
&lt;p&gt;Again, we can choose model 1 as our best model. Though it was little easier to calculate the 10-fold cross validation error rate than the LOOCV error rate but our conclusion is the same.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the  library load the  dataset. This contains Daily percentage returns for the S&amp;amp;P 500 stock index between 2001 and 2005. There are 1250 observations and 9 variables. The variable of interest is Direction which is a factor with levels Down and Up indicating whether the market had a positive or negative return on a given day. Since the goal is to predict the direction of the stock market in the future, here it would make sense to use the data from years 2001 - 2004 as training and 2005 as validation. According to this, create a training set and testing set. Perform logistic regression and assess the error rate.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;Smarket&amp;quot;, package = &amp;quot;ISLR&amp;quot;)
Smarket$Direction &amp;lt;- as.numeric(Smarket$Direction == &amp;quot;Up&amp;quot;)

train.mark &amp;lt;- subset(Smarket, Year &amp;lt;= 2004)
val.mark &amp;lt;- subset(Smarket, Year &amp;gt; 2004)


#Model 1
mod.train.mark &amp;lt;-
  glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 ,
      data = train.mark,
      family = binomial())
summary(mod.train.mark)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, family = binomial(), 
##     data = train.mark)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.339  -1.189   1.070   1.163   1.326  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  0.032269   0.063379   0.509    0.611
## Lag1        -0.055510   0.051706  -1.074    0.283
## Lag2        -0.044218   0.051681  -0.856    0.392
## Lag3         0.008918   0.051517   0.173    0.863
## Lag4         0.008556   0.051514   0.166    0.868
## Lag5        -0.003243   0.051089  -0.063    0.949
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1383.3  on 997  degrees of freedom
## Residual deviance: 1381.3  on 992  degrees of freedom
## AIC: 1393.3
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate1 &amp;lt;-
  mean((
    predict(mod.train.mark, val.mark, type = &amp;quot;response&amp;quot;) - val.mark$Direction
  ) ^ 2)
cat(&amp;quot;Error rate model1:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate model1:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2483559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Model 2
mod.train.mark2 &amp;lt;-
  glm(Direction ~ Lag1 + Lag2 + Lag3,
      data = train.mark,
      family = binomial())
summary(mod.train.mark2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3, family = binomial(), 
##     data = train.mark)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.338  -1.189   1.072   1.163   1.335  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  0.032230   0.063377   0.509    0.611
## Lag1        -0.055523   0.051709  -1.074    0.283
## Lag2        -0.044300   0.051674  -0.857    0.391
## Lag3         0.008815   0.051495   0.171    0.864
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1383.3  on 997  degrees of freedom
## Residual deviance: 1381.4  on 994  degrees of freedom
## AIC: 1389.4
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate2 &amp;lt;-
  mean((
    predict(mod.train.mark2, val.mark, type = &amp;quot;response&amp;quot;) - val.mark$Direction
  ) ^ 2)
cat(&amp;quot;Error rate model2:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error rate model2:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err.rate2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2483144&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Discussions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The error rate for model 1 which includes predictor variables lag 1-5 is: 0.4126984.&lt;/p&gt;
&lt;p&gt;The error rate for model 2 which includes predictor variables lag 1-3 is: 0.4087302.&lt;/p&gt;
&lt;p&gt;We can choose the simpler model 2 based on the error rate. This error rate suggests that we are able to predict the direction of the stock market. We can predict the right outcome at around 60% of the time, which is still better than predicting randomly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression&amp;GLM-I</title>
      <link>/achalneupane.github.io/post/logistic_regression_glm_i/</link>
      <pubDate>Wed, 18 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/logistic_regression_glm_i/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how
your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word
Document saved as a PDF). If you are having trouble with .rmd, let us know and
we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please
follow the instructions found under Content for Formatting and Guidelines. No
code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well
as do the same but using the respective commands in the GGPLOT2 library. (So if
the question asks for one plot, your results should have two plots. One produced
using the given R-function and one produced from the GGPLOT2 equivalent). This
doesn’t apply to questions that don’t specifically ask for a plot, however I
still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Collett (2003) argues that two outliers need to be removed from the
 data. Try to identify those two unusual observations by means of
a scatterplot. (7.2 on Handbook)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;calibrate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;HSAUR3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;knitr&amp;quot;)
data(&amp;quot;plasma&amp;quot;)

plasma$rownumber &amp;lt;- 1:nrow(plasma)
plot.new()
# square plotting region, independent of device size; 1 x 1 pictures on one plot
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(
  plasma$fibrinogen,
  plasma$globulin,
  col = plasma$ESR,
  data = plasma,
  pch = 18,
  main = &amp;quot;base R: Scatterplot of plasma data&amp;quot;,
  xlab = &amp;quot;Fibrinogen&amp;quot;,
  ylab = &amp;quot;Globulin&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in plot.window(...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in plot.xy(xy, type, ...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in axis(side = side, at = at, labels = labels, ...): &amp;quot;data&amp;quot; is not a
## graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &amp;quot;data&amp;quot; is not a
## graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in box(...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in title(...): &amp;quot;data&amp;quot; is not a graphical parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text(globulin ~fibrinogen, labels = rownumber,data=plasma, cex=0.8, font=0.5)
abline(v = 3.5)
abline(h = 45)
# points(plasma[c(27, 30, 32), 1:2], pch = 5)
legend(
  &amp;quot;bottomright&amp;quot;,
  c(&amp;quot;ESR&amp;lt;20&amp;quot;, &amp;quot;ESR&amp;gt;20&amp;quot;),
  # title = &amp;quot;ESR&amp;quot;,
  inset = c(0, 1),
  xpd=TRUE, 
  horiz=TRUE,
  col = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;),
  # lty = c(1, 1),
  pch = c(18, 18),
  bty = &amp;quot;n&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = plasma, aes(x = fibrinogen, y = globulin, colour = ESR)) +
  geom_point() +
  ggtitle(&amp;quot;ggplot: Scatterplot of Plasma data&amp;quot;) +
  xlab(&amp;quot;Fibrinogen&amp;quot;) +
  ylab(&amp;quot;Globulin&amp;quot;) +
  geom_text(aes(label=plasma$rownumber),hjust=0, vjust=0) +
  geom_hline(yintercept=45, linetype=&amp;quot;dashed&amp;quot;, 
             color = &amp;quot;black&amp;quot;, size=0.5) +
  geom_vline(xintercept=3.5, linetype=&amp;quot;dashed&amp;quot;, 
             color = &amp;quot;black&amp;quot;, size=0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion: Based on the scatter plot (and if we consider all data points, both ESR &amp;gt; and &amp;lt; 20, together), we could say that rows 27, 30 and 32 as the potential outliers. If we separate the data by ESR &amp;gt;20 and ESR &amp;lt; 20, there may be other outliers for each group. I think we could make box plots by group to determine the outliers more effectively.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;(Multiple Regression) Continuing from the lecture on the  data
from  library;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Fit a quadratic regression model, i.e.,a model of the form
&lt;span class=&#34;math display&#34;&gt;\[\text{Model 2:   } velocity = \beta_1 \times distance + \beta_2 \times distance^2 +\epsilon\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamair)
data(hubble)

# A. Fit a quadratic regression model
model2 &amp;lt;- lm(y~x + I(x^2) -1, data = hubble)
# summary(model2)
kable(summary(model2)$coefficients, caption = &amp;quot;Summary of the model2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Summary of the model2&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.9046424&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.5725817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.4852433&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000164&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;I(x^2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8837375&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9925378&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8903817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3828949&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;b) Plot the fitted curve from Model 2 on the scatterplot of the data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitted curve
# index  &amp;lt;- seq(0, 22, 0.1)
index &amp;lt;- seq(min(hubble$x),max(hubble$x),0.1)
index2 &amp;lt;- index^2
# predicted &amp;lt;- predict(model2,list(x = index, x2=index2))
predicted &amp;lt;- model2$fitted.values
#create a data frame of x nd y values for plotting for ggplot
data &amp;lt;- as.data.frame(cbind(x = hubble$x,predicted))
# Scatter Plot
plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(y~x, data = hubble, main = &amp;quot;base R: Scatter plot with fitted curve from Model2&amp;quot;, xlab = &amp;quot;Distance&amp;quot;, ylab = &amp;quot;Velocity&amp;quot;)
lines(data$x[order(data$x)], data$predicted[order(data$predicted)], col = &amp;quot;green&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = model2, aes(x = model2$model$x, y = model2$model$y)) +
  geom_point() +
  geom_line(aes(x = model2$model$x, y = model2$fitted.values), colour = &amp;quot;green&amp;quot;) +
  labs(title = &amp;quot;ggplot: Scatter plot with fitted curve from Model2&amp;quot;, x = &amp;quot;Distance&amp;quot;, y = &amp;quot;velocity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;c) Add the simple linear regression fit (fitted in class) on this plot - use
different color and line type to differentiate the two and add a legend to
your plot.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Simple lm fitted in class
hmod &amp;lt;- lm(y~x - 1 , data = hubble)
plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(y~x, data = hubble, main = &amp;quot;base R: scatter plot for hubble data&amp;quot;, xlab = &amp;quot;Distance&amp;quot;, ylab = &amp;quot;Velocity&amp;quot;)
lines(data$x[order(data$x)], data$predicted[order(data$predicted)], col = &amp;quot;green&amp;quot;)
abline(hmod, lty=2, col=2)

# Legend
legend(&amp;quot;bottomright&amp;quot;, c(&amp;quot;Quadratic&amp;quot;, &amp;quot;Linear&amp;quot;), lty = 1:2, col = 2:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## ggplot version

ggplot(data = model2, aes(x = model2$model$x, y = model2$model$y)) +
  geom_point() +
  geom_line(aes(x = model2$model$x, y = model2$fitted.values, colour = &amp;quot;Quadratic&amp;quot;)) +
  geom_line(data = hmod, aes(x = hmod$model$x, y = hmod$fitted.values, colour = &amp;quot;Linear&amp;quot;)) +
  labs(title = &amp;quot;ggplot: Scatter plot with fitted curve from Model2&amp;quot;, x = &amp;quot;Distance&amp;quot;, y = &amp;quot;velocity&amp;quot;, colour = &amp;quot;Models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;d) Which model do you consider most sensible considering the nature of the data - looking at the plot? 

Answer: The simple model seems more sensible to me.  The data points seem to
follow a line from lower left to upper right of the plot without a clear
curvature. However, strictly saying, there isn&amp;#39;t much difference between the two models.

e) Which model is better? - provide a statistic to support you claim.

Note: The quadratic model here is still regarded as a ``linear regression&amp;quot;
model since the term ``linear&amp;quot; relates to the parameters of the model and
not to the powers of the explanatory variables.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model2) # # Quadratic regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + I(x^2) - 1, data = hubble)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -713.15 -152.76  -54.85  163.92  557.01 
## 
## Coefficients:
##        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## x       90.9046    16.5726   5.485 1.64e-05 ***
## I(x^2)  -0.8837     0.9925  -0.890    0.383    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 260.1 on 22 degrees of freedom
## Multiple R-squared:  0.944,  Adjusted R-squared:  0.9389 
## F-statistic: 185.3 on 2 and 22 DF,  p-value: 1.715e-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- summary(model2)
summary(hmod)  # Simple linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x - 1, data = hubble)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -736.5 -132.5  -19.0  172.2  558.0 
## 
## Coefficients:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## x   76.581      3.965   19.32 1.03e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 258.9 on 23 degrees of freedom
## Multiple R-squared:  0.9419, Adjusted R-squared:  0.9394 
## F-statistic: 373.1 on 1 and 23 DF,  p-value: 1.032e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hmod.1 &amp;lt;- summary(hmod) 
cat (&amp;quot;Adjusted R-square&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adjusted R-square&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(cbind(Quadratic = mod.2$adj.r.squared, Linear = hmod.1$adj.r.squared), caption = &amp;quot;Adjusted R-square&amp;quot;, row.names = FALSE )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 2: &lt;/span&gt;Adjusted R-square&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Quadratic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Linear&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.9388554&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9394063&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;Answer to 2e. The statistics appear to support the simple model as the
better one. Since the Adjusted r-squared statistic is higher for the simple model
(0.9394) Vs. Quadratic (0.9388554) which indicates that the simple model explains more of the variability in the response data than does the quadratic model.&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The  data from package  shows the survival times
from diagnosis of patients suffering from leukemia and the values of two
explanatory variables, the white blood cell count (wbc) and the presence or
absence of a morphological characteristic of the white blood cells (ag).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Define a binary outcome variable according to whether or not patients
lived for at least 24 weeks after diagnosis. Call it .&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add a binary column named surv24 for time greater than or less than 24. 
library(MASS)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q3_subset &amp;lt;- leuk %&amp;gt;%
  mutate(surv24 = ifelse(time &amp;gt;= 24, 1,0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b) Fit a logistic regression model to the data with \textit{surv24} as
response. It is advisable to transform the very large white blood counts to
avoid regression coefficients very close to 0 (and odds ratio close to 1).
You may use log transformation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surv24.model &amp;lt;- glm(surv24 ~ log(wbc) + ag, data=q3_subset,family = &amp;#39;binomial&amp;#39;)
kable(summary(surv24.model)$coefficient, caption = &amp;quot;Summary coefficients of the glm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 3: &lt;/span&gt;Summary coefficients of the glm&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;z value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|z|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.4555870&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.9821469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.158758&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2465548&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;log(wbc)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4821891&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3149136&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.531179&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1257252&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;agpresent&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.7621259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8093190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.177295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0294586&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;c) Construct some graphics useful in the interpretation of the final model you fit. &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Create a scatter plot of the data fitting the two curves of test results to the fitted output of the model prediciton&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Create a scatter plot of the data fitting the two curves of test results to the fitted output of the model prediciton&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.extension &amp;lt;- seq(0, max(log(q3_subset$wbc)+4.5), by = 0.5)
espframe &amp;lt;- data.frame(&amp;quot;x.extension&amp;quot; = x.extension, &amp;quot;agpress&amp;quot; = (exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3]))), &amp;quot;agabs&amp;quot; = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension)/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension)))


plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(x = log(leuk$wbc), y = surv24.model$fitted.values, col = leuk$ag, xlim = c(0,15), ylim = c(0,1), ylab = &amp;quot;Survive (Time, surv24wks)&amp;quot;, xlab = &amp;quot;log (wbc counts)&amp;quot;, main = &amp;quot;base R: plot of logistic model of Leuk data&amp;quot;)
lines(x = x.extension, y = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension)/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension)))
lines(x = x.extension, y = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])))
legend(&amp;quot;bottomleft&amp;quot;, legend = c(&amp;quot;Ag Absent&amp;quot;, &amp;quot;Ag Present&amp;quot;), col = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;), lty = c(1,1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leuk.gg &amp;lt;- data.frame(&amp;quot;logwbc&amp;quot; = log(leuk$wbc), surv24 = q3_subset$surv24, &amp;quot;fv&amp;quot; = surv24.model$fitted.values, &amp;quot;ag&amp;quot; = leuk$ag)

leuk.gg &amp;lt;- cbind(leuk.gg, espframe)
ggplot(leuk.gg, aes(x = logwbc, y = fv, colour = ag)) + 
  geom_point() +
  # scale_colour_discrete(guide = FALSE) +
  # guides(colour=FALSE) +
  geom_line(aes(x = x.extension, y = agpress, colour = &amp;quot;present&amp;quot;)) +
  geom_line(aes(x = x.extension, y = agabs, colour = &amp;quot;absent&amp;quot;)) +
  labs ( title = &amp;quot;ggplot: plot of logistic model of Leuk data&amp;quot;, x = &amp;quot;log of WBC count&amp;quot;, y = &amp;quot;Survive (Time, surv24wks)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Survival Vs WBC count with logistic model on actual data points&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Survival Vs WBC count with logistic model on actual data points&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # base plot version
line.1.dat &amp;lt;- leuk.gg[leuk.gg$ag == &amp;#39;absent&amp;#39;, ]
line.2.dat &amp;lt;- leuk.gg[leuk.gg$ag == &amp;#39;present&amp;#39;, ]
plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(
  x = leuk.gg$logwbc,
  y = leuk.gg$surv24,
  xlim=c(0,15),
  ylim = c(0,1),
  col = leuk.gg$ag,
  xlab = &amp;quot;WBC counts&amp;quot;,
  ylab = &amp;quot;Probability of Death prior to 24 Weeks&amp;quot;,
  main = &amp;quot;base R: Survival Vs WBC Counts in Leukaemia Patients&amp;quot;
)
lines(x.extension, leuk.gg$agpress, col = &amp;quot;green&amp;quot;)
lines(x.extension, leuk.gg$agabs, col = &amp;quot;black&amp;quot;)
legend(
  &amp;quot;topleft&amp;quot;,
  title = &amp;quot;AG test&amp;quot;,
  legend = c(&amp;quot;absent&amp;quot;, &amp;quot;present&amp;quot;),
  inset = c(1, 0),
  xpd = TRUE,
  horiz = FALSE,
  col = c(&amp;quot;black&amp;quot;, &amp;quot;green&amp;quot;),
  lty = c(1,1),
  pch = c(1, 2),
  bty = &amp;quot;n&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(leuk.gg, aes(x = logwbc, y = surv24, color = ag)) +
  geom_point() +
  scale_colour_manual(name = &amp;quot;AG test&amp;quot;, values = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;)) +
  geom_line(aes(x = x.extension, y = agpress, colour = &amp;quot;present&amp;quot;)) +
  geom_line(aes(x = x.extension, y = agabs, colour = &amp;quot;absent&amp;quot;)) +
  labs(title = &amp;#39;ggplot: Survival Vs WBC Counts in Leukaemia Patients&amp;#39;,
       x = &amp;#39;log WBC Count&amp;#39;,
       y = &amp;#39;Probability of Death prior to 24 Weeks&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-8-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;d) Fit a model with an interaction term between the two predictors. Which
model fits the data better? Justify your answer.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#fitting the model with the interaction term ag * log(wbc)
surv24.model2 &amp;lt;- lm(surv24 ~ ag * log(wbc), data=q3_subset,family=&amp;#39;binomial&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
##  extra argument &amp;#39;family&amp;#39; will be disregarded&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(summary(surv24.model2)$coefficients, caption = &amp;quot;Summary of the linear model with an interaction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 4: &lt;/span&gt;Summary of the linear model with an interaction&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0258017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8719219&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0295918&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9765953&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;agpresent&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.4360783&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1398202&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.1372479&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0411387&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;log(wbc)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0286636&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0898818&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3189031&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7520857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;agpresent:log(wbc)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2156187&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1183543&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.8218074&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0788139&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 = summary(surv24.model2)
mod = summary(surv24.model)
# we can also calculate adjusted r-square value for glm using 
library(rsq)
mod.rsq.adj = rsq(surv24.model,adj=TRUE,type=c(&amp;#39;v&amp;#39;,&amp;#39;kl&amp;#39;,&amp;#39;sse&amp;#39;,&amp;#39;lr&amp;#39;,&amp;#39;n&amp;#39;),data=NULL)
mod2.rsq.adj = rsq(surv24.model2,adj=TRUE,type=c(&amp;#39;v&amp;#39;,&amp;#39;kl&amp;#39;,&amp;#39;sse&amp;#39;,&amp;#39;lr&amp;#39;,&amp;#39;n&amp;#39;),data=NULL)
# if not using package rsq
# adj.rsq = rbind(mod2$adj.r.squared, (1 -(mod$deviance/mod$null.deviance)) * 32/(32-2-2))
adj.rsq = rbind(mod2.rsq.adj, mod.rsq.adj)

row.names(adj.rsq) &amp;lt;- c(&amp;quot;Linear model with interation&amp;quot;, &amp;quot;Linear model&amp;quot;)
kable(adj.rsq, col.names = &amp;quot;Adjusted R-square values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Adjusted R-square values&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Linear model with interation&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2308546&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Linear model&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1890705&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Since the adjusted R-square value for Linear model with the interacion is higher, I would say the model with an interaction fits the data better.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load the  dataset from  library. The dataset
contains information on ten thousand customers. The aim here is to predict which
customers will default on their credit card debt. It is a four-dimensional
dataset with 10000 observations. The question of interest is to predict
individuals who will default . We want to examine how each predictor variable is
related to the response (default). Do the following on this dataset&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Perform descriptive analysis on the dataset to have an insight. Use
summaries and appropriate exploratory graphics to answer the question of
interest.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use R to build a logistic regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discuss your result. Which predictor variables were important? Are there
interactions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How good is your model? Assess the performance of the logistic regression
classifier. What is the error rate?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up data
data(&amp;quot;Default&amp;quot;, package = &amp;quot;ISLR&amp;quot;)

kable(summary(Default[,1:2]), caption = &amp;quot;Summary of default and student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-10&#34;&gt;Table 5: &lt;/span&gt;Summary of default and student status&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;default&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;student&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;No :9667&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;No :7056&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yes: 333&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yes:2944&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(summary(Default[,3:4]), caption = &amp;quot;Summary of Income and Balance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-10&#34;&gt;Table 5: &lt;/span&gt;Summary of Income and Balance&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;balance&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;income&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Min. : 0.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Min. : 772&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1st Qu.: 481.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1st Qu.:21340&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Median : 823.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Median :34553&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mean : 835.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mean :33517&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3rd Qu.:1166.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3rd Qu.:43808&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Max. :2654.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Max. :73554&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create default binary
default_binary     &amp;lt;-
  ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$default) == -1, 0, 1)
dflt_str &amp;lt;-
  ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$default) == -1,
         &amp;quot;Not Defaulted&amp;quot;,
         &amp;quot;Defaulted&amp;quot;)

stdn     &amp;lt;- ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$student) == -1, 0, 1)
stdn_str &amp;lt;-
  ifelse(regexpr(&amp;#39;Yes&amp;#39;, Default$student) == -1, &amp;quot;Not-Student&amp;quot;, &amp;quot;Student&amp;quot;)

blnc &amp;lt;- Default$balance
incm &amp;lt;- Default$income

df &amp;lt;-  data.frame(default_binary, dflt_str, stdn, stdn_str, blnc, incm)

# par(mfrow = c(1, 1))

cat(&amp;quot;Balance appears roughly normal&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance appears roughly normal&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(blnc, main = &amp;quot;Balance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot() + geom_histogram(aes(blnc), bins = 13, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;)


cat(&amp;quot;Income appears roughly normal with two means&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Income appears roughly normal with two means&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(incm, main = &amp;quot;Income&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Dual means in income appears explained by student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Dual means in income appears explained by student status&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:2, ncol = 2))
hist(
  subset(df$incm, df$stdn == 1),
  main = &amp;quot;Income by Student Status&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Student: Yes&amp;quot;
)
hist(
  subset(df$incm, df$stdn == 0),
  main = &amp;quot;&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Student: No&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;**And** the dual means in income appears NOT to be explained by default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## **And** the dual means in income appears NOT to be explained by default status&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout(matrix(1:2, ncol = 2))
hist(
  subset(df$incm, df$default_binary == 1),
  main = &amp;quot;Income by Default Status&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Default: Yes&amp;quot;
)
hist(
  subset(df$incm, df$default_binary == 0),
  main = &amp;quot;&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Default: No&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Clustering of income v. balance explained by student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Clustering of income v. balance explained by student status&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(
  Default$income ~ Default$balance,
  col = Default$student,
  main = &amp;quot;base R: Income by Balance&amp;quot;,
  ylab = &amp;quot;Income&amp;quot;,
  xlab = &amp;quot;Balance&amp;quot;,
  pch = 18
)
legend(
  &amp;quot;topright&amp;quot;,
  c(&amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;),
  title = &amp;quot;Student?&amp;quot;,
  # bty = &amp;quot;n&amp;quot;,
  fill = c(&amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;),
  pch = c(18,18)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = balance, y = income, colour = student)) + 
  geom_point() +
  labs(title = &amp;quot;ggplot: Income by Balance&amp;quot;) + 
  guides(colour=guide_legend(title=&amp;quot;Student?&amp;quot;)) +
  scale_color_manual(values = c(&amp;quot;No&amp;quot; = &amp;quot;black&amp;quot;, &amp;quot;Yes&amp;quot; = &amp;quot;red&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(balance~student, data = Default, main = &amp;quot;base R: Balance grouped by Student status&amp;quot;, xlab = &amp;quot;student&amp;quot;, ylab = &amp;quot;balance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = student, y = balance)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Balance grouped by Student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(balance~default, data = Default, main = &amp;quot;base R: Balance grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = default, y = balance)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Balance grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(income~student, data = Default, main = &amp;quot;base R: Income grouped by Student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-12.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = student, y = income)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Income grouped by Student status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-13.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
boxplot(income~default, data = Default, main = &amp;quot;base R: Income grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-14.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Default, aes(x = default, y = income)) +
  geom_boxplot() +
  labs(title = &amp;quot;ggplot: Income grouped by Default status&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-10-15.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Median and Max income are lower for defaulted than not defaulted loans&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Median and Max income are lower for defaulted than not defaulted loans&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(df$incm, df$dflt_str, FUN = summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Defaulted
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    9664   19028   31515   32089   43067   66466 
## 
## $`Not Defaulted`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     772   21405   34589   33566   43824   73554&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Median and max balance are higher for defaulted rather than not defaulted loans&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Median and max balance are higher for defaulted rather than not defaulted loans&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(df$blnc, df$dflt_str, FUN = summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Defaulted
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   652.4  1511.6  1789.1  1747.8  1988.9  2654.3 
## 
## $`Not Defaulted`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     0.0   465.7   802.9   803.9  1128.2  2391.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;#B. Use R to build a logistic regression model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #B. Use R to build a logistic regression model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # https://stackoverflow.com/questions/13366755/what-does-the-r-formula-y1-mean
regression_model0 &amp;lt;- glm(default_binary ~ stdn + blnc + incm, family = binomial())
summary(regression_model0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default_binary ~ stdn + blnc + incm, family = binomial())
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4691  -0.1418  -0.0557  -0.0203   3.7383  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.087e+01  4.923e-01 -22.080  &amp;lt; 2e-16 ***
## stdn        -6.468e-01  2.363e-01  -2.738  0.00619 ** 
## blnc         5.737e-03  2.319e-04  24.738  &amp;lt; 2e-16 ***
## incm         3.033e-06  8.203e-06   0.370  0.71152    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.5  on 9996  degrees of freedom
## AIC: 1579.5
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # we could also do to select all predictors in the data
# mod &amp;lt;- glm(default~., data = Default, family = binomial)
# summary(mod)

cat(&amp;quot;Then with interactions:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Then with interactions:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regression_model1 &amp;lt;- glm(default_binary ~ stdn + blnc + incm + stdn * blnc + stdn * incm + blnc * incm, family = binomial())
summary(regression_model1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default_binary ~ stdn + blnc + incm + stdn * blnc + 
##     stdn * incm + blnc * incm, family = binomial())
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4848  -0.1417  -0.0554  -0.0202   3.7579  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.104e+01  1.866e+00  -5.914 3.33e-09 ***
## stdn        -5.201e-01  1.344e+00  -0.387    0.699    
## blnc         5.882e-03  1.180e-03   4.983 6.27e-07 ***
## incm         4.050e-06  4.459e-05   0.091    0.928    
## stdn:blnc   -2.551e-04  7.905e-04  -0.323    0.747    
## stdn:incm    1.447e-05  2.779e-05   0.521    0.602    
## blnc:incm   -1.579e-09  2.815e-08  -0.056    0.955    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.1  on 9993  degrees of freedom
## AIC: 1585.1
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# D. Error Rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # D. Error Rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dflt.fitted0 &amp;lt;- predict(regression_model0, type = &amp;quot;response&amp;quot;)
dflt.fitted1 &amp;lt;- predict(regression_model1, type = &amp;quot;response&amp;quot;)

levs &amp;lt;- c(&amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;)
Tr &amp;lt;- default_binary

Predicted0 &amp;lt;-
  factor(ifelse(dflt.fitted0 &amp;gt;= 0.50, &amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;),
         levels = levs)
Predicted1 &amp;lt;-
  factor(ifelse(dflt.fitted1 &amp;gt;= 0.50, &amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;),
         levels = levs)
Tr1 &amp;lt;-
  factor(ifelse(Tr &amp;gt;= 0.50, &amp;quot;Defaulted&amp;quot;, &amp;quot;Not Defaulted&amp;quot;), levels = levs)
rate0 &amp;lt;- table(Predicted0, True = Tr1)
rate1 &amp;lt;- table(Predicted1, True = Tr1)
rate0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                True
## Predicted0      Defaulted Not Defaulted
##   Defaulted           105            40
##   Not Defaulted       228          9627&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_rate0 &amp;lt;- 1 - (rate0[1, 1] + rate0[2, 2]) / sum(rate0)
error_rate0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0268&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                True
## Predicted1      Defaulted Not Defaulted
##   Defaulted           104            40
##   Not Defaulted       229          9627&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_rate1 &amp;lt;- 1 - (rate1[1, 1] + rate1[2, 2]) / sum(rate1)
error_rate1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0269&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;analysis of variance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## analysis of variance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(regression_model0, regression_model1, test = &amp;#39;Chisq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: default_binary ~ stdn + blnc + incm
## Model 2: default_binary ~ stdn + blnc + incm + stdn * blnc + stdn * incm + 
##     blnc * incm
##   Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)
## 1      9996     1571.5                     
## 2      9993     1571.1  3  0.47911   0.9235&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;4a. Based on the outputs for 4a. we can tell that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Fewer people default than don’t default.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Defaulters and non-defaulters appear to have the same income range, given student status.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Defaulters appear to have higher balances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If students default, they likely do it with over $1,000 balance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If non-students default, they are likely do it with over $500 balance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4c.
Without taking interactions into account, it appears that two predictors-
student and balance are significant. With interactions involved, it appears that
only balance predictor is important.&lt;/p&gt;
&lt;p&gt;4d.
The model without interactions has an AICof 1579.5 and the interaction model has
an AIC of 1585.1 (slightly higher). But, both have almost similar error rates
~2.7 %. Also, since analysis of deviance also shows that the chi-square test has
no significance at 5% level, we can conclude that both models are almost the
same as a working model.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go through Section 7.3.1 of the Handbook. Run all the codes (additional exploration of data is allowed) and write your own version of explanation and interpretation.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# density plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # density plot&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plasma &amp;lt;- plasma

layout(matrix(1:2,ncol=2))
cdplot(ESR ~ fibrinogen, data=plasma)
cdplot(ESR ~ globulin,data=plasma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that above a certain level of fibrogen, ESR drops sucessively. This is not the case for globulin.&lt;/p&gt;
&lt;p&gt;ESR Logistic Regression an Confidence Interval Estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plasma_glm_1 &amp;lt;- glm(ESR ~ fibrinogen, data = plasma, family=binomial())
confint(plasma_glm_1,parm=&amp;#39;fibrinogen&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5 %    97.5 % 
## 0.3387619 3.9984921&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, fibrinogen might have value as a predictor of ESR. We can look at the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(plasma_glm_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = ESR ~ fibrinogen, family = binomial(), data = plasma)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9298  -0.5399  -0.4382  -0.3356   2.4794  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)  -6.8451     2.7703  -2.471   0.0135 *
## fibrinogen    1.8271     0.9009   2.028   0.0425 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 30.885  on 31  degrees of freedom
## Residual deviance: 24.840  on 30  degrees of freedom
## AIC: 28.84
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary output indicates a 5% significance of fibrinogenand and increase of the log-odds of ESR &amp;gt; 20 by about 1.83 with confidence interval (CI) of 0.33 to 3.99.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(coef(plasma_glm_1)[&amp;#39;fibrinogen&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## fibrinogen 
##   6.215715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fibrinogen might have value as a predictor of ESR.
To make the results more readable, it is useful to apply an exponent function. This exponenetiates the log-odds of fibriogen and CI to correspond with the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(confint(plasma_glm_1, parm=&amp;#39;fibrinogen&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Waiting for profiling to be done...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5 %    97.5 % 
##  1.403209 54.515884&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also perform logistic regression of both explanatory variables (fibrinogen and globulin) and text for the deviance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plasma_glm_2 &amp;lt;- glm(ESR ~ fibrinogen + globulin, data = plasma, family = binomial())
summary(plasma_glm_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = ESR ~ fibrinogen + globulin, family = binomial(), 
##     data = plasma)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9683  -0.6122  -0.3458  -0.2116   2.2636  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept) -12.7921     5.7963  -2.207   0.0273 *
## fibrinogen    1.9104     0.9710   1.967   0.0491 *
## globulin      0.1558     0.1195   1.303   0.1925  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 30.885  on 31  degrees of freedom
## Residual deviance: 22.971  on 29  degrees of freedom
## AIC: 28.971
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;# comparison of models&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # comparison of models&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(plasma_glm_1, plasma_glm_2, test= &amp;#39;Chisq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: ESR ~ fibrinogen
## Model 2: ESR ~ fibrinogen + globulin
##   Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)
## 1        30     24.840                     
## 2        29     22.971  1   1.8692   0.1716&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can make the bubble plot of the predicted values of model II (plasma_glm_2). The plot shows that the probablity of ‘good’ ESR reading increases as fibrinogen increases. This is true of globulin only up to a point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob &amp;lt;- predict(plasma_glm_2, type=&amp;#39;response&amp;#39;)

plot.new()
par(mfrow = c(1, 1), pty = &amp;quot;s&amp;quot;)
plot(globulin ~ fibrinogen,data=plasma,xlim=c(2,6),ylim=c(25,55),pch=&amp;#39;.&amp;#39;, main = &amp;quot;Bubble plot of the predicted values of model II&amp;quot;)
symbols(plasma$fibrinogen,plasma$globulin,circles=prob,add=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Logistic_Regression_GLM_I_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graphical Summary</title>
      <link>/achalneupane.github.io/post/graphical_summary/</link>
      <pubDate>Tue, 10 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/graphical_summary/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content titled Format+STAT-701+HW. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.1, pg. 23 in Handbook &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, for this question, we assume that we have data available for the given countries and we will remove any NAs from the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;HSAUR3&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#subset the data for wanted contries only first
median.subset &amp;lt;- subset(Forbes2000,country %in% c(&amp;quot;United States&amp;quot;,&amp;quot;United Kingdom&amp;quot;,&amp;quot;France&amp;quot;,&amp;quot;Germany&amp;quot;))
# library(&amp;quot;plyr&amp;quot;)
# ddply(median.subset,c(&amp;quot;country&amp;quot;),function(x){median(x$profits,na.rm=T)})
by.country &amp;lt;- setNames(aggregate(median.subset$profits, by = list(median.subset$country), function(x){median(x,na.rm=T)}), c(&amp;quot;country&amp;quot;, &amp;quot;median&amp;quot;))
by.country&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          country median
## 1         France  0.190
## 2        Germany  0.230
## 3 United Kingdom  0.205
## 4  United States  0.240&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.2, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Forbes2000[Forbes2000$country == &amp;quot;Germany&amp;quot; &amp;amp; Forbes2000$profits &amp;lt; 0,&amp;quot;name&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Allianz Worldwide&amp;quot;       &amp;quot;Deutsche Telekom&amp;quot;       
##  [3] &amp;quot;E.ON&amp;quot;                    &amp;quot;HVB-HypoVereinsbank&amp;quot;    
##  [5] &amp;quot;Commerzbank&amp;quot;             &amp;quot;Infineon Technologies&amp;quot;  
##  [7] &amp;quot;BHW Holding&amp;quot;             &amp;quot;Bankgesellschaft Berlin&amp;quot;
##  [9] &amp;quot;W&amp;amp;W-Wustenrot&amp;quot;           &amp;quot;mg technologies&amp;quot;        
## [11] &amp;quot;Nurnberger Beteiligungs&amp;quot; &amp;quot;SPAR Handels&amp;quot;           
## [13] &amp;quot;Mobilcom&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.3, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(table(Forbes2000[Forbes2000$country==&amp;quot;Bermuda&amp;quot;,&amp;quot;category&amp;quot;]),decreasing=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##                        Insurance                    Conglomerates 
##                               10                                2 
##             Oil &amp;amp; gas operations                          Banking 
##                                2                                1 
##                    Capital goods             Food drink &amp;amp; tobacco 
##                                1                                1 
##                     Food markets                            Media 
##                                1                                1 
##              Software &amp;amp; services              Aerospace &amp;amp; defense 
##                                1                                0 
##     Business services &amp;amp; supplies                        Chemicals 
##                                0                                0 
##                     Construction                Consumer durables 
##                                0                                0 
##           Diversified financials            Drugs &amp;amp; biotechnology 
##                                0                                0 
## Health care equipment &amp;amp; services     Hotels restaurants &amp;amp; leisure 
##                                0                                0 
##    Household &amp;amp; personal products                        Materials 
##                                0                                0 
##                        Retailing                   Semiconductors 
##                                0                                0 
##  Technology hardware &amp;amp; equipment      Telecommunications services 
##                                0                                0 
##                Trading companies                   Transportation 
##                                0                                0 
##                        Utilities 
##                                0&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.4, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Forbes2000.data &amp;lt;- Forbes2000[order(Forbes2000$profits,decreasing=T),]
par(mfrow = c(1,1))
# Using base plot
plot(
    log(Forbes2000.data[1:50, &amp;quot;assets&amp;quot;]),
    log(Forbes2000.data[1:50, &amp;quot;sales&amp;quot;])
    ,
    ylab = &amp;quot;sales (log scale)&amp;quot;,
    xlab = &amp;quot;assets (log scale)&amp;quot;,
    main = &amp;quot;Sales vs Assets: Log transformed&amp;quot;
)
# we can add texts to data points
text(
    x = log(Forbes2000.data[1:50, &amp;quot;assets&amp;quot;]),
    y = log(Forbes2000.data[1:50, &amp;quot;sales&amp;quot;]),
    labels = abbreviate(Forbes2000.data$country[1:50]),
    col = &amp;quot;black&amp;quot;,
    font = 1,
    pos = 1
)

#Using ggplot
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Forbes2000.data[1:50, ], aes(x = log(assets), y = log(sales))) +
    geom_point(shape = 1, size = 2) +
    geom_text(aes(label = abbreviate(country)), hjust = 0, vjust = 0) +
    labs(title = &amp;quot;Sales vs Assets: Log transformed&amp;quot;, color = &amp;quot;country&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 1.5, pg. 23 in Handbook&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
# selecting companies with profit more than 5 billion
Forbes2000.5billions.dat &amp;lt;- Forbes2000[Forbes2000$profits &amp;gt; 5 ,]
Forbes2000.5billions.dat &amp;lt;- Forbes2000.5billions.dat[complete.cases(Forbes2000.5billions.dat), ]

setNames(ddply(Forbes2000.5billions.dat,c(&amp;quot;country&amp;quot;),function(x){c(nrow(x),mean(x$sales))}), c(&amp;quot;Country&amp;quot;, &amp;quot;Company_counts&amp;quot;, &amp;quot;Avg_Sales&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       Country Company_counts Avg_Sales
## 1                       China              1   29.5300
## 2                      France              1  131.6400
## 3                     Germany              1  157.1300
## 4                       Japan              1  135.8200
## 5 Netherlands/ United Kingdom              1  133.5000
## 6                 South Korea              1   50.2200
## 7                 Switzerland              3   46.7600
## 8              United Kingdom              3  103.6867
## 9               United States             20   77.2835&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# #or we could also do:
# aggregate(Forbes2000.5billions.dat$sales, by = list(Forbes2000.5billions.dat$country), function(x){c(Company_counts = length(x), Avg_Sales = mean(x))})&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 2.1, pg. 39 in Handbook (see Chapter 6 of R Graphcis Cookbook for GGPlot)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this, we will first calculate total household expenses as below. Then calculate the itemized expenses (proportion).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;household&amp;quot;, package = &amp;quot;HSAUR3&amp;quot;)

# total expenses
household$total&amp;lt;-household$housing+household$food+household$goods+household$service
library(ggplot2)

ggplot(household, aes(x=gender, y=total))+geom_boxplot() +
  labs(title = &amp;quot;Total expenses per gender&amp;quot;) + 
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# base R version
boxplot(household$total ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;total&amp;quot;, main = &amp;quot;Total household expenses by gender&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This indicates that males tend to have more expenses than females in total. 
#calculate proportions
household$housing&amp;lt;-household$housing/household$total
household$service&amp;lt;-household$service/household$total
household$goods&amp;lt;-household$goods/household$total
household$food&amp;lt;-household$food/household$total

#plot boxplots of proportions
a&amp;lt;-ggplot(household, aes(x=gender, y=housing))+geom_boxplot() + labs(title = &amp;quot;Housing expenses (in %) per gender&amp;quot;) 
b&amp;lt;-ggplot(household, aes(x=gender, y=service))+geom_boxplot() + labs(title = &amp;quot;Service expenses (in %) per gender&amp;quot;)
c&amp;lt;-ggplot(household, aes(x=gender, y=goods))+geom_boxplot() + labs(title = &amp;quot;Goods expenses (in %) per gender&amp;quot;)
d&amp;lt;-ggplot(household, aes(x=gender, y=food))+geom_boxplot() + labs(title = &amp;quot;Food expenses (in %) per gender&amp;quot;)

# combine all 4 gg objects
library(gridExtra)
grid.arrange(a,b,c,d, nrow=2, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# base R version
par(mfrow=c(2,2))
boxplot(household$housing ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;Housing expenses&amp;quot;, main = &amp;quot;Housing expenses (in %) by gender&amp;quot;)
boxplot(household$service ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;Service expenses&amp;quot;, main = &amp;quot;Service expenses (in %) by gender&amp;quot;)
boxplot(household$goods ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;Goods expenses&amp;quot;, main = &amp;quot;Goods expenses (in %) by gender&amp;quot;)
boxplot(household$food ~ household$gender,
        xlab = &amp;quot;Gender&amp;quot;, ylab = &amp;quot;food expenses&amp;quot;, main = &amp;quot;Food expenses (in %) by gender&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# histogram of total expenditures by gender
ggplot(household, aes(x=total, fill=gender)) +
  geom_histogram(position=&amp;quot;identity&amp;quot;, alpha=0.4, binwidth = 1000) + 
  labs(title = &amp;quot;Histogram of total expenditures by gender&amp;quot; ) + 
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-6-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on these results, males tend to spend more money than females on Foods
and Services, where as females take the lead on Housing and Goods.&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Question 2.3, pg. 41 in Handbook (see Chapter 6 of R Graphcis Cookbook for GGPlot)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;suicides2&amp;quot;)
#Boxplot of mortality
boxplot(suicides2,
        xlab = &amp;quot;Age group&amp;quot;, ylab = &amp;quot;Mortality rates per 100,000&amp;quot;, main = &amp;quot;Mortality by suicide&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
suicides2.melted &amp;lt;- melt(suicides2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No id variables; using all as measure variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot version
ggplot(suicides2.melted, aes(x=factor(variable), y=value)) + 
  geom_boxplot() +
  labs(x = &amp;quot;Age group&amp;quot;, y = &amp;quot;Mortality rates per 100,000&amp;quot;, title = &amp;quot;Mortality by suicide&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Graphical_summary_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using a single R expression, calculate the median absolute deviation, &lt;span class=&#34;math inline&#34;&gt;\(1.4826\cdot median|x-\mu|\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the sample median. Use the dataset . Use the R function mad() to verify your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(abs(chickwts$weight - median(chickwts$weight))) * 1.4826&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.9212&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#check using mad
mad(chickwts$weight, median (chickwts$weight), constant = 1.4826)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.9212&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using the data matrix , find the state with the minimum per capita income in the New England region as defined by the factor . Use the vector  to get the state name.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(state.x77)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in data(state.x77): data set &amp;#39;state.x77&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create df with state name, income, and division
state_df &amp;lt;- data.frame(income=state.x77[,&amp;quot;Income&amp;quot;],
                    name=state.name,
                    div=state.division)
#Subset for New England
New_Eng &amp;lt;- subset(state_df, div ==&amp;quot;New England&amp;quot;)

#State with min income per capita
New_Eng[which(New_Eng$income == min(New_Eng$income)), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       income  name         div
## Maine   3694 Maine New England&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;11&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use subscripting operations on the dataset  to find the vehicles with highway mileage of less than 25 miles per gallon (variable ) and weight (variable ) over 3500lbs. Print the model name, the price range (low, high), highway mileage, and the weight of the cars that satisfy these conditions.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Cars93, package= &amp;quot;MASS&amp;quot;)
df.cars93 &amp;lt;- Cars93[Cars93$MPG.highway &amp;lt; 25 &amp;amp; Cars93$Weight &amp;gt; 3500, c(&amp;quot;Model&amp;quot;, &amp;quot;Price&amp;quot;, &amp;quot;MPG.highway&amp;quot;, &amp;quot;Weight&amp;quot;)]
df.cars93[with(df.cars93, order(Price)), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Model Price MPG.highway Weight
## 16 Lumina_APV  16.3          23   3715
## 17      Astro  16.6          20   4025
## 26    Caravan  19.0          21   3705
## 56        MPV  19.1          24   3735
## 66      Quest  19.1          23   4100
## 70 Silhouette  19.5          23   3715
## 89    Eurovan  19.7          21   3960
## 36   Aerostar  19.9          20   3735
## 87     Previa  22.7          22   3785
## 28    Stealth  25.8          24   3805
## 63   Diamante  26.1          24   3730
## 49      ES300  28.0          24   3510
## 50      SC300  35.2          23   3515
## 48        Q45  47.9          22   4000&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;12&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Form a matrix object named  from the variables  from the  dataframe from the  package. Use it to create a list object named  containing named components as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;A vector of means, named &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A vector of standard errors of the means, named &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Cars93, package= &amp;quot;MASS&amp;quot;)
mycars &amp;lt;- Cars93[,c(&amp;quot;Min.Price&amp;quot;, &amp;quot;Max.Price&amp;quot;, &amp;quot;MPG.city&amp;quot;, &amp;quot;MPG.highway&amp;quot;, &amp;quot;EngineSize&amp;quot;, &amp;quot;Length&amp;quot;, &amp;quot;Weight&amp;quot;)]
Cars.Means &amp;lt;- sapply(mycars, mean)       
Cars.Means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Min.Price   Max.Price    MPG.city MPG.highway  EngineSize      Length 
##   17.125806   21.898925   22.365591   29.086022    2.667742  183.204301 
##      Weight 
## 3072.903226&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to calculate standard error &amp;#39;standard.err&amp;#39;
standard.err &amp;lt;- function(x) sqrt(var(x)/length(x))
Cars.Std.Errors &amp;lt;- lapply(mycars, standard.err)  
Cars.Std.Errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Min.Price
## [1] 0.906921
## 
## $Max.Price
## [1] 1.143805
## 
## $MPG.city
## [1] 0.5827473
## 
## $MPG.highway
## [1] 0.5528742
## 
## $EngineSize
## [1] 0.1075695
## 
## $Length
## [1] 1.514196
## 
## $Weight
## [1] 61.16942&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the  function on the three-dimensional array  to compute:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Sample means of the variables , for each of the three species &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(iris3, c(2,3), mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Setosa Versicolor Virginica
## Sepal L.  5.006      5.936     6.588
## Sepal W.  3.428      2.770     2.974
## Petal L.  1.462      4.260     5.552
## Petal W.  0.246      1.326     2.026&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b) Sample means of the variables \textit{Sepal Length, Sepal Width, Petal Width} for the entire data set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(iris3, 2, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sepal L. Sepal W. Petal L. Petal W. 
## 5.843333 3.057333 3.758000 1.199333&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;14&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the data matrix  and the  function to obtain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The mean per capita income of the states in each of the four regions defined by the factor &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mydf &amp;lt;- data.frame(state.x77, state.region = state.region, stringsAsFactors = FALSE)
tapply(mydf$Income, mydf$state.region, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Northeast         South North Central          West 
##      4570.222      4011.938      4611.083      4702.615&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b) The maximum illiteracy rates for states in each of the nine divisions defined by the factor \textit{state.division}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mydf &amp;lt;- data.frame(state.x77, state.division = state.division, stringsAsFactors = FALSE)
tapply(mydf$Illiteracy, mydf$state.division, max)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        New England    Middle Atlantic     South Atlantic East South Central 
##                1.3                1.4                2.3                2.4 
## West South Central East North Central West North Central           Mountain 
##                2.8                0.9                0.8                2.2 
##            Pacific 
##                1.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;c) The number of states in each region&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mydf &amp;lt;- data.frame(state.x77, state.region = state.region, state.name = state.name, stringsAsFactors = FALSE)
tapply(mydf$state.name, mydf$state.region, length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Northeast         South North Central          West 
##             9            16            12            13&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;15&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Using the dataframe , produce a scatter plot matrix of the variables . Use different colors to identify cars belonging to each of the categories defined by the  variable in different colors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carsize = cut(mtcars[,&amp;quot;wt&amp;quot;], breaks=c(0, 2.5, 3.5, 5.5), 
labels = c(&amp;quot;Compact&amp;quot;,&amp;quot;Midsize&amp;quot;,&amp;quot;Large&amp;quot;))
carsize = cut(mtcars[,&amp;quot;wt&amp;quot;], breaks=c(0, 2.5, 3.5,
                                  5.5), labels = c(&amp;quot;Compact&amp;quot;,&amp;quot;Midsize&amp;quot;,&amp;quot;Large&amp;quot;))

mydf &amp;lt;- data.frame(mtcars, carsize = carsize)

# Using base R
par(mfrow = c(1,1))
pairs(~mpg + disp + hp + drat + qsec, data=mydf, 
  main=&amp;quot;mtcars Scatterplot Matrix&amp;quot;)

# Using ggplot
library(ggplot2)
library(&amp;#39;GGally&amp;#39;)
ggpairs(mydf[,c(&amp;quot;mpg&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;qsec&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the function  to perform a one-way analysis of variance on the  data with  as the treatment factor. Assign the result to an object named  and use it to print an ANOVA table.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chick.aov &amp;lt;- aov( weight ~ feed, chickwts)

# summary aov
summary.aov(chick.aov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## feed         5 231129   46226   15.37 5.94e-10 ***
## Residuals   65 195556    3009                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# post-hoc test (Tukey HSD)
TukeyHSD(chick.aov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = weight ~ feed, data = chickwts)
## 
## $feed
##                            diff         lwr       upr     p adj
## horsebean-casein    -163.383333 -232.346876 -94.41979 0.0000000
## linseed-casein      -104.833333 -170.587491 -39.07918 0.0002100
## meatmeal-casein      -46.674242 -113.906207  20.55772 0.3324584
## soybean-casein       -77.154762 -140.517054 -13.79247 0.0083653
## sunflower-casein       5.333333  -60.420825  71.08749 0.9998902
## linseed-horsebean     58.550000  -10.413543 127.51354 0.1413329
## meatmeal-horsebean   116.709091   46.335105 187.08308 0.0001062
## soybean-horsebean     86.228571   19.541684 152.91546 0.0042167
## sunflower-horsebean  168.716667   99.753124 237.68021 0.0000000
## meatmeal-linseed      58.159091   -9.072873 125.39106 0.1276965
## soybean-linseed       27.678571  -35.683721  91.04086 0.7932853
## sunflower-linseed    110.166667   44.412509 175.92082 0.0000884
## soybean-meatmeal     -30.480519  -95.375109  34.41407 0.7391356
## sunflower-meatmeal    52.007576  -15.224388 119.23954 0.2206962
## sunflower-soybean     82.488095   19.125803 145.85039 0.0038845&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;17&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write an R function named  for conducting a one-sample t-test. Return a list object containing the two components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the t-statistic named T;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the two-sided p-value named P.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Annotated R Code
  library(HSAUR3)
  library(stats)
  attach(chickwts)
  head(chickwts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   weight      feed
## 1    179 horsebean
## 2    160 horsebean
## 3    136 horsebean
## 4    227 horsebean
## 5    217 horsebean
## 6    168 horsebean&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  t.test(chickwts$weight,mu=240)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  chickwts$weight
## t = 2.2999, df = 70, p-value = 0.02444
## alternative hypothesis: true mean is not equal to 240
## 95 percent confidence interval:
##  242.8301 279.7896
## sample estimates:
## mean of x 
##  261.3099&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  ttest=function(x,mu,alpha){
    # x = data
    # mu = sample mean, 
    # alpha = alpha 
    # level = (1-confidence level)
    me=mean(x$weight)
    p1=qt(alpha/2,(nrow(x)-1))
    p2=qt(1-alpha/2,(nrow(x)-1))
    s=sqrt(var(x$weight))
    n=nrow(x)
    
    T=(me-mu)/(s/sqrt(nrow(x))) 
    P=seq(1,1,1)
    P[1]=2*(1-pt(T,n))
    P=data.frame(P)
    return (cbind(P,T))
  }
  
  t_test_values &amp;lt;- ttest(chickwts,240,0.05)
  print (&amp;quot;T value and two sided P values returned by the funtion: &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;T value and two sided P values returned by the funtion: &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  print(t_test_values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            P        T
## 1 0.02439824 2.299879&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use this function to test the hypothesis that the mean of the  variable (in the  dataset) is equal to 240 against the two-sided alternative. &lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  message(&amp;quot;Hypothesis Result:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hypothesis Result:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  if (2*abs(t_test_values[1,2])&amp;gt;2*abs(t_test_values[1,1])){
    print(&amp;quot;Rejected! The true mean is NOT 240 !!&amp;quot;)
  } else if (2*abs(t_test_values[1,2])&amp;lt;2*abs(t_test_values[1,1])){
    print(&amp;quot;Null. The mean is 240 !!&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Rejected! The true mean is NOT 240 !!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Density Estimation</title>
      <link>/achalneupane.github.io/post/density_estimation/</link>
      <pubDate>Sun, 01 Sep 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/density_estimation/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.&lt;/p&gt;
&lt;p&gt;Submit your  file with the knitted  (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.&lt;/p&gt;
&lt;p&gt;This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.&lt;/p&gt;
&lt;p&gt;For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn’t apply to questions that don’t specifically ask for a plot, however I still would encourage you to produce both.&lt;/p&gt;
&lt;p&gt;You do not need to include the above statements.&lt;/p&gt;
&lt;p&gt;Please do the following problems from the text book R Handbook and stated.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The  data from  contains the velocities of 82 galaxies from six well-separated conic sections of space (Postman et al., 1986, Roeder, 1990). The data are intended to shed light on whether or not the observable universe contains superclusters of galaxies surrounded by large voids. The evidence for the existence of superclusters would be the multimodality of the distribution of velocities.(8.1 Handbook)&lt;/p&gt;
&lt;p&gt;a.) Construct histograms using the following functions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; -hist() and ggplot()+geom_histogram()

 -truehist() and ggplot+geom_histogram() (pay attention to the y-axis!)

 -qplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comment on the shape and distribution of the variable based on the three plots. (Hint: Also play around with binning)&lt;/p&gt;
&lt;p&gt;b.) Create a new variable  = &lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt;(galaxies). Construct histograms using the functions in part a.) and comment on the shape and differences.&lt;/p&gt;
&lt;p&gt;c.) Construct kernel density estimates using two different choices of kernel functions and three choices of bandwidth (one that is too large and “oversmooths,” one that is too small and “undersmooths,” and one that appears appropriate.) Therefore you should have six different kernel density estimates plots. Discuss your results. You can use the log scale or original scale for the variable.&lt;/p&gt;
&lt;p&gt;d.) What is your conclusion about the possible existence of superclusterd of galaxies? How many superclusters (1,2, 3, … )?&lt;/p&gt;
&lt;p&gt;e.) How many clusters did it find? Did it match with your answer from (d) above? Report parameter estimates and BIC of the best model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
library(ggplot2)
# Load the data
data(galaxies)
# #set the vector of numeric velocities to dataframe
# galaxies &amp;lt;- as.data.frame(galaxies)
# #rename the single column in the galaxies dataframe to Velocity
# names(galaxies) &amp;lt;- &amp;#39;Velocity&amp;#39;
# #replace the typo with the correct numeric value
# # galaxies[78,1] &amp;lt;- 26960
# #add the log velocity column
# galaxies &amp;lt;- galaxies %&amp;gt;%
#   mutate(loggalaxies = log(Velocity))

message(&amp;quot;# 1a.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1a.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# histogram
message(&amp;quot;Using hist and geom_histogram:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using hist and geom_histogram:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set the figure position
# par(fig=c(0,1,0,1),new=T)
#draw the histogram
hist(galaxies,
     xlab = &amp;#39;velocity&amp;#39;,
     main = &amp;#39;base R: Histogram showing galaxies&amp;#39;,
     ylab = &amp;#39;Frequency&amp;#39;, freq = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(galaxies) +
  geom_histogram(binwidth = 5000, breaks = c(seq(5000, 35000, 5000)), boundary = NULL, fill = &amp;#39;white&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(title = &amp;#39;ggplot: Histogram showing galaxies&amp;#39;, x= &amp;quot;velocity&amp;quot;, y = &amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# truehist
message(&amp;quot;Using truehist and geom_histogram():&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using truehist and geom_histogram():&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# par(fig=c(0,1,0,1),new=T)
#draw the histogram
truehist(galaxies,
         xlab = &amp;#39;velocity&amp;#39;,
         main = &amp;#39;base R: True Histogram showing galaxies&amp;#39;,
         ylab = &amp;#39;density&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######### *****
# ggplot() + aes(galaxies) +
#   geom_histogram(binwidth = 5000, bins = 10, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;) +
#   labs(x = &amp;quot;velocity&amp;quot;, title = &amp;#39;ggplot: True Histogram showing galaxies&amp;#39;) 

BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
# Frequency
# ggplot() + aes(galaxies) + geom_histogram(binwidth = BINWIDTH, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) 
# Density
ggplot() + aes(galaxies) + geom_histogram(aes(y = ..density..), binwidth = BINWIDTH, breaks = c(seq(5000, 35000, 5000)), boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) + 
labs(title = &amp;quot;ggplot: True Histogram showing galaxies&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######




# qplot
message(&amp;quot;Using qplot:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using qplot:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(galaxies) +
  labs(title=&amp;#39;base R: Histogram showing galaxies (qplot)&amp;#39;,
       x=&amp;#39;Velocity&amp;#39;,
       y=&amp;#39;Frequency&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 30
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() + aes(galaxies) +
  geom_histogram(bins = BINS, breaks = BREAKS, binwidth = BINWIDTH, boundary = NULL, fill = &amp;#39;grey&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;velocity&amp;quot;, y= &amp;quot;Frequency&amp;quot;, title = &amp;#39;ggplot: Histogram showing galaxies (qplot)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;# 1b.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1b.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loggalaxies &amp;lt;- log(galaxies)
# histogram
message(&amp;quot;Using hist and geom_histogram:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using hist and geom_histogram:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set the figure position
# par(fig=c(0,1,0,1),new=T)
#draw the histogram
hist(loggalaxies,
     xlab = &amp;#39;log velocity&amp;#39;,
     main = &amp;#39;base R: Histogram showing log velocity of galaxies&amp;#39;,
     ylab = &amp;#39;Frequency&amp;#39;, freq = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 7
BREAKS &amp;lt;- seq(9, 10.5, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() + aes(loggalaxies) +
  geom_histogram(binwidth = .15, bins = 8, boundary = NULL, fill = &amp;#39;white&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;log velocity&amp;quot;, y = &amp;quot;Frequency&amp;quot;, title = &amp;#39;ggplot: Histogram showing log velocity of galaxies&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# truehist
message(&amp;quot;Using truehist and geom_histogram():&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using truehist and geom_histogram():&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# par(fig=c(0,1,0,1),new=T)
#draw the histogram
truehist(loggalaxies,
         xlab = &amp;#39;log velocity of galaxies&amp;#39;,
         main = &amp;#39;base R: True Histogram of log velocity of galaxies&amp;#39;,
         ylab = &amp;#39;density&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######### *****
# ggplot() + aes(loggalaxies) +
#   geom_histogram(binwidth = 0.1, bins = 0.1, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;) +
#   labs(x = &amp;quot;log velocity&amp;quot;, title = &amp;#39;ggplot: True Histogram showing log velocity of galaxies&amp;#39;) 

BINS &amp;lt;- 7
BREAKS &amp;lt;- seq(9, 10.5, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
# ggplot() + aes(loggalaxies) + geom_histogram(binwidth = BINWIDTH, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) # Frequency
ggplot() + aes(loggalaxies) + geom_histogram(aes(y = ..density..), binwidth = BINWIDTH, bins = BINS, breaks = BREAKS, boundary = NULL, fill = &amp;#39;skyblue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
labs(title = &amp;quot;ggplot: True Histogram of log velocity of galaxies&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######


# qplot
message(&amp;quot;Using qplot:&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using qplot:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(loggalaxies) +
  labs(title=&amp;#39;base R: Histogram of log velocity of galaxies (qplot)&amp;#39;,
       x=&amp;#39;log velocity of Galaxies&amp;#39;,
       y=&amp;#39;Frequency&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + aes(loggalaxies) +
  geom_histogram(bins = 30, boundary = NULL, fill = &amp;#39;grey&amp;#39;, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;log velocity&amp;quot;, title = &amp;#39;ggplot: Histogram showing log velocity of Galaxies (qplot)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-12.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message (&amp;quot;# 1c.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1c.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;blue&amp;quot;, main=&amp;quot;base R: Gaussian Over Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;gaussian&amp;quot;,bw=5000),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-13.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;blue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;gaussian&amp;quot;, bw = 5000, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Gaussian Over Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-14.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;blue&amp;quot;, main=&amp;quot;base R: Gaussian Under Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;gaussian&amp;quot;,bw=500),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-15.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
# BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
# BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;blue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;gaussian&amp;quot;, bw = 500, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Gaussian Under Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-16.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;blue&amp;quot;, main=&amp;quot;base R: Gaussian Best Appx&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;gaussian&amp;quot;,bw=1100),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-17.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;blue&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;gaussian&amp;quot;, bw = 1100, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Gaussian Best Appx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-18.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;green&amp;quot;, ylab = &amp;quot;density&amp;quot;, main=&amp;quot;base R: Triangular Over Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;triangular&amp;quot;,bw=5000),col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-19.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;green&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;triangular&amp;quot;, bw = 5000, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Triangular Over Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-20.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;green&amp;quot;, ylab= &amp;quot;density&amp;quot;, main=&amp;quot;base R: Triangular Under Smooth&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;triangular&amp;quot;,bw=500),col=&amp;quot;red&amp;quot;,main=&amp;quot;Triangular_Under&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-21.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;green&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;triangular&amp;quot;, bw = 500, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Triangular Under Smooth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-22.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truehist(galaxies,ymax=0.0002,col=&amp;quot;green&amp;quot;, ylab = &amp;quot;density&amp;quot;, main=&amp;quot;base R: Triangular Best Appx&amp;quot;)
lines(density(galaxies,kernel=&amp;quot;triangular&amp;quot;,bw=1100), col=&amp;quot;red&amp;quot;,main=&amp;quot;Triangular_Appx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-23.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BINS &amp;lt;- 6
BREAKS &amp;lt;- seq(5000, 35000, length.out = BINS + 1)
BINWIDTH &amp;lt;- BREAKS[2] - BREAKS[1]
ggplot() +
  aes(galaxies) +
  geom_histogram(aes(y=..density..), bins = BINS, binwidth = BINWIDTH, breaks = BREAKS, boundary = NULL, fill = &amp;#39;green&amp;#39;, color = &amp;quot;black&amp;quot;, closed = &amp;quot;left&amp;quot;) +
  stat_density(kernel = &amp;quot;triangular&amp;quot;, bw = 1100, fill = NA, col = &amp;quot;red&amp;quot;) +
  labs(title = &amp;quot;ggplot: Triangular Best Appx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-24.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # hist(galaxies, xlab = &amp;quot;Waiting times (in min.)&amp;quot;, ylab = &amp;quot;Frequency&amp;quot;,
# # probability = TRUE, main = &amp;quot;Rectangular kernel&amp;quot;, border = &amp;quot;gray&amp;quot;)
# # lines(density(galaxies, bw = 5000, window = &amp;quot;gaussian&amp;quot;))
# # rug(galaxies)
# 
# #construct a stat density plot with ggplot2, adjust = 1 for less smoothing
# p1_g &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=1,aes(x=galaxies)) +
#   labs(title = &amp;#39;Gaussian Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a stat density plot with ggplot2, adjust = 2 for moderate smoothing
# p2_g &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=2,aes(x=galaxies)) +
#   labs(title = &amp;#39;Gaussian Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a stat density plot with ggplot2, adjust = 3 for over-smoothing
# p3_g &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=3,aes(x=galaxies$Velocity)) +
#   labs(title = &amp;#39;Gaussian Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a triangular stat density plot, adjust for less smoothing
# p1_t &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;triangular&amp;#39;,adjust=1,aes(x=galaxies$Velocity)) +
#   labs(title = &amp;#39;Triangular Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a triangular stat density plot, adjust for moderate smoothing
# p2_t &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;triangular&amp;#39;,adjust=2,aes(x=galaxies$Velocity)) +
#   labs(title = &amp;#39;Triangular Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)
# #construct a triangular stat density plot, adjust for over-smoothing
# p3_t &amp;lt;- ggplot() +
#   stat_density(kernel=&amp;#39;triangular&amp;#39;,adjust=3,aes(x=galaxies)) +
#   labs(title = &amp;#39;Triangular Kernal Density of Galaxy Velocity&amp;#39;,
#        x = &amp;#39;Galaxy Velocity&amp;#39;,
#        y=&amp;#39;Density Estimate&amp;#39;)


message(&amp;quot;# 1e.&amp;quot;,&amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 1e.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can use:
library(mclust)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package &amp;#39;mclust&amp;#39; version 5.4.5
## Type &amp;#39;citation(&amp;quot;mclust&amp;quot;)&amp;#39; for citing this R package in publications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod=Mclust(galaxies)
print(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (V,4) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(mod, parameters = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust V (univariate, unequal variance) model with 4 components: 
## 
##  log-likelihood  n df       BIC       ICL
##        -765.694 82 11 -1579.862 -1598.907
## 
## Clustering table:
##  1  2  3  4 
##  7 35 32  8 
## 
## Mixing probabilities:
##          1          2          3          4 
## 0.08440635 0.38660329 0.37116156 0.15782880 
## 
## Means:
##         1         2         3         4 
##  9707.492 19804.259 22879.486 24459.536 
## 
## Variances:
##          1          2          3          4 
##   177296.7   436160.9  1261611.3 34437115.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#pot the density plot of the model
# par(fig=c(0,1,0,1),new=T)
plot(mod,what=&amp;quot;density&amp;quot;)
title (&amp;quot;Density plot of the finite mixture model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-1-25.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BIC
mclustBIC(galaxies)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Bayesian Information Criterion (BIC): 
##           E         V
## 1 -1622.361 -1622.361
## 2 -1631.243 -1595.403
## 3 -1584.016 -1592.299
## 4 -1592.828 -1579.862
## 5 -1592.299 -1593.277
## 6 -1601.228 -1604.069
## 7 -1588.610 -1611.538
## 8 -1597.427 -1625.804
## 9 -1600.709 -1633.494
## 
## Top 3 models based on the BIC criterion: 
##       V,4       E,3       E,7 
## -1579.862 -1584.016 -1588.610&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;1.a. All graphs have similar shapes for hist() and truehist(), except for a scaling factor. This is expected as hist() shows the frequency along the y axis, whereas truehist() produces the probability, which is a scaled version of the frequency. We can make some estimation about the distribution of the data, but cannot comment in a parametric way. Just based on the plots, we can tell that the data are highly congested in the middle of the data range. We can best think that there are three clusters of data, but qplot(), shows an extra cluster in the middle-congested cluster of data. By observing the qplot(),we can assume that there are four clusters in the data set.&lt;/p&gt;
&lt;p&gt;1.b. Here a scaled version of the data in part (a) is used to construct the same plots. Similar comments like in part (a) is valid here as well to describe the similarity of hist() and truehist(). From this scaled data hist() and truehist(), still it is reasonable to guess that there are three clusters.&lt;/p&gt;
&lt;p&gt;1.c. Here over smoothing and under smoothing bin width choice were easy and straight forward. Startting with the extreme bw values, 5000, 500 and 1100 were used to generate the three Kernels for Gaussian and Triangular. Searching for the best fit just by visualizing was not easy. By changing the the binwidth, we can actually set the Kernel to better fit for the best approximate of clusters.&lt;/p&gt;
&lt;p&gt;1.d. From the figures found in part (c), we can assume that there are four clusters in the data set with unequal variances. It was hard to determine manually, but my guess was that there are about three or four clusters with almost equal possiblity.&lt;/p&gt;
&lt;p&gt;1.e. The mclust() found four clusters of unequal variance for the best fit. However, the density plot of the model indicates there are three clusters. Based on this information, we can say that there are at least 3 or 4 clusters that can be determined from this analysis.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The  data from  gives the birth and death rates for 69 countries (from Hartigan, 1975). (8.2 Handbook)&lt;/p&gt;
&lt;p&gt;a.) Produce a scatterplot of the data and overlay a contour plot of the estimated bivariate density.&lt;/p&gt;
&lt;p&gt;b.) Does the plot give you any interesting insights into the possible structure of the data?&lt;/p&gt;
&lt;p&gt;c.) Construct the perspective plot (persp() in R, GGplot is not required for this question).&lt;/p&gt;
&lt;p&gt;d.) Model-based clustering (Mclust). Provide plot of the summary of your fit (BIC, classification, uncertainty, and density).&lt;/p&gt;
&lt;p&gt;e.) Discuss the results (structure of data, outliers, etc.). Write a discussion in the context of the problem.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HSAUR3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: tools&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(KernSmooth)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## KernSmooth 2.23 loaded
## Copyright M. P. Wand 1997-2009&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(birthdeathrates)
head(birthdeathrates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     birth death
## alg  36.4  14.6
## con  37.3   8.0
## egy  42.1  15.3
## gha  55.8  25.6
## ict  56.1  33.1
## mag  41.8  15.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(birthdeathrates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(birthdeathrates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 69&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message (&amp;quot;# 2a&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 2a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BDRd &amp;lt;- bkde2D(birthdeathrates, bandwidth = sapply(birthdeathrates, dpik))
contour(x=BDRd$x1, y=BDRd$x2, z=BDRd$fhat,
        main = &amp;quot;base R: Countour Scatterplot of Birth_Death_Rates&amp;quot;,
        xlab=&amp;quot;Birth Rates&amp;quot;, 
        ylab=&amp;quot;Death Rates&amp;quot;,
        xlim =c(0,60), ylim = c(0,35))
points(birthdeathrates, pch=16, col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=birthdeathrates,aes(birth,death)) + 
  geom_density2d(aes(colour=..level..)) + 
  scale_colour_gradient(low=&amp;quot;green&amp;quot;,high=&amp;quot;red&amp;quot;) + 
  theme_bw() +
  geom_point() +
  # geom_text(aes(label=ifelse(death &amp;gt;= 15 | birth &amp;gt;= 35,row.names(birthdeathrates),&amp;#39;&amp;#39;),hjust=0, vjust=0)) +
  labs(title=&amp;#39;ggplot: Countour Scatterplot of Birth_Death_Rates&amp;#39;,
       x=&amp;#39;Birth Rate&amp;#39;,
       y=&amp;#39;Death Rate&amp;#39;) +
  scale_x_continuous(limits = c(0,60)) +
  scale_y_continuous(limits = c(0,35))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;# 2c.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 2c.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persp (x=BDRd$x1, y=BDRd$x2, z=BDRd$fhat,
       xlab=&amp;quot;Birth Rates&amp;quot;, 
       ylab=&amp;quot;Death Rates&amp;quot;,
       zlab=&amp;quot;Estimated Density&amp;quot;,
       theta=-35, axes=TRUE, box=TRUE, main = &amp;quot;Perspective plot for birthdeathrates data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;# 2d.&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # 2d.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mclust)

mod &amp;lt;- Mclust(birthdeathrates)
mod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (EII,4) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod, parameters = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust EII (spherical, equal volume) model with 4 components: 
## 
##  log-likelihood  n df       BIC       ICL
##       -424.4194 69 12 -899.6481 -906.4841
## 
## Clustering table:
##  1  2  3  4 
##  2 17 38 12 
## 
## Mixing probabilities:
##          1          2          3          4 
## 0.02898652 0.24555002 0.55023375 0.17522972 
## 
## Means:
##           [,1]     [,2]      [,3]      [,4]
## birth 55.94967 43.80396 19.922913 33.730672
## death 29.34960 12.09411  9.081348  8.535812
## 
## Variances:
## [,,1]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108
## [,,2]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108
## [,,3]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108
## [,,4]
##         birth   death
## birth 10.2108  0.0000
## death  0.0000 10.2108&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;Value for mod$parameters$mean&amp;quot;, &amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Value for mod$parameters$mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(mod$parameters$variance$sigmasq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.195434&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BIC.data&amp;lt;- as.data.frame(mod$BIC[,])
BIC.data$NumComp&amp;lt;-rownames(BIC.data)
melted.BIC&amp;lt;- reshape2::melt(BIC.data, var.ids= &amp;quot;NumComp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Using NumComp as id variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# length(levels(melted.BIC$variable))

par(mfrow=c(1,1), ask=FALSE)
# BIC
plot(mod, what=&amp;quot;BIC&amp;quot;, main = &amp;quot;base R: Plot of BIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(melted.BIC, aes(x=as.numeric(NumComp), y=value, colour=variable, group=variable))+
  scale_x_continuous(&amp;quot;Number of Components&amp;quot;)+
  scale_y_continuous(&amp;quot;BIC&amp;quot;)+
  scale_colour_hue(&amp;quot;&amp;quot;)+
  geom_point()+
  geom_line()+
  theme_bw() + 
  labs(title = &amp;quot;ggplot: BIC&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 19 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 19 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# uncertainty
par(mfrow=c(1,1), ask=FALSE)
plot(mod, what=&amp;quot;uncertainty&amp;quot;)
title(main = &amp;quot;base R: Plot of uncertainty&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birthdeathrates %&amp;gt;% mutate(uncertainty = mod$uncertainty,
                classification = factor(mod$classification)) %&amp;gt;% 
  ggplot(aes(birth, death, size = uncertainty, color = classification)) +
  geom_point() + 
  guides(size = guide_legend(), colour = &amp;quot;legend&amp;quot;) + theme_classic() +
  stat_ellipse(level = 0.5, type = &amp;quot;t&amp;quot;) + 
  labs(x = &amp;quot;birth&amp;quot;, y = &amp;quot;death&amp;quot;, title = &amp;quot;ggplot: Uncertainty&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Too few points to calculate an ellipse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# classification
par(mfrow=c(1,1), ask=FALSE)
plot(mod, what=&amp;quot;classification&amp;quot;)
title(main = &amp;quot;base R: Plot of classification&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birthdeathrates %&amp;gt;% mutate(uncertainty = mod$uncertainty,
                classification = factor(mod$classification)) %&amp;gt;% 
  ggplot(aes(birth, death, shape = classification, color = classification)) +
  geom_point() + 
  guides(size = guide_legend(), shape = guide_legend()) + theme_classic() +
  stat_ellipse(level = 0.5, type = &amp;quot;t&amp;quot;) + 
  labs(x = &amp;quot;birth&amp;quot;, y = &amp;quot;death&amp;quot;, title = &amp;quot;ggplot: Plot of classification&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Too few points to calculate an ellipse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# density
par(mfrow=c(1,1), ask=FALSE)
plot(mod, what=&amp;quot;density&amp;quot;)
title(main = &amp;quot;base R: Plot of density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(birthdeathrates, aes(x = birth, y = death)) +
  geom_point() +
  geom_density_2d() + 
  labs ( title = &amp;quot;ggplot: Plot of density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-2-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Discussion:&lt;/p&gt;
&lt;p&gt;2.b. Comparaing data for birth rates from 10 to about 50 with the death rates, we can tell that the death rate is relatively slow. I would say twice as many people are being born than they are dying (i.e., 2:1 birth to death ratio). The countour appears to show the majority of countries grouping around a birthrate of 20 and a death rate of 10.&lt;/p&gt;
&lt;p&gt;2.e. The BIC plot indicates that there are 4 clusters. The classification plot seems to indicate the groupings of the data. The table of means indicate that the birth rates group along ~ 20, 34, 44, and 56. The death rates group around 9, 8, 12 and 30.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A sex difference in the age of onset of schizophrenia was noted by Kraepelin (1919). Subsequent epidemiological studies of the disorder have consistently shown an earlier onset in men than in women. One model that has been suggested to explain this observed difference is known as the subtype model which postulates two types of schizophrenia, one characterized by early onset, typical symptoms and poor premorbid competence; and the other by late onset, atypical symptoms and good premorbid competence. The early onset type is assumed to be largely a disorder of men and the late onset largely a disorder of women. Fit finite mixutres of normal densities separately to the onset data for men and women given in the  data from . See if you can produce some evidence for or against the subtype model. (8.3 Handbook)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Answer:&lt;/p&gt;
&lt;p&gt;Based on this density plot faceted by gender, we can tell that the distribution in diagnosis of disease is centered towards 20 years (age) in males whereas, for females, its more uniform thoughout the life.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(schizophrenia)
#plot the schizophrenia data using stat_density within ggplot2, facet by gender
# par(fig=c(0,1,0,1),new=T)
ggplot(data=schizophrenia)+
  stat_density(kernel=&amp;#39;gaussian&amp;#39;,adjust=1,aes(age,fill=gender)) +
  facet_grid(gender~.) +
       labs(title = &amp;#39;Density plot (gaussian) of Schizophrenia diagnosis data&amp;#39;,
       x = &amp;quot;Diagnosis Age&amp;quot;,
       y=&amp;#39;Density Estimate&amp;#39;) +
  scale_fill_manual( values = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;)) +
  theme(
        strip.background = element_blank(),
        strip.text.y = element_blank(),
        panel.background = element_blank()
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also check the distribution of the data using histogram plots. These plots also show that male diagnosis age numbers are centered more around mid 20’s, where as females patients can also be diagnosed in 40s and fewer (compared to males) in mid 20s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# par(fig=c(0,1,0,1),new=T)
ggplot(data=schizophrenia)+
  geom_histogram(aes(age,fill=gender)) +
  facet_grid(gender~.) +
  labs(title = &amp;#39;Histogram of Schizophrenia Diagnosis by Gender&amp;#39;,
       x = &amp;#39;Age of diagnosis&amp;#39;,
       y=&amp;#39;Frequency&amp;#39;) +
scale_fill_manual( values = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;)) +
# scale_color_brewer(palette = &amp;quot;Set2&amp;quot;) +
  theme(
        strip.background = element_blank(),
        strip.text.y = element_blank(),
        panel.background = element_blank()
        )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or we can visualize both together and see the same results discussed above by making this plot below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setMen &amp;lt;- subset(schizophrenia, gender==&amp;quot;male&amp;quot;)$age
setWmn &amp;lt;- subset(schizophrenia, gender!=&amp;quot;male&amp;quot;)$age

par(mfrow=c(1,1))
hist (schizophrenia$age, xlab=&amp;quot;Age&amp;quot;, ylab=&amp;quot;Density&amp;quot;, main=&amp;quot;Distribution of Schizophrenia Onset by Age&amp;quot;, freq=FALSE, ylim=c(0,.075), border=4)
lines(density(setMen), col=1, )
lines(density(setWmn), col=2)
legend(40, 0.05, legend=c(&amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;),
       col=c(1,2), lty=c(1,1), cex=0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can subset the schizophrenia data by male and female for fit of model analysis by gender:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(schizophrenia)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   age gender
## 1  20 female
## 2  30 female
## 3  21 female
## 4  23 female
## 5  30 female
## 6  25 female&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;onset=schizophrenia
male=subset(onset,gender==&amp;quot;male&amp;quot;)
female=subset(onset,gender==&amp;quot;female&amp;quot;)
message(&amp;quot;
Mclust Data
        &amp;quot;,&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mclust Data
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_male=Mclust(male$age)
mod_female=Mclust(female$age)
# par(mfrow=c(2,2))
plot(mod_male, what = &amp;quot;BIC&amp;quot;)
title(main=&amp;#39;BIC Of schizophrenia for male&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mod_female, what = &amp;quot;BIC&amp;quot;)
title(main=&amp;#39;BIC Of schizophrenia for female&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Density_Estimation_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;message(&amp;quot;Male
        &amp;quot;,&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Male
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(mod_male)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (V,2) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(mod_male, parameters = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust V (univariate, unequal variance) model with 2 components: 
## 
##  log-likelihood   n df       BIC       ICL
##       -520.9747 152  5 -1067.069 -1134.392
## 
## Clustering table:
##  1  2 
## 99 53 
## 
## Mixing probabilities:
##         1         2 
## 0.5104189 0.4895811 
## 
## Means:
##        1        2 
## 20.23922 27.74615 
## 
## Variances:
##          1          2 
##   9.395305 111.997525&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# print(mod_male$parameters)
message(&amp;quot;FeMale
        &amp;quot;,&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## FeMale
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(mod_female)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;Mclust&amp;#39; model object: (E,2) 
## 
## Available components: 
##  [1] &amp;quot;call&amp;quot;           &amp;quot;data&amp;quot;           &amp;quot;modelName&amp;quot;      &amp;quot;n&amp;quot;             
##  [5] &amp;quot;d&amp;quot;              &amp;quot;G&amp;quot;              &amp;quot;BIC&amp;quot;            &amp;quot;bic&amp;quot;           
##  [9] &amp;quot;loglik&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;hypvol&amp;quot;         &amp;quot;parameters&amp;quot;    
## [13] &amp;quot;z&amp;quot;              &amp;quot;classification&amp;quot; &amp;quot;uncertainty&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(mod_female, parameters = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust E (univariate, equal variance) model with 2 components: 
## 
##  log-likelihood  n df       BIC       ICL
##       -373.6992 99  4 -765.7788 -774.8935
## 
## Clustering table:
##  1  2 
## 74 25 
## 
## Mixing probabilities:
##         1         2 
## 0.7472883 0.2527117 
## 
## Means:
##        1        2 
## 24.93517 46.85570 
## 
## Variances:
##        1        2 
## 44.55641 44.55641&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# print(mod_female$parameters)

# male group mean
message(paste0(&amp;quot;Male Group mean is 0.51*20.23+(1-0.51)*27.75 = &amp;quot;, 0.51*20.23+(1-0.51)*27.75))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Male Group mean is 0.51*20.23+(1-0.51)*27.75 = 23.9148&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Female group mean
message(paste0(&amp;quot;Female Group mean is 0.746*24.93+(1-0.747)*46.85 = &amp;quot;, 0.746*24.93+(1-0.747)*46.85))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Female Group mean is 0.746*24.93+(1-0.747)*46.85 = 30.45083&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model summary above, we can see that the female model showing data points centered at about 25 and age 47 of age marks, whereas for males it was at around 20 and 27 years of age (i.e., within 20s). For males, the mean was calculated to be 23.91. Similarly, the mean of whole female group was calculated to be 30.45. So female group has a larger age mean which tells us that the males have onset of disorder earlier than females. Additionally, the BIC plot shows that the optimal number of cluster for both males and females is 2.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resampling</title>
      <link>/achalneupane.github.io/post/resampling/</link>
      <pubDate>Sun, 18 Aug 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/resampling/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;instructions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Instructions&lt;/h1&gt;
&lt;p&gt;There are four exercises below. All are optional. You may solve these, one solution per exercise, and submit them for extra credit. You use R, SAS or Python as you wish.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1.&lt;/h1&gt;
&lt;p&gt;Consider the Hidalgo data set from Homework 9. Load the data, and calculate mean and median.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hidalgo.dat = read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/hidalgo.dat&amp;quot;,
                         header = T,
                         sep = &amp;quot;,&amp;quot;
)

mean.hidalgo &amp;lt;- mean(hidalgo.dat$X.060)
mean.hidalgo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08607851&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median.hidalgo &amp;lt;- median(hidalgo.dat$X.060)
median.hidalgo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;part-a.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Calculate a jackknife estimate of the mean, and jackknife standard error of the mean, of these data. Are these values what you expect?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- hidalgo.dat$X.060
n &amp;lt;- length(y)
hat.theta.rep &amp;lt;- {}
for(i in 1:n){
  hat.theta.rep[i] = mean(y[-i])
}
bar.theta &amp;lt;- mean(hat.theta.rep)
# jackknife estimate of the mean
bar.theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08607851&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hat.theta &amp;lt;- mean.hidalgo
# bias
print(bias.jack &amp;lt;- (n-1) * (bar.theta - hat.theta))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#jackknife standard error for mean would be 
jackknife.stderr.mean &amp;lt;- sqrt((n-1) * mean((hat.theta.rep - mean(hat.theta.rep))^2))
jackknife.stderr.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006787497&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which can also be calculated as:
variance.of.hat.theta.rep &amp;lt;- var(hat.theta.rep)
jackknife.var = ((n-1)^2/n)*variance.of.hat.theta.rep
jackknife.stderr.mean = sqrt(jackknife.var)
jackknife.stderr.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006787497&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, I think these values are what we should expect.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Calculate a jackknife estimate of the median, and jackknife standard error of the median, of these data. Are these values what you expect?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(hat.theta &amp;lt;- median(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  n &amp;lt;- length(y)
  hat.theta.rep &amp;lt;- {}
  for(i in 1:n){
    hat.theta.rep[i] = median(y[-i])
  }
  bar.theta &amp;lt;- mean(hat.theta.rep)
  bar.theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  hat.theta &amp;lt;- median.hidalgo
  # bias
  print(bias.jack &amp;lt;- (n-1) * (bar.theta - hat.theta))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  #jackknife standard error for median would be 
  jackknife.stderr.median &amp;lt;- sqrt((n-1) * mean((hat.theta.rep - mean(hat.theta.rep))^2))
  jackknife.stderr.median&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # # Also can be calculated as
  # variance.of.ests &amp;lt;- var(hat.theta.rep)
  # jackknife.var = ((n-1)^2/n)*variance.of.ests
  # jackknife.stderr = sqrt(jackknife.var)
  # jackknife.stderr
  
  # Or as:
  
  # n &amp;lt;-length(y)
  # theta &amp;lt;- median(y)
  # jk &amp;lt;- sapply(1:n, function(i) median(y[-i]))
  # 
  # thetaBar &amp;lt;- mean(jk)
  # thetaBar
  # biasEst &amp;lt;- (n - 1) * (thetaBar - theta)
  # biasEst
  # seEst &amp;lt;- sqrt((n - 1) * mean((jk - thetaBar) ^ 2))
  # seEst
  
  
  
  # # To check our answer, we can check with the bootstrap package
  # 
  # library(bootstrap)
  # out &amp;lt;- jackknife(y, median)
  # out$jack.se
  # out$jack.bias&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Calculate a bootstrap estimate of the mean, and of the median, of the Hidalgo data. Use &lt;span class=&#34;math inline&#34;&gt;\(B = 1000\)&lt;/span&gt; samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; B &amp;lt;- 1000
  star.theta.rep &amp;lt;- rep(0,B)
  star.theta &amp;lt;- mean(y)
  for(i in 1:B) {
    star.theta.rep[i] &amp;lt;- mean(sample(y,replace=TRUE))
  }
  # bootstrap estimate of the mean
  star.bar.theta &amp;lt;- mean (star.theta.rep)
  star.bar.theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08608903&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # BS estimate of standard error for mean
  se.boot.mean &amp;lt;- sd(star.theta.rep - mean(star.theta.rep))
  se.boot.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006715704&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  bstraps &amp;lt;- c()
  for (i in 1:1000) {
    bsample &amp;lt;- sample(y, length(y), replace=T)
    bstraps &amp;lt;- c(median(bsample), bstraps)}
  star.hat.theta &amp;lt;- median(bstraps)
  
  # bootstrap estimate of median
  star.hat.theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # BS estimate of standard error for mean
  se.boot.median &amp;lt;- sd(star.theta.rep - median(star.theta.rep))
  se.boot.median&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006715704&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-d.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part d.&lt;/h3&gt;
&lt;p&gt;When data are normally distributed, and for large samples, the standard error of the median can be approximated by
&lt;span class=&#34;math display&#34;&gt;\[
s.e._{med} = 1.253 \times s.e._{mean}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(s.e._{mean} = \sigma /\sqrt{n}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How do the jackknife and bootstrap estimates of standard error compare to the parametric estimates?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;std &amp;lt;- function(x) sd(x)/sqrt(length(x))
s.e.mean &amp;lt;- std(hidalgo.dat$X.060)
s.e.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006787497&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s.e.median &amp;lt;- s.e.mean * 1.253
s.e.median&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0008504733&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compared to 
jackknife.stderr.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006787497&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;jackknife.stderr.median&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;se.boot.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006715704&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;se.boot.median&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0006715704&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2.&lt;/h1&gt;
&lt;p&gt;Consider the data from Homework 6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NATR332.DAT &amp;lt;- data.frame(
  Y1 = c(146,141,135,142,140,143,138,137,142,136),
  Y2 = c(141,143,139,139,140,141,138,140,142,138)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the ratio of the two population means:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \frac{\mu_{Y1}}{\mu_{Y2}}
\]&lt;/span&gt;
Calculate jackknife and bootstrap estimates for &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\theta}\)&lt;/span&gt;, and for the standard error for &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\theta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;part-a.-jacknife.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a. Jacknife.&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  n &amp;lt;- nrow(NATR332.DAT)
  y &amp;lt;- NATR332.DAT$Y1
  z &amp;lt;- NATR332.DAT$Y2
  theta.hat &amp;lt;- mean(y)/ mean(z)
  
  # computing jackknife replicates leaving one-out estimates
  theta.jack &amp;lt;- numeric(n)
  
  for( i in 1:n){
    theta.jack[i] &amp;lt;- mean(y[-i])/mean(z[-1])
  }
  bias &amp;lt;- (n-1) * (mean(theta.jack)- theta.hat)
  bias&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.006423983&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # now, we calculate standard error for jackknife
  se.jack &amp;lt;- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2))
  se.jack&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.007824608&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-bootstrap.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b. Bootstrap.&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For Bootstrap
  
  # let
  B = 1000
  theta.b &amp;lt;- numeric(B)
  theta.hat &amp;lt;- mean(NATR332.DAT$Y1)/mean(NATR332.DAT$Y2)
  
  n &amp;lt;- nrow(NATR332.DAT)
  
  for(b in 1:B){
    i &amp;lt;- sample (1:n, size = n, replace = TRUE)
    y &amp;lt;- NATR332.DAT$Y1[i]
    z &amp;lt;- NATR332.DAT$Y2[i]
    theta.b[b] &amp;lt;- mean(y)/mean(z)
  }
  
  bias &amp;lt;- mean(theta.b) - theta.hat
  se.boot &amp;lt;- sd(theta.b)
  
  print(list(est = theta.hat, bias = bias, cv  = bias/se.boot, standard_error_bootstrap = se.boot))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $est
## [1] 0.9992862
## 
## $bias
## [1] -0.000204354
## 
## $cv
## [1] -0.03371384
## 
## $standard_error_bootstrap
## [1] 0.006061426&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;div id=&#34;part-a.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Consider the ELO data. Subset the data to exclude non-qualifiers - &lt;code&gt;NQ&lt;/code&gt; - then create a factor &lt;code&gt;AA&lt;/code&gt;. This will indicate if the wrestler that as All-American (top 8 places), or did not place in the tournament. Use &lt;code&gt;ActualFinish&lt;/code&gt; equals &lt;code&gt;AA&lt;/code&gt;. Next, calculate an effect size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; for the difference in ELO scores between All-American and non-All-American wrestlers; you will need to calculate means and standard deviations as necessary. Since the populations are unbalanced, you will need to use a pooled sd of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s_{pooled} = \sqrt{\frac{(n_1-1) s_1^2 + (n_2-1) s_2^2} {n_1 + n_2 -2}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elo.dat &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/elo.csv&amp;quot;, header = TRUE, row.names = 1, sep = &amp;quot;,&amp;quot;)
head(elo.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Weight Conference     ELO ActualFinish ExpectedFinish
## 3     125        ACC 1380.84      cons 24     E[cons 16]
## 4     125        ACC 1404.51      cons 12     E[cons 12]
## 5     125        ACC 1348.79      cons 24     E[cons 24]
## 6     125     Big 12 1312.73      cons 32          E[NQ]
## 8     125     Big 12 1373.79      cons 24     E[cons 16]
## 12    125     Big 12 1398.16      cons 12          E[NQ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elo.exNQ &amp;lt;- elo.dat[!grepl(&amp;quot;NQ&amp;quot;, elo.dat$ExpectedFinish),]
# elo.exNQ$ActualFinish &amp;lt;- as.factor(elo.exNQ$ActualFinish[grepl(&amp;quot;AA&amp;quot;, elo.exNQ$ActualFinish)])

# Creating a new column with AA and other
elo.exNQ$AA.Other &amp;lt;- as.factor(ifelse(grepl(&amp;#39;AA&amp;#39;, elo.exNQ$ActualFinish), &amp;#39;AA&amp;#39;,
                            ifelse(!grepl(&amp;#39;AA&amp;#39;, elo.exNQ$ActualFinish), &amp;#39;Other&amp;#39;, NA)))

# # meand AA
# mean.dat &amp;lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$AA.Other), mean), c(&amp;quot;group&amp;quot;, &amp;quot;mean&amp;quot;))
# sd.dat &amp;lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$AA.Other), sd), c(&amp;quot;group&amp;quot;, &amp;quot;SD&amp;quot;))
# count.dat &amp;lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$AA.Other), length), c(&amp;quot;group&amp;quot;, &amp;quot;Count&amp;quot;))

mean.dat &amp;lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$ExpectedFinish), mean), c(&amp;quot;group1&amp;quot;, &amp;quot;mean&amp;quot;))
sd.dat &amp;lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$ExpectedFinish), sd), c(&amp;quot;group1&amp;quot;, &amp;quot;SD&amp;quot;))
count.dat &amp;lt;- setNames(aggregate(elo.exNQ$ELO, list(elo.exNQ$ExpectedFinish), length), c(&amp;quot;group1&amp;quot;, &amp;quot;Count&amp;quot;))



Mean_SD_Count.dat &amp;lt;- Reduce(function(...)
  merge(..., by = c(&amp;quot;group1&amp;quot;), all.x = TRUE),
  lapply(
    list(mean.dat, sd.dat, count.dat),
    transform
    # grp = ave(seq_along(group), group, FUN = seq_along)
  ))

# Here as well, we will relabel our groups as &amp;#39;AA&amp;#39; and &amp;#39;Other&amp;#39;
Mean_SD_Count.dat$group &amp;lt;- as.factor(ifelse(grepl(&amp;#39;AA&amp;#39;, Mean_SD_Count.dat$group1), &amp;#39;AA&amp;#39;,
                       ifelse(!grepl(&amp;#39;AA&amp;#39;, Mean_SD_Count.dat$group1), &amp;#39;Other&amp;#39;, NA)))

sd_pooled  &amp;lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$group),
                                  function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) )
sd_pooled&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $AA
## [1] 41.31208
## 
## $Other
## [1] 16.58026&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we get the mean of mean
Mean &amp;lt;- lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$group), function(dd) mean(dd$mean))

# Now  for Counts
Count &amp;lt;- lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$group), function(dd) sum(dd$Count))
# combine two lists as a dataframe
table.mean.sd &amp;lt;- do.call(rbind, Map(data.frame, Mean = Mean, sd_pooled = sd_pooled, Count = Count))


cohen.d &amp;lt;- function(m1, s1, m2, s2){
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  return(cohens_d)
}

required.replicates &amp;lt;- function (m1, s1, m2, s2, alpha=0.05, beta=0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}


effect_size &amp;lt;- cohen.d(
  m1 = table.mean.sd$Mean[1],
  s1 = table.mean.sd$sd_pooled[1],
  m2 = table.mean.sd$Mean[2],
  s2 = table.mean.sd$sd_pooled[2]
)
effect_size&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.687879&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Req.Replicates &amp;lt;- required.replicates(
  m1 = table.mean.sd$Mean[1],
  s1 = table.mean.sd$sd_pooled[1],
  m2 = table.mean.sd$Mean[2],
  s2 = table.mean.sd$sd_pooled[2]
)
Req.Replicates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Calculate jackknife and bootstrap estimates of the error of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Since ELO is determined by a wrestlers success within a weight class, you will need to honor this grouping (or sampling) of the data. Calculate the jackknife by excluding one &lt;code&gt;Weight&lt;/code&gt; at a time from the data, and recalculating &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;; since there are 10 weight classes there should be 10 jackknife replicates.&lt;/p&gt;
&lt;p&gt;For the bootstrap, sample from the 10 weight classes (use &lt;code&gt;unique&lt;/code&gt; or &lt;code&gt;levels&lt;/code&gt;). Note that you will not be able simply subset the data on something like &lt;code&gt;Weight %in% samples&lt;/code&gt;, since the bootstrap will require duplicate samples. Instead, iterate over weight class samples and merge subsets of the original data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Compare your estimates of standard error to the parametric estimate, approximated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s.e._d ~ \sqrt{\frac{n_1 + n_2}{n_1 n_2} + \frac{d^2} {2(n_1 + n_2)}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n1 = table.mean.sd$Count[1]
n2 = table.mean.sd$Count[2]
d = effect_size

# parametric estimate of standard error
s.e_d &amp;lt;- sqrt((((n1 + n2)/(n1*n2))) + ((d^2)/(2*(n1+n2))))
s.e_d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1778143&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;p&gt;Consider the data for U.S. Wholesale price for pumpkins 2018 in &lt;code&gt;pumpkins.csv&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;part-a.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Load the data, and calculate the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test and the parametric &lt;span class=&#34;math inline&#34;&gt;\(P(&amp;gt;F)\)&lt;/span&gt; using the code below. (set &lt;code&gt;eval=TRUE&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins.dat &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv&amp;quot;, header = TRUE, sep = &amp;quot;,&amp;quot;)
attach(pumpkins.dat)
# print(summary(pumpkins.dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(aov(Price ~ Class, data=pumpkins.dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## Class        3  44687   14896   94.03 4.42e-14 ***
## Residuals   26   4119     158                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt &amp;lt;- summary(aov(Price ~ Class, data = pumpkins.dat))
# p value
tt[[1]][[&amp;quot;Pr(&amp;gt;F)&amp;quot;]][1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.421291e-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# F test
tt[[1]][[&amp;quot;F value&amp;quot;]][1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 94.03394&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Permute &lt;code&gt;Price&lt;/code&gt; over &lt;code&gt;Class&lt;/code&gt; - that assume create a new data set on the assumption that &lt;code&gt;Class&lt;/code&gt; has not influence on &lt;code&gt;Price&lt;/code&gt;. Do this 1000 times, and calculate the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; ratio for each. Plot the distribution of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, and calculate how many &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; are greater than the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; from part a. How does this compare with the parametric estimate for &lt;span class=&#34;math inline&#34;&gt;\(P(&amp;gt;F\)&lt;/span&gt;? Do you need to increase the number of permutations?&lt;/p&gt;
&lt;p&gt;Answer: I do not see any of the calculated F-statistics greater then the F-statistics calculated from part a. Perhaps I need to increase the number of permutations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
perm.theta.rep &amp;lt;- rep(0,B)
F_ratio &amp;lt;- {}
for(i in 1:B) {
  y.s &amp;lt;- sample(Price)
  gg &amp;lt;- aov(y.s ~ Class)
  # summary(tt)[[1]][[&amp;quot;Pr(&amp;gt;F)&amp;quot;]]
  F_ratio.tmp &amp;lt;- summary(gg)[[1]][[&amp;quot;F value&amp;quot;]][1]
  F_ratio &amp;lt;- c(F_ratio, F_ratio.tmp)
}

F_ratio[1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 4.16161486 0.69671325 1.38049072 0.74068580 0.07683598 0.70792231
##  [7] 1.21110343 0.49387700 1.33360773 3.19540938&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Distribution of F-ratio
hist(F_ratio)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Resampling_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many $F$ are greater than the $F$ from part a
sum(F_ratio &amp;gt; tt[[1]][[&amp;quot;F value&amp;quot;]][1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Repeat part b, but this time, honor the &lt;code&gt;Week&lt;/code&gt; grouping. That is, permute &lt;code&gt;Price&lt;/code&gt; over &lt;code&gt;Class&lt;/code&gt; only within observations grouped by &lt;code&gt;Week&lt;/code&gt;. Compare this to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(aov(Price ~ Class + as.factor(Week), data=pumpkins.dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## Class            3  44687   14896  76.833 3.75e-11 ***
## as.factor(Week)  6    241      40   0.207     0.97    
## Residuals       20   3877     194                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
perm.theta.rep &amp;lt;- rep(0,B)
F_ratio &amp;lt;- {}
for(i in 1:B) {
  y.s &amp;lt;- sample(Price)
  gg &amp;lt;- aov(y.s ~ Class + as.factor(Week))
  # summary(tt)[[1]][[&amp;quot;Pr(&amp;gt;F)&amp;quot;]]
  F_ratio.tmp &amp;lt;- summary(gg)[[1]][[&amp;quot;F value&amp;quot;]][1]
  F_ratio &amp;lt;- c(F_ratio, F_ratio.tmp)
}

F_ratio[1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.3341739 1.8332347 0.2162528 0.5687652 0.9283558 3.0491414 0.9716676
##  [8] 0.1792787 1.4972060 0.1746379&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Distribution of F-ratio
hist(F_ratio)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Resampling_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt &amp;lt;- summary(aov(Price ~ Class + as.factor(Week), data = pumpkins.dat))
sum(F_ratio &amp;gt; tt[[1]][[&amp;quot;F value&amp;quot;]][1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which are more appropriate for these data?&lt;/p&gt;
&lt;p&gt;I think having more number of permutations help in getting better statistics for these data.&lt;/p&gt;
&lt;p&gt;Comments:
Ex 3. For the bootstrap, something like&lt;/p&gt;
&lt;p&gt;weights &amp;lt;- unique(elo.dat&lt;span class=&#34;math inline&#34;&gt;\(Weight) for (i in 1:samples) { weight.samples &amp;lt;- sample(weights,replace=TRUE) elo.sub &amp;lt;- c() for(w in weight.samples) { elo.sub &amp;lt;- elo.dat[(elo.dat\)&lt;/span&gt;Weight == w),]
}&lt;/p&gt;
&lt;p&gt;It is a harder exercise, because we need to respect the sampling method of the original data, and for these data, wrestlers are grouped by weight class, so we need to sample weight classes. Similarly, for Ex 4, we want to sample Price observations within Week groups, so&lt;/p&gt;
&lt;p&gt;weeks &amp;lt;- unique(pumpkins.dat&lt;span class=&#34;math inline&#34;&gt;\(Week) for(i in 1:10000) {  for(week in weeks) {  mask &amp;lt;- pumpkins.dat\)&lt;/span&gt;Week==week
pumpkins.dat&lt;span class=&#34;math inline&#34;&gt;\(Price[mask] &amp;lt;- sample(pumpkins.dat\)&lt;/span&gt;Price[mask])
}
pumpkins.dat&lt;span class=&#34;math inline&#34;&gt;\(Price[mask] &amp;lt;-sample(pumpkins.dat\)&lt;/span&gt;Price[mask])
}&lt;/p&gt;
&lt;p&gt;Consider if Ex 4 had been executed as a randomized complete block design, with Week as block. You would have independently randomized and applied treatment (Class) to each experimental unit within the block. We need to do the same thing with the bootstrap.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Roles of argonautes  and dicers on Sclerotinia sclerotiorum antiviral RNA silencing</title>
      <link>/achalneupane.github.io/publication/neupane_etal_2019_frontiers/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/publication/neupane_etal_2019_frontiers/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RNA silencing or RNA interference (RNAi) is an essential mechanism in animals, fungi, and plants that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. We previously determined that mycoviruses are triggers and targets of RNA silencing in Sclerotinia sclerotiorum. However, recent progresses in RNAi or dsRNA-based pest control requires more detailed characterization of the RNA silencing pathways in S. sclerotiorum to investigate the utility of dsRNA-based strategy for white mold control. This study elucidates the roles of argonaute enzymes, agl-2 and agl-4, in small RNA metabolism in S. sclerotiorum. Gene disruption mutants of agl-2 and agl-4 were compared for changes in phenotype, virulence, viral susceptibility, and small RNA profiles. The Δagl-2 mutant but not the Δagl-4 mutant had significantly slower growth and virulence prior to virus infection. Similarly, the Δagl-2 mutant but not the Δagl-4 mutant, showed greater debilitation under virus infection compared to uninfected strains. The responses were confirmed in complementation studies and revealed the antiviral role of agl-2. Gene disruption mutants of agl-2, agl-4, Dicer-like (dcl)-1, and dcl-2 did not change the stability of the most abundant endogenous small RNAs, which suggests the existence of alternative enzymes/pathways for small RNA biogenesis in S. sclerotiorum. Furthermore, in vitro synthesized dsRNA targeting agl-2 showed a significantly reduced average lesion diameter (P &amp;lt; 0.05) on canola leaves with agl-2 down-regulated compared to controls. This is the first report describing the effectiveness of RNA pesticides targeting S. sclerotiorum RNA silencing pathway for the control of the economically important pathogen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Profiling the Classes of the RNA Editing in Virus-derived small RNAs in White Mold Sclerotinia sclerotiorum</title>
      <link>/achalneupane.github.io/talk/asv_2019/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/asv_2019/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Profiling the classes of the RNA silencing, also known as RNA interference, is
an essential mechanism in plants, animals and fungi
that functions in gene regulation and defense against
foreign nucleic acids. In fungi, RNA silencing has
been shown to function primarily in defense against
invasive nucleic acids. RNA-silencing- deficient fungi
show increased susceptibility to virus infection. Little
is known about the classes of RNA editing in virus-
derived small RNA which will teach us the nature
of self-nonself recognition and ways to modulate
RNA modification to control fungal infections.
The present study dissects the RNA silencing pathway
in &lt;em&gt;S&lt;/em&gt;. &lt;em&gt;sclerotiorum&lt;/em&gt; by disrupting its key silencing
genes using the split-marker recombination method
in order to probe the contributions of these genes,
specifically argonautes, to fungal virulence and viral
defense mechanisms. Following gene disruption,
mutants were studied for changes in phenotype,
pathogenicity, viral susceptibility, and small RNA
processing compared to the wild-type strain, DK3.
Among the argonaute mutants, the agl-2 mutant
had significantly slower growth and virulence prior
to and following virus infection. Additional analyses
indicated that the virus-infected wild-type strain
accumulated virus-derived small RNAS (vsiRNAs)
with distinct patterns of internal and terminal
nucleotide mismatches. Dicer 1 mutant produced
less vsiRNA compared to dicer 2 mutant and the wild
type strain, suggesting the two dicers are not in the
state of complete redundancy. This finding expands
our overall understanding of S. sclerotiorum and has
important implications for any current or future uses
of dsRNA and mycoviruses as disease control agents.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Additional graphs</title>
      <link>/achalneupane.github.io/post/additional_graphs/</link>
      <pubDate>Mon, 15 Jul 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/additional_graphs/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;general-instructions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General Instructions&lt;/h1&gt;
&lt;p&gt;There are 5 exercises, each is worth 10 points. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points.&lt;/p&gt;
&lt;p&gt;For this exercise, you may use whatever graphics library you desire.&lt;/p&gt;
&lt;div id=&#34;experimental&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Experimental&lt;/h4&gt;
&lt;p&gt;Again, you will be allowed to provide one solution using Python. Elaborate on the similarities and differences between Python function definitions and R or IML or Macro language.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1.&lt;/h1&gt;
&lt;p&gt;Load the &lt;code&gt;ncaa2018.csv&lt;/code&gt; data set and create histograms, QQ-norm and box-whisker plots for &lt;code&gt;ELO&lt;/code&gt;. Add a title to each plot, identifying the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncaa2018.dat = read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2018.csv&amp;quot;,
  header = T,
  sep = &amp;quot;,&amp;quot;
)
# head(ncaa2018.dat)
# histogram
hist(ncaa2018.dat$ELO, xlab = &amp;quot;ELO&amp;quot;, main = &amp;quot;Histogram of ELO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# QQ plot
qqnorm(ncaa2018.dat$ELO, main = &amp;quot;Normal QQ plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# box-whisker plot
boxplot(ncaa2018.dat$ELO, main = &amp;quot;Box-whisker of ELO&amp;quot;, ylab = &amp;quot;ELO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;A common recommendation to address issues of non-normality is to transform data to correct for skewness. One common transformation is the log transform.&lt;/p&gt;
&lt;p&gt;Transform &lt;code&gt;ELO&lt;/code&gt; to &lt;code&gt;log(ELO)&lt;/code&gt; and produce histograms, box-whisker and qqnorm plots of the transformed values. Are the transformed values more or less skewed than the original? (Note - the log transform is used to correct skewness, it is less useful for correcting kurtosis).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(log(ncaa2018.dat$ELO), xlab = &amp;quot;ELO&amp;quot;, main = &amp;quot;Histogram of log transformed ELO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# QQ plot
qqnorm(log(ncaa2018.dat$ELO), main = &amp;quot;Normal QQ plot of log transformed ELO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# box-whisker plot
boxplot(log(ncaa2018.dat$ELO), main = &amp;quot;Box-whisker of log transformed ELO&amp;quot;, ylab = &amp;quot;ELO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;
The values are less skewed now. Log transformation is a good way to display plot with skewedness in data.
# Exercise 2.&lt;/p&gt;
&lt;p&gt;Review Exercise 4, Homework 6, where you calculated skewness and kurtosis. The reference for this exercise, &lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm&#34; class=&#34;uri&#34;&gt;https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm&lt;/a&gt;, gives four example statistical distributions. We will reproduce the histograms, and add qqnorm and box-whisker plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a&lt;/h2&gt;
&lt;p&gt;Use the code below from lecture to draw 1000 samples from the normal distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.sample &amp;lt;- rnorm(1000, mean=0, sd=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look up the corresponding &lt;code&gt;r*&lt;/code&gt; functions in R for the Cauchy distribution (use location=0, scale=1), and the Weibull distribution (use shape = 1.5). For the double exponential, use you can use the &lt;code&gt;*laplace&lt;/code&gt; functions from the &lt;code&gt;rmutil&lt;/code&gt; library, or you can use &lt;code&gt;rexp(1000) - rexp(1000)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Draw 1000 samples from each of these distributions. Calculate skewness and kurtosis for each sample. You may use your own function, or use the &lt;code&gt;moments&lt;/code&gt; library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dcauchy.1000 &amp;lt;- rcauchy(1:1000, location = 0, scale = 1)
dweibull.1000 &amp;lt;- rweibull(1:1000, shape = 1.5)
rexp.1000 &amp;lt;- rexp(1000) - rexp(1000)



# Kurtosis
library(moments)

norm.sample.kurt &amp;lt;- kurtosis(norm.sample, na.rm = TRUE)
norm.sample.kurt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.021838&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dcauchy.1000.kurt &amp;lt;- kurtosis(dcauchy.1000, na.rm = TRUE)
dcauchy.1000.kurt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 738.9989&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dweibull.1000.kurt &amp;lt;- kurtosis(dweibull.1000, na.rm = TRUE)
dweibull.1000.kurt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.200549&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rexp.1000.kurt &amp;lt;- kurtosis(rexp.1000, na.rm = TRUE)
rexp.1000.kurt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.822286&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.sample.skew &amp;lt;- skewness(norm.sample, na.rm = TRUE)
norm.sample.skew&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.07906608&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dcauchy.1000.skew &amp;lt;- skewness(dcauchy.1000, na.rm = TRUE)
dcauchy.1000.skew&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25.7455&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dweibull.1000.skew &amp;lt;- skewness(dweibull.1000, na.rm = TRUE)
dweibull.1000.skew&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.010605&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rexp.1000.skew &amp;lt;- skewness(rexp.1000, na.rm = TRUE)
rexp.1000.skew&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04160438&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b&lt;/h2&gt;
&lt;p&gt;Plot the histograms for each distribution. Use &lt;code&gt;par(mfrow=c(2,2))&lt;/code&gt; in your code chunk to combine the four histogram in a single plot. Add titles to the histograms indicating the distribution. Set the x-axis label to show the calculated skewness and kurtosis, i.e. &lt;code&gt;skewness = ####, kurtosis = ####&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2,2))
hist(norm.sample, xlab = paste0(&amp;quot;skewness = &amp;quot;, norm.sample.skew , &amp;quot;, kurtosis = &amp;quot;, norm.sample.kurt), main = &amp;quot;Histogram of Normal Distribution&amp;quot;)
hist(dcauchy.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, dcauchy.1000.skew , &amp;quot;, kurtosis = &amp;quot;, dcauchy.1000.kurt), main = &amp;quot;Histogram of Cauchy Distribution&amp;quot;)
hist(dweibull.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, dweibull.1000.kurt , &amp;quot;, kurtosis = &amp;quot;, dweibull.1000.kurt), main = &amp;quot;Histogram of Dweibull Distribution&amp;quot;)
hist(rexp.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, rexp.1000.skew , &amp;quot;, kurtosis = &amp;quot;, rexp.1000.kurt), main = &amp;quot;Histogram of double Exponential Distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part c&lt;/h2&gt;
&lt;p&gt;Repeat Part b, but with QQ-norm plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2,2))
qqnorm(norm.sample, xlab = paste0(&amp;quot;skewness = &amp;quot;, norm.sample.skew , &amp;quot;, kurtosis = &amp;quot;, norm.sample.kurt), main = &amp;quot;QQplot of Normal Distribution&amp;quot;)
qqnorm(dcauchy.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, dcauchy.1000.skew , &amp;quot;, kurtosis = &amp;quot;, dcauchy.1000.kurt), main = &amp;quot;QQplot of Cauchy Distribution&amp;quot;)
qqnorm(dweibull.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, dweibull.1000.kurt , &amp;quot;, kurtosis = &amp;quot;, dweibull.1000.kurt), main = &amp;quot;QQplot of Dweibull Distribution&amp;quot;)
qqnorm(rexp.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, rexp.1000.skew , &amp;quot;, kurtosis = &amp;quot;, rexp.1000.kurt), main = &amp;quot;QQplot of rexp Distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-d&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part d&lt;/h2&gt;
&lt;p&gt;Repeat Part b, but with box-whisker plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2,2))
boxplot(norm.sample, xlab = paste0(&amp;quot;skewness = &amp;quot;, norm.sample.skew , &amp;quot;, kurtosis = &amp;quot;, norm.sample.kurt), main = &amp;quot;Boxplot of Normal Distribution&amp;quot;)
boxplot(dcauchy.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, dcauchy.1000.skew , &amp;quot;, kurtosis = &amp;quot;, dcauchy.1000.kurt), main = &amp;quot;Boxplot of Cauchy Distribution&amp;quot;)
boxplot(dweibull.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, dweibull.1000.kurt , &amp;quot;, kurtosis = &amp;quot;, dweibull.1000.kurt), main = &amp;quot;Boxplot of Dweibull Distribution&amp;quot;)
boxplot(rexp.1000, xlab = paste0(&amp;quot;skewness = &amp;quot;, rexp.1000.skew , &amp;quot;, kurtosis = &amp;quot;, rexp.1000.kurt), main = &amp;quot;Boxplot of double rexp Distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hints for SAS. If you create the samples in IML, use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Normal = j(1, 1000, .);
call randgen(Normal, &amp;quot;NORMAL&amp;quot;, 0, `);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can generate samples in the data step using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;do i = 1 to 1000;
  Normal = rand(&amp;#39;NORMAL&amp;#39;,0,1);
  output;
end;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RAND doesn’t provide a Laplace option, but you can create samples from this distribution by&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rand(&amp;#39;EXPONENTIAL&amp;#39;)-rand(&amp;#39;EXPONENTIAL&amp;#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To group multiple plots, use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ods graphics / width=8cm height=8cm;
ods layout gridded columns=2;
ods region;
 ... first plot

ods region;
 ... second plot

ods layout end;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might need to include&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ods graphics off;

ods graphics on;
ODS GRAPHICS / reset=All;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to return the SAS graphics output to normal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 3.&lt;/h2&gt;
&lt;p&gt;We will create a series of graphs illustrating how the Poisson distribution approaches the normal distribution with large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. We will iterate over a sequence of &lt;code&gt;lambda&lt;/code&gt;, from 2 to 64, doubling &lt;code&gt;lambda&lt;/code&gt; each time. For each ‘lambda’ draw 1000 samples from the Poisson distribution.&lt;/p&gt;
&lt;p&gt;Calculate the skewness of each set of samples, and produce histograms, QQ-norm and box-whisker plots. You can use &lt;code&gt;par(mfrow=c(1,3))&lt;/code&gt; to display all three for one &lt;code&gt;lambda&lt;/code&gt; in one line. Add &lt;code&gt;lambda=##&lt;/code&gt; to the title of the histogram, and &lt;code&gt;skewness=##&lt;/code&gt; to the title of the box-whisker plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b.&lt;/h2&gt;
&lt;p&gt;Remember that &lt;code&gt;lambda&lt;/code&gt; represents the mean of a discrete (counting) variable. At what size mean is Poisson data no longer skewed, relative to normally distributed data? You might run this 2 or 3 times, with different seeds; this number varies in my experience.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(moments)

rpois_skew_collect &amp;lt;- {}
mu &amp;lt;- 2

# set.seed(54321)
while(mu &amp;lt;= 64){
  
rpois_values &amp;lt;- rpois(1000, lambda = mu)
rpois_skew &amp;lt;- skewness(rpois_values, na.rm = TRUE)
rpois_skew_collect[mu-1] &amp;lt;- skewness(rpois_values)
# attaching all three plots together
par(mfrow=c(1,3))

# histogram
hist(rpois_values, main = paste0(&amp;quot;Histogram of rpois Distribution for lambda = &amp;quot;, mu))
# QQ plot
qqnorm(rpois_values, main = &amp;quot;Normal QQ plot for poisson&amp;quot;)

# box-whisker plot
boxplot(rpois_values, main = paste0(&amp;quot;Box-whisker with skewness = &amp;quot;, rpois_skew), ylab = &amp;quot;rpois_values&amp;quot;)
mu &amp;lt;- mu*2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-3.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-4.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-5.png&#34; width=&#34;1152&#34; /&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-6.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rpois_skew_collect)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-8-7.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the skewness starts to diminish significantly after mean size 32 (lambda) for poisson data.&lt;/p&gt;
&lt;p&gt;If you do this in SAS, create a data table with data columns each representing a different &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. You can see combined histogram, box-whisker and QQ-norm, for all columns, by calling&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;proc univariate data=Distributions plot;
run;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At what &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is skewness of the Poisson distribution small enough to be considered normal?
It looks like after mu 60, skewness is small enough to be considered normal.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;div id=&#34;part-a-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a&lt;/h2&gt;
&lt;p&gt;Write a function that accepts a vector &lt;code&gt;vec&lt;/code&gt;, a vector of integers, a main axis label and an x axis label. This function should
1. iterate over each element &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the vector of integers
2. produce a histogram for &lt;code&gt;vec&lt;/code&gt; setting the number of bins in the histogram to &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;
3. label main and x-axis with the specified parameters.
4. label the y-axis to read &lt;code&gt;Frequency, bins =&lt;/code&gt; and the number of bins.&lt;/p&gt;
&lt;p&gt;Hint:
You can simplify this function by using the parameter &lt;code&gt;...&lt;/code&gt; - see &lt;code&gt;?plot&lt;/code&gt; or ?&lt;code&gt;hist&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Writing the asked function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# vec &amp;lt;- c(12,36,60)
# dat &amp;lt;- hidalgo.dat[,1]
# i=1

plot.histograms &amp;lt;- function(dat, vec, main, xlab){
  par(mfrow=c(1,length(vec)))
  for(i in 1:length(vec)){
  print(hist(dat, breaks = vec[i],  main = main, xlab = xlab, ylab = paste0(&amp;#39;Frequency, bins = &amp;#39;, vec[i])))

}
}

# now read the file
hidalgo.dat = read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/hidalgo.dat&amp;quot;,
                   header = T,
                   sep = &amp;quot;,&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b&lt;/h2&gt;
&lt;p&gt;Test your function with the &lt;code&gt;hidalgo&lt;/code&gt; data set (see below), using bin numbers 12, 36, and 60. You should be able to call your function with something like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.histograms(hidalgo.dat[,1],c(12,36,60), main=&amp;quot;1872 Hidalgo issue&amp;quot;,xlab= &amp;quot;Thickness (mm)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $breaks
##  [1] 0.060 0.065 0.070 0.075 0.080 0.085 0.090 0.095 0.100 0.105 0.110 0.115
## [13] 0.120 0.125 0.130 0.135
## 
## $counts
##  [1]   3  35  93 131  45  24  19  35  31  32  15   8   7   5   1
## 
## $density
##  [1]  1.2396694 14.4628099 38.4297521 54.1322314 18.5950413  9.9173554
##  [7]  7.8512397 14.4628099 12.8099174 13.2231405  6.1983471  3.3057851
## [13]  2.8925620  2.0661157  0.4132231
## 
## $mids
##  [1] 0.0625 0.0675 0.0725 0.0775 0.0825 0.0875 0.0925 0.0975 0.1025 0.1075
## [11] 0.1125 0.1175 0.1225 0.1275 0.1325
## 
## $xname
## [1] &amp;quot;dat&amp;quot;
## 
## $equidist
## [1] TRUE
## 
## attr(,&amp;quot;class&amp;quot;)
## [1] &amp;quot;histogram&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $breaks
##  [1] 0.064 0.066 0.068 0.070 0.072 0.074 0.076 0.078 0.080 0.082 0.084 0.086
## [13] 0.088 0.090 0.092 0.094 0.096 0.098 0.100 0.102 0.104 0.106 0.108 0.110
## [25] 0.112 0.114 0.116 0.118 0.120 0.122 0.124 0.126 0.128 0.130 0.132
## 
## $counts
##  [1]  4  1 33 52 21 38 34 79 33 10  4  3 19  8  9  5 12 20 17  9  9 10 18  9  3
## [26]  3  1  7  3  2  2  1  4  1
## 
## $density
##  [1]  4.132231  1.033058 34.090909 53.719008 21.694215 39.256198 35.123967
##  [8] 81.611570 34.090909 10.330579  4.132231  3.099174 19.628099  8.264463
## [15]  9.297521  5.165289 12.396694 20.661157 17.561983  9.297521  9.297521
## [22] 10.330579 18.595041  9.297521  3.099174  3.099174  1.033058  7.231405
## [29]  3.099174  2.066116  2.066116  1.033058  4.132231  1.033058
## 
## $mids
##  [1] 0.065 0.067 0.069 0.071 0.073 0.075 0.077 0.079 0.081 0.083 0.085 0.087
## [13] 0.089 0.091 0.093 0.095 0.097 0.099 0.101 0.103 0.105 0.107 0.109 0.111
## [25] 0.113 0.115 0.117 0.119 0.121 0.123 0.125 0.127 0.129 0.131
## 
## $xname
## [1] &amp;quot;dat&amp;quot;
## 
## $equidist
## [1] TRUE
## 
## attr(,&amp;quot;class&amp;quot;)
## [1] &amp;quot;histogram&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## $breaks
##  [1] 0.064 0.065 0.066 0.067 0.068 0.069 0.070 0.071 0.072 0.073 0.074 0.075
## [13] 0.076 0.077 0.078 0.079 0.080 0.081 0.082 0.083 0.084 0.085 0.086 0.087
## [25] 0.088 0.089 0.090 0.091 0.092 0.093 0.094 0.095 0.096 0.097 0.098 0.099
## [37] 0.100 0.101 0.102 0.103 0.104 0.105 0.106 0.107 0.108 0.109 0.110 0.111
## [49] 0.112 0.113 0.114 0.115 0.116 0.117 0.118 0.119 0.120 0.121 0.122 0.123
## [61] 0.124 0.125 0.126 0.127 0.128 0.129 0.130 0.131
## 
## $counts
##  [1]  3  1  0  1  7 26 20 32 11 10 20 18 11 23 42 37 15 18  7  3  2  2  1  2 10
## [26]  9  3  5  6  3  2  3  7  5  5 15  9  8  7  2  5  4  3  7  7 11  4  5  0  3
## [51]  3  0  1  0  4  3  1  2  2  0  2  0  0  1  3  1  1
## 
## $density
##  [1]  6.198347  2.066116  0.000000  2.066116 14.462810 53.719008 41.322314
##  [8] 66.115702 22.727273 20.661157 41.322314 37.190083 22.727273 47.520661
## [15] 86.776860 76.446281 30.991736 37.190083 14.462810  6.198347  4.132231
## [22]  4.132231  2.066116  4.132231 20.661157 18.595041  6.198347 10.330579
## [29] 12.396694  6.198347  4.132231  6.198347 14.462810 10.330579 10.330579
## [36] 30.991736 18.595041 16.528926 14.462810  4.132231 10.330579  8.264463
## [43]  6.198347 14.462810 14.462810 22.727273  8.264463 10.330579  0.000000
## [50]  6.198347  6.198347  0.000000  2.066116  0.000000  8.264463  6.198347
## [57]  2.066116  4.132231  4.132231  0.000000  4.132231  0.000000  0.000000
## [64]  2.066116  6.198347  2.066116  2.066116
## 
## $mids
##  [1] 0.0645 0.0655 0.0665 0.0675 0.0685 0.0695 0.0705 0.0715 0.0725 0.0735
## [11] 0.0745 0.0755 0.0765 0.0775 0.0785 0.0795 0.0805 0.0815 0.0825 0.0835
## [21] 0.0845 0.0855 0.0865 0.0875 0.0885 0.0895 0.0905 0.0915 0.0925 0.0935
## [31] 0.0945 0.0955 0.0965 0.0975 0.0985 0.0995 0.1005 0.1015 0.1025 0.1035
## [41] 0.1045 0.1055 0.1065 0.1075 0.1085 0.1095 0.1105 0.1115 0.1125 0.1135
## [51] 0.1145 0.1155 0.1165 0.1175 0.1185 0.1195 0.1205 0.1215 0.1225 0.1235
## [61] 0.1245 0.1255 0.1265 0.1275 0.1285 0.1295 0.1305
## 
## $xname
## [1] &amp;quot;dat&amp;quot;
## 
## $equidist
## [1] TRUE
## 
## attr(,&amp;quot;class&amp;quot;)
## [1] &amp;quot;histogram&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to plot three different histograms of the &lt;code&gt;hidalgo&lt;/code&gt; data set.&lt;/p&gt;
&lt;p&gt;If you do this in SAS, write a macro that accepts a table name, a column name, a list of integers, a main axis label and an x axis label. This macro should scan over each element in the list of integers and produce a histogram for each integer value, setting the bin count to the element in the input list, and labeling main and x-axis with the specified parameters. You should label the y-axis to read &lt;code&gt;Frequency, bins =&lt;/code&gt; and the number of bins.&lt;/p&gt;
&lt;p&gt;Test your macro with the &lt;code&gt;hidalgo&lt;/code&gt; data set (see below), using bin numbers 12, 36, and 60. You should be able to call your macro with something like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;%plot_histograms(hidalgo, y, 12 36 60, main=&amp;quot;1872 Hidalgo issue&amp;quot;, xlabel=&amp;quot;Thickness (mm)&amp;quot;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to plot three different histograms of the &lt;code&gt;hidalgo&lt;/code&gt; data set.&lt;/p&gt;
&lt;p&gt;Hint:
Assume &lt;code&gt;12 36 60&lt;/code&gt; resolve to a single macro parameter and use &lt;code&gt;%scan&lt;/code&gt;. Your macro definition can look something like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;%macro plot_histograms(table_name, column_name, number_of_bins, main=&amp;quot;Main&amp;quot;, xlabel=&amp;quot;X Label&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;hidalgo&lt;/code&gt; data set is in the file &lt;code&gt;hidalgo.dat&lt;/code&gt; These data consist of paper thickness measurements of stamps from the 1872 Hidalgo issue of Mexico. This data set is commonly used to illustrate methods of determining the number of components in a mixture (in this case, different batches of paper). See &lt;a href=&#34;https://www.jstor.org/stable/2290118&#34; class=&#34;uri&#34;&gt;https://www.jstor.org/stable/2290118&lt;/a&gt;,&lt;br /&gt;
&lt;a href=&#34;https://books.google.com/books?id=1CuznRORa3EC&amp;amp;lpg=PA95&amp;amp;pg=PA94#v=onepage&amp;amp;q&amp;amp;f=false&#34; class=&#34;uri&#34;&gt;https://books.google.com/books?id=1CuznRORa3EC&amp;amp;lpg=PA95&amp;amp;pg=PA94#v=onepage&amp;amp;q&amp;amp;f=false&lt;/a&gt; and &lt;a href=&#34;https://books.google.com/books?id=c2_fAox0DQoC&amp;amp;pg=PA180&amp;amp;lpg=PA180&amp;amp;f=false&#34; class=&#34;uri&#34;&gt;https://books.google.com/books?id=c2_fAox0DQoC&amp;amp;pg=PA180&amp;amp;lpg=PA180&amp;amp;f=false&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Some analysis suggest there are three different mixtures of paper used to produce the 1872 Hidalgo issue; other analysis suggest seven. Why do you think there might be disagreement about the number of mixtures?&lt;/p&gt;
&lt;p&gt;That is perhaps because of the uncontrollable variation
in paper thickness people used to have from sheet to sheet, we would expect more than three or seven mixtures, but in case of hidalgo, thickness was not that high. We can see in the histogram that the thickness was maintianed below 0.07 mm with highest frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5.&lt;/h1&gt;
&lt;p&gt;We’ve been working with data from Wansink and Payne, Table 1:&lt;/p&gt;
&lt;div id=&#34;reproducing-part-of-wansink-table-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reproducing part of Wansink Table 1&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Measure&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1936&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1946&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1951&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1963&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1975&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1997&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2006&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;calories per recipe (SD)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2123.8 (1050.0)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2122.3 (1002.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2089.9 (1009.6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2250.0 (1078.6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2234.2 (1089.2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2249.6 (1094.8)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3051.9 (1496.2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;calories per serving (SD)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.1 (124.8)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;271.1 (124.2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;280.9 (116.2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;294.7 (117.7)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;285.6 (118.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;288.6 (122.0)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;384.4&lt;/strong&gt; (168.3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;servings per recipe (SD)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.9 (13.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.9 (13.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.0 (14.5)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.7 (14.6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.4 (14.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.4 (14.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.7 (13.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;However, in Homework 2, we also considered the value given in the text&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The resulting increase of 168.8 calories (from 268.1 calories … to &lt;strong&gt;436.9&lt;/strong&gt; calories …) represents a 63.0% increase … in calories per serving.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is a discrepancy between two values reported for calories per serving, 2006. We will use graphs to attempt to determine which value is most consistent.&lt;/p&gt;
&lt;p&gt;First, consider the relationship between Calories per Serving and Calories per Recipe:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Calories per Serving = Calories per Recipe / Servings per Recipe&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;Servings per Recipe&lt;/code&gt; is effectively constant over time (12.4-13.0), we can assume the relationship between &lt;code&gt;Calories per Serving&lt;/code&gt; and &lt;code&gt;Calories per Recipe&lt;/code&gt; is linear,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Calories per Serving} = \beta_0 + \beta_1 \times \text{Calories per Recipe}
\]&lt;/span&gt;
with &lt;span class=&#34;math inline&#34;&gt;\(\text{Servings per Recipe} = 1/\beta_1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will fit a linear model, with &lt;code&gt;Calories per Recipe&lt;/code&gt; as the independent variable against two sets of values for &lt;code&gt;Calories per Serving&lt;/code&gt;, such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assumption 1. The value in the table (&lt;span class=&#34;math inline&#34;&gt;\(384.4\)&lt;/span&gt;) is correct.&lt;/li&gt;
&lt;li&gt;Assumption 2. The value in the text (&lt;span class=&#34;math inline&#34;&gt;\(436.9\)&lt;/span&gt;) is correct.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Assumptions.dat &amp;lt;- data.frame(
  CaloriesPerRecipe = c(2123.8, 2122.3, 2089.9, 2250.0, 2234.2, 2249.6, 3051.9),
  Assumption1 = c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4),
  Assumption2 = c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 436.9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and fit linear models&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Assumption1.lm &amp;lt;- lm(Assumption1 ~ CaloriesPerRecipe,data=Assumptions.dat)
Assumption2.lm &amp;lt;- lm(Assumption2 ~ CaloriesPerRecipe,data=Assumptions.dat)
summary(Assumption1.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Assumption1 ~ CaloriesPerRecipe, data = Assumptions.dat)
## 
## Residuals:
##       1       2       3       4       5       6       7 
## -7.0238 -3.8475  9.7610  4.7417 -2.5010 -1.3112  0.1808 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       25.477429  17.351550   1.468    0.202    
## CaloriesPerRecipe  0.117547   0.007466  15.745 1.88e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 6.163 on 5 degrees of freedom
## Multiple R-squared:  0.9802, Adjusted R-squared:  0.9763 
## F-statistic: 247.9 on 1 and 5 DF,  p-value: 1.879e-05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(Assumption2.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Assumption2 ~ CaloriesPerRecipe, data = Assumptions.dat)
## 
## Residuals:
##       1       2       3       4       5       6       7 
## -4.1798 -0.9169 14.5608  0.3051 -6.0261 -5.7248  1.9817 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       -99.891018  21.933161  -4.554  0.00609 ** 
## CaloriesPerRecipe   0.175238   0.009437  18.569 8.34e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 7.79 on 5 degrees of freedom
## Multiple R-squared:  0.9857, Adjusted R-squared:  0.9828 
## F-statistic: 344.8 on 1 and 5 DF,  p-value: 8.336e-06&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-a.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Plot the regression. Use points to plot &lt;code&gt;Assumption1&lt;/code&gt; vs &lt;code&gt;CaloriesPerRecipe&lt;/code&gt;, and &lt;code&gt;Assumption2&lt;/code&gt; vs &lt;code&gt;CaloriesPerRecipe&lt;/code&gt;, on the same graph. Add lines (i.e. &lt;code&gt;abline&lt;/code&gt;) to show the fit from the regression. Use different colors for the two assumptions. Which of the two lines appears to best explain the data?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Assumptions.dat)
par(mfrow=c(1,1))
plot(Assumption1~CaloriesPerRecipe, cex = 1.5, type = &amp;#39;p&amp;#39;, col = &amp;#39;red&amp;#39;)
abline(Assumption1.lm)
points(Assumption2~CaloriesPerRecipe, cex = 1.5, col = &amp;#39;blue&amp;#39;)
abline(Assumption2.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Produce diagnostic plots plots of the residuals from both linear models (in R, use &lt;code&gt;residuals(Assumption1.lm)&lt;/code&gt;). qqnorm or box-whisker plots will probably be the most effective; there are too few points for a histogram.&lt;/p&gt;
&lt;p&gt;Use the code below to place two plots, side by side. You can produce more than one pair of plots, if you wise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
boxplot(residuals(Assumption1.lm), main = &amp;quot;Boxplot for Assumption1&amp;quot; ) 
boxplot(residuals(Assumption2.lm), main = &amp;quot;Boxplot for Assumption2&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
qqnorm(residuals(Assumption1.lm), main = &amp;quot;QQplot for Assumption1&amp;quot; )
qqline(residuals(Assumption1.lm))
qqnorm(residuals(Assumption2.lm), main = &amp;quot;QQplot for Assumption2&amp;quot; )
qqline(residuals(Assumption2.lm))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Additional_Graphs_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From these plots, which assumption is most likely correct. That is, which assumption produces a linear model that least violates assumptions of normality of the residual errors? Which assumption produces outliers in the residuals?&lt;/p&gt;
&lt;p&gt;Answer: Based on the plots, assumption1 produces a more linear model that least violates assumption of normality of the residuals errors. Assumption2 produces the outlier/s.&lt;/p&gt;
&lt;p&gt;I’ve included similar data and linear models for SAS in the SAS template. If you choose SAS, you will need to modify the PROC GLM code to produce the appropriate diagnostic plots.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Final assignment</title>
      <link>/achalneupane.github.io/post/final_assignment/</link>
      <pubDate>Mon, 15 Jul 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/final_assignment/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;&lt;strong&gt;Project Overview:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final project will be a continuation of the midterm project. We will continue working with data from four corn fields, but will be looking at some issues with times and dates.
For each field, I’ve uploaded two files, seed&lt;em&gt;.csv and harvest&lt;/em&gt;.csv, corresponding to seeding rate and yield data, respectively. Both sets of files have a column Timestamp with strings of the form
2018-05-20T13:20:08.201Z
Your task will be to extract the date and time values from these strings, and answer these questions. The text before ’T&amp;quot; is the date string, and the text between ’T&amp;quot; and “Z” is the time string, in universal time.
1. What is the range of planting dates in these data?
2. Was each field harvested entirely in one day, and where they harvested each at approximately the same time of day?
It might be enough to process only the first and last rows in the data. Each should be sampled at one second intervals, so the time difference between the first and last rows, in seconds, should be almost equal to the number of rows. I would not be too worried about gaps on the data on the order of seconds, but I would want to know about hour long gaps in the data, and where they occur. Note that time will reset to 0 at 23:59:59 and date will increment, so account for this in the analysis.
Some additional thoughts.
There is a relationship between planting date and yield. We could review the literature to get a more precise estimate (and you can do that, if you wish), but we’ll start with a rough back-of-the-envelope calculation.
Suppose we are working with 100-day corn - we expect corn to reach maturity in roughly 100 days. Let’s suppose that the difference between the first field planting date and the last field planting date is 5 days. That’s 5 percent of the growing period, so let %Diff = 5. What is the standard deviation for Yield, with regard to planting date? Again, we can look to the literature, but we’ll use a simplifying assumption that it will be similar to the sd for yield vs planting rate, as determined in the midterm project. Convert that to CV.
Now we have a first approximation for effect size (%Diff/CV). Is this effect size large enough that we need to worry that the effect of planting date will confound our analysis of the relationship between yield and planting rate? When I gather data from more fields, do I need to be careful and only include fields in a narrow range of dates? What is the first approximation for the number of fields required to test the relationship between planting date and yield?
Not, about gaps in harvest. I’ve been arguing that analyzing yield monitor data should be a two-step process. First, analysis the yield as a time series (as grain moves through the harvester) then analyze as spatial data (the as the harvester moves over the field). There will be auto-correlated errors in each process that should be analyzed independently.
In particular, the value for yield as reported in these data is not the yield of the grain as it is measured going through the harvester. The grain moving through the harvester will be of varying degrees of maturity, thus of grain moisture - less mature grain will have more water content, thus more weight. Yield values are standardized to a define percent moisture, so the harvester has a moisture sensor, and the percent moisture reading is use to normalize yield.
But yield is measured at one second intervals, while moisture can only be measured at approximately 10-15 second intervals. I’m curious if gaps in the harvest record will affect how yield is normalized by moisture - there may be some cases where the moisture reading is uncorrelated with yield, because of this difference in measurement (I suspect it will be very small, but it’s an interesting problem, to me).
I’m also interested in how percent moisture changes over the course of a day, and if there is an effect when fields are harvested at different times of day.
More will follow in the discussions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The algorithm to do this analysis is divided into several steps as described below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First you have to define the path to all csv files for final under (&lt;code&gt;path.final&lt;/code&gt;) and all .csv files from midterm under &lt;code&gt;path.midterm&lt;/code&gt; in the begining of the code below.&lt;/li&gt;
&lt;li&gt;I have used some external packages like ggplot2 to make the plots.&lt;/li&gt;
&lt;li&gt;I have written several functions to analyze each individual field as well as merged field statistics that calculates cohens effect size, required replicates, and does ANOVA analysis on individual field as well as on merged data from all fields.&lt;/li&gt;
&lt;li&gt;This function &lt;code&gt;get_question_1_and_2_answers&lt;/code&gt; calculates the time spent each day on field, start and end time start and end date. observations each day, and also calculate one hour gap (more than an hour gap in consecutive rows) if present in the data. It spits out a table (&lt;code&gt;range_table&lt;/code&gt;) with all the date data required for this project. I then used this range table to plot time, and dates.&lt;/li&gt;
&lt;li&gt;I have also plotted the Moisture content in each harvest field and how it gets affected during the day, and across the range of dates.&lt;/li&gt;
&lt;li&gt;Function &lt;code&gt;calculate_field_mean_SD_and_get_RR_EffSize&lt;/code&gt; also uses mean, sd, and counts from each field to calculate cohens d and Required replicates. It also uses pooled standard deviations and Mean (mean of Means) calculated from all four fields for different control Rates to calculate cohens d and required replicates of combined field data. I have tested this function on all four fields at ControlRate level of 500, 1000, 2000 and 3000 intervals for mid-term, but this time, I have modified this function to calculate cohends d and required replicates for 1000 control rates also taking dates into consideration. This function also calculates these values for date only (neglecting control Rates).&lt;/li&gt;
&lt;li&gt;I have also shown ANOVA analysis followed by TUKEY HSD test to show which ControlRates of seeding have significant effect on Yield for each field and then for all fields (combined all four fields).&lt;/li&gt;
&lt;li&gt;Lastly, I have plotted EffectSize Vs RequiredReplicates for all combined field.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First of all we, will set two directory paths where we have all our files we want to anlalyze. first path is &lt;code&gt;path.final&lt;/code&gt; for final data and &lt;code&gt;path.midterm&lt;/code&gt; for midterm data&lt;/p&gt;
&lt;p&gt;&lt;code&gt;all.files&lt;/code&gt; contains the file name of all csv files in our directory. I have decided to
read all files from the system rather than reading one file at a time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # Path to all .csv data files for final.
# path.final &amp;lt;-
# &amp;quot;/Users/owner1/Box/sdsu/statistical_programming_course/final/&amp;quot;
# 
# # Path to all .csv data files for midterm.
# path.midterm &amp;lt;-
#   &amp;quot;/Users/owner1/Box/sdsu/statistical_programming_course/midterm/&amp;quot;

# instead for local path, we can get files from git repos
library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: xml2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path.final &amp;lt;- read_html(&amp;quot;https://github.com//achalneupane/data&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;.js-navigation-open&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;)

path.midterm &amp;lt;- read_html(&amp;quot;https://github.com//achalneupane/data&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;.js-navigation-open&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we first install and load some of the packages (“multcompView”, “ggplot2”,
“scales”, “data.table”, etc.) we will be using for this exercise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First, install missing packages and load them
myPackages &amp;lt;-
c(
&amp;quot;multcompView&amp;quot;,
&amp;quot;ggplot2&amp;quot;,
&amp;quot;scales&amp;quot;,
&amp;quot;data.table&amp;quot;,
&amp;quot;reshape2&amp;quot;,
&amp;quot;RColorBrewer&amp;quot;,
&amp;quot;plyr&amp;quot;,
&amp;quot;ggpmisc&amp;quot;
)
my.installed.packages &amp;lt;- installed.packages()
available.packages &amp;lt;- myPackages %in% my.installed.packages
if (sum(!available.packages) &amp;gt; 0) {
install.packages(myPackages[!available.packages])
}
# Load all required packages
lapply(myPackages, require, character.only = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: multcompView&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: scales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: data.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: reshape2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;reshape2&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:data.table&amp;#39;:
## 
##     dcast, melt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: RColorBrewer&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: plyr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggpmisc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## News about &amp;#39;ggpmisc&amp;#39; at https://www.r4photobiology.info/&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] TRUE
## 
## [[2]]
## [1] TRUE
## 
## [[3]]
## [1] TRUE
## 
## [[4]]
## [1] TRUE
## 
## [[5]]
## [1] TRUE
## 
## [[6]]
## [1] TRUE
## 
## [[7]]
## [1] TRUE
## 
## [[8]]
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally, I would write all functions in a separate file and use them as
source(‘function_file.r’) However, I have my functions for this project written
here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# field_or_seed &amp;lt;- harvestB


# functions

# function for question 1 and 2
get_question_1_and_2_answers &amp;lt;- function(field_or_seed) {
# range of planting dates
field_or_seed$Date &amp;lt;- gsub(&amp;quot;T.*&amp;quot;, &amp;quot;&amp;quot;, field_or_seed$Timestamp)
field_or_seed$Time &amp;lt;- gsub(&amp;quot;.*T|Z&amp;quot;, &amp;quot;&amp;quot;, field_or_seed$Timestamp)
# Range in days
field_or_seed$date_time &amp;lt;-
as.POSIXct(paste(field_or_seed$Date, field_or_seed$Time), tz = &amp;quot;UTC&amp;quot;)
# field_or_seed$date_time &amp;lt;- paste(field_or_seed$Date, field_or_seed$Time)

# num_days &amp;lt;- round(difftime(max(as.POSIXct(field_or_seed$date_time)), min(as.POSIXct(field_or_seed$date_time))), 3)
num_days &amp;lt;-
round(difftime(max(as.POSIXct(
field_or_seed$date_time
)), min(as.POSIXct(
field_or_seed$date_time
)), units = &amp;quot;days&amp;quot;), 3)

range_from_start_to_end_date_in_days &amp;lt;-
ifelse(num_days &amp;lt;= 1,
paste0(num_days, &amp;quot; day&amp;quot;),
paste0(num_days, &amp;quot; days&amp;quot;))
# Range in span of date for the field
range_of_start_end_date &amp;lt;-
paste0(range(as.Date(field_or_seed$Date))[1], &amp;quot; to &amp;quot;, range(as.Date(field_or_seed$Date))[2])



field_or_seed$Date &amp;lt;- as.factor(field_or_seed$Date)

# time range each day
tt &amp;lt;- field_or_seed[, c(&amp;quot;Date&amp;quot;, &amp;quot;Time&amp;quot;)]




time.range.each.day &amp;lt;- aggregate(data.frame(Time = strptime(do.call(paste, tt), &amp;#39;%F %R:%OS&amp;#39;, tz = &amp;#39;UTC&amp;#39;)),
by = list(Date = tt$Date),
function(Time) {
paste(format(min(Time), &amp;#39;%T&amp;#39;),
format(max(Time), &amp;#39;%T&amp;#39;),
sep = &amp;#39; to &amp;#39;)
})

rm(tt)


# Function to find more than an hour gap in consecutive rows
about_an_hour_gap &amp;lt;- function(field_or_seed) {
one_hour_thing &amp;lt;- {
}
for (i in 1:nrow(field_or_seed)) {
# i=3
if (i == nrow(field_or_seed))
{
break
} else
time.lapse &amp;lt;-
abs(as.numeric(
difftime(field_or_seed$date_time[i + 1], field_or_seed$date_time[i]),
units = &amp;quot;mins&amp;quot;
))

# find consecutive rows where the time is more than or equal 60 minutes and
# dates are the same
if (time.lapse &amp;gt;= 60 &amp;amp;
field_or_seed$Date[i + 1] == field_or_seed$Date[i]) {
one_hour_thing.tmp &amp;lt;- field_or_seed[i:(i + 1), ]
one_hour_thing &amp;lt;- rbind(one_hour_thing, one_hour_thing.tmp)
}
}
return(one_hour_thing)
}
about_an_hour_gap(field_or_seed)
one_hour_gap_time_stamp &amp;lt;- about_an_hour_gap(field_or_seed)

# function to calculate total time spent each day
time.elapsed.each.day.function &amp;lt;- function(x) {
difftime(max(as.POSIXct(x)), min(as.POSIXct(x)), units = &amp;quot;mins&amp;quot;)
}


# time.range.each.day(tt)
time.elapsed.each.day &amp;lt;-
aggregate(date_time ~ Date, data = field_or_seed, FUN = time.elapsed.each.day.function)
colnames(time.elapsed.each.day) &amp;lt;-
c(&amp;quot;Date&amp;quot;, &amp;quot;Time_elapsed (minutes)&amp;quot;)

observations.each.day &amp;lt;- table(field_or_seed$Date)


# Elapsed Dates
dates.elapsed.each.field.function &amp;lt;- function(x){
  difftime(max(as.POSIXct(x)), min(as.POSIXct(x)), units = &amp;quot;days&amp;quot;)
}
elapsed.dates.each.field &amp;lt;- as.integer(dates.elapsed.each.field.function(field_or_seed$Date))

return(
list(
observations.each.day = observations.each.day,
range_from_start_to_end_date_in_days = range_from_start_to_end_date_in_days,
range_of_start_end_date = range_of_start_end_date,
time.elapsed.each.day = time.elapsed.each.day,
elapsed.dates.each.field = elapsed.dates.each.field, 
time.range.each.day = time.range.each.day,
one_hour_gap_time_stamp = one_hour_gap_time_stamp
)
)


}



# Functions to calculate cohen&amp;#39;s d and Required Replicates
cohen.d &amp;lt;- function(m1, s1, m2, s2) {
cohens_d &amp;lt;- (abs(m1 - m2) / sqrt((s1 ^ 2 + s2 ^ 2) / 2))
return(cohens_d)
}

required.replicates &amp;lt;-
function (m1,
s1,
m2,
s2,
alpha = 0.05,
beta = 0.2) {
n &amp;lt;-
2 * ((((sqrt((s1 ^ 2 + s2 ^ 2) / 2
)) / (m1 - m2)) ^ 2) * (qnorm((1 - alpha / 2)) + qnorm((1 - beta))) ^ 2)
return(round(n, 0))
}


# Additional functions: They do anova and also calculate required replicates for fields
# We willl also perform ANOVA analysis with Tukey Test for paired comparision of
# mean for each field data as well as merged data at different ControlRate
# intervals. This function does Tukey HSD test and generates label for
# significant outcomes.

# Create function to get the labels for Tukey HSD:
generate_label_df &amp;lt;- function(TUKEY, variable) {
# Extract labels and factor levels from Tukey post-hoc
Tukey.levels &amp;lt;- TUKEY[[variable]][, 4]
Tukey.labels &amp;lt;-
data.frame(multcompLetters(Tukey.levels)[&amp;#39;Letters&amp;#39;])

#I need to put the labels in the same order as in the boxplot :
Tukey.labels$treatment = rownames(Tukey.labels)
Tukey.labels = Tukey.labels[order(Tukey.labels$treatment) ,]
return(Tukey.labels)
}

# This function does ANOVA and makes boxplots with Tukey statistics for
# comparing Mean yield.
get_my_box_plot &amp;lt;- function (field, plot_name = &amp;quot;the Field&amp;quot;) {
field$CR.Date.Levels &amp;lt;- gsub(&amp;quot;-&amp;quot;, &amp;quot;_&amp;quot;, field$CR.Date.Levels)
field$CR.Date.Levels &amp;lt;- as.factor(field$CR.Date.Levels)
model = lm(field$Yield ~ field$CR.Date.Levels)
ANOVA = aov(model)
# If residual degrees of freedom is less than or equal 1, don&amp;#39;t do Tukey test;
# simply return ANOVA summary.
if (ANOVA$df.residual &amp;lt;= 1) {
return(summary.aov(ANOVA))
}
# Tukey test to study each pair of treatment :
TUKEY &amp;lt;-
TukeyHSD(x = ANOVA,
&amp;#39;field$CR.Date.Levels&amp;#39;,
conf.level = 0.95)

# generate labels using function
labels &amp;lt;- generate_label_df(TUKEY , &amp;quot;field$CR.Date.Levels&amp;quot;)

# rename columns for merging
names(labels) &amp;lt;- c(&amp;#39;Letters&amp;#39;, &amp;#39;CR.Date.Levels&amp;#39;)

# Obtain letter positions for y axis using means
yvalue &amp;lt;- aggregate(Yield ~ CR.Date.Levels, data = field, mean)

final &amp;lt;- merge(labels, yvalue) #merge dataframes

p &amp;lt;- ggplot(field, aes(x = CR.Date.Levels, y = Yield)) +
geom_blank() +
theme_bw() +
# theme(panel.grid.major = element_blank(),
# panel.grid.minor = element_blank()) +
labs(x = &amp;#39;CR.Date.Levels&amp;#39;, y = &amp;#39;Mean Yield&amp;#39;) +
ggtitle(paste0(&amp;quot;CR.Date.Levels Vs Mean yield for &amp;quot;, plot_name),
expression(atop(italic(
&amp;quot;(Anova:TukeyHSD)&amp;quot;
), &amp;quot;&amp;quot;))) +
# ggtitle(paste0(&amp;quot;CR.Date.Levels Vs Mean yield for &amp;quot;, plot_name)) +
theme(plot.title = element_text(hjust = 0.5, face = &amp;#39;bold&amp;#39;)) +
geom_boxplot(fill = &amp;#39;grey&amp;#39;, stat = &amp;quot;boxplot&amp;quot;) +
# coord_cartesian(clip = &amp;#39;off&amp;#39;) +
geom_text(
data = final,
aes(x = CR.Date.Levels, y = Yield, label = Letters),
vjust = -3.5,
hjust = -.5
) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
# geom_vline(aes(xintercept = 4.5), linetype = &amp;quot;dashed&amp;quot;) +
theme(plot.title = element_text(vjust = -0.6)) +
coord_cartesian(ylim = c(min(field$Yield), max(field$Yield) + 10))
print(p)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Function to calculate mean and sd for each field for a given interval of
ControlRate and then calculate Required Replicates and EffectSize. I have made
one function to do all that so I can just use this function to analyze all
four fields. To analyze single field set &lt;code&gt;Single.Field.Analysis = TRUE&lt;/code&gt;, and
to analyze all four field together, set &lt;code&gt;Single.Field.Analysis = FALSE&lt;/code&gt;,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calculate_field_mean_SD_and_get_RR_EffSize &amp;lt;- 
  function(
    field, intervals = 1000, Single.Field.Analysis = TRUE, Date.Only = FALSE, printplot = FALSE){
    options(warn=1)
    plot_NAME &amp;lt;- (deparse(substitute(field)))
    if(Single.Field.Analysis &amp;gt; 0){
      # removing rows with NAs in Yield
      sum(is.na(field$Yield))
      # field &amp;lt;- field[!complete.cases(field$Yield), ]
      field &amp;lt;- field[!is.na(field$Yield),]
      # as.factor(fieldA$ControlRate)
      
      
      # work on timestamp columns
      field$Date &amp;lt;- gsub(&amp;quot;T.*&amp;quot;, &amp;quot;&amp;quot;, field$Timestamp)
      field$Time &amp;lt;- gsub(&amp;quot;.*T|Z&amp;quot;, &amp;quot;&amp;quot;, field$Timestamp)
      # Range in days
      field$date_time &amp;lt;- as.POSIXct(paste(field$Date, field$Time), tz = &amp;quot;UTC&amp;quot;)
      
      
      # table.field &amp;lt;- table(as.factor(field$Date))
      field$ControlRate.Levels &amp;lt;- as.factor(intervals * ceiling(field$ControlRate/intervals))
      if(Date.Only &amp;lt; 1){
      field$CR.Date.Levels &amp;lt;- as.factor(paste(field$ControlRate.Levels, field$Date, sep = &amp;quot;*&amp;quot;))
      } else {
        field$CR.Date.Levels &amp;lt;- as.factor(field$Date)
      }
      table.field &amp;lt;- table(as.factor(field$CR.Date.Levels))
      
      
      
      
      field.Count &amp;lt;- setNames(aggregate(field$CR.Date.Levels, 
                                        by = list(field$CR.Date.Levels), FUN = length), c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;Count&amp;quot;))
      # # Degree of freedome = n * k - k
      # field.Count$degree.Freedom &amp;lt;- 
      #   (field.Count$Count * length(field.Count$ControlRate)) -length(field.Count$ControlRate)
      
      field.mean &amp;lt;-
        setNames(aggregate(
          field$Yield,
          by = list(field$CR.Date.Levels),
          FUN = mean
        ),
        c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;Mean&amp;quot;))
      
      
      field.SD &amp;lt;-
        setNames(aggregate(
          field$Yield,
          by = list(field$CR.Date.Levels),
          FUN = sd
        ),
        c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;SD&amp;quot;))
      
      
      if(printplot == 1 ){
        get_my_box_plot(field, plot_name = plot_NAME)
      }
      
      # plot individual fields with tukey test We will print box plot only if we
      # want for certain ControlRates intervals. Otherwise we will have too many
      # plots
    } else {
      temp.Field &amp;lt;- field
      colnames(temp.Field)[colnames(temp.Field) == &amp;quot;CR.Date.Levels&amp;quot;] &amp;lt;- &amp;quot;ControlRate.Levels&amp;quot;
      colnames(temp.Field)[colnames(temp.Field) == &amp;quot;Mean&amp;quot;] &amp;lt;- &amp;quot;Yield&amp;quot;
      # get_my_box_plot(temp.Field)
      field.SD  &amp;lt;- 
        as.data.frame(cbind(ControlRate = field[&amp;quot;CR.Date.Levels&amp;quot;], SD = field[&amp;quot;SD_pooled&amp;quot;]))
      field.mean &amp;lt;- 
        as.data.frame(cbind(ControlRate = field[&amp;quot;CR.Date.Levels&amp;quot;], Mean = field[&amp;quot;Mean&amp;quot;]))
    }
    
    # Calculate Required replicate and Effect Size from each
    # field for ControlRate i vs i+1
    
    # ReqRep_EffectSize_table &amp;lt;- function (field.mean, field.SD){
    Req.Rep.table.field &amp;lt;- {}
    for (i in 1:nrow(field.SD)){
      if(i+1 &amp;gt; nrow(field.SD) ){
        break
      }
      temp.Effect.size &amp;lt;-
        cohen.d(
          m1 = field.mean$Mean[i],
          s1 = field.SD$SD[i],
          m2 = field.mean$Mean[i + 1],
          s2 = field.SD$SD[i + 1]
        )
      
      tmp.req.reps &amp;lt;-
        required.replicates(
          m1 = field.mean$Mean[i],
          s1 = field.SD$SD[i],
          m2 = field.mean$Mean[i + 1],
          s2 = field.SD$SD[i + 1]
        )
      
      tmp.table &amp;lt;-
        cbind(
          Group = paste0(field.SD$CR.Date.Levels[i], &amp;quot; Vs &amp;quot;, field.SD$CR.Date.Levels[i + 1]),
          EffectSize = temp.Effect.size,
          RequiredReplicates = tmp.req.reps
        )
      
      Req.Rep.table.field &amp;lt;- rbind(Req.Rep.table.field, tmp.table)
    }
    if (Single.Field.Analysis &amp;gt; 0) {
      return(
        list(
          field.mean = field.mean,
          fieldSD = field.SD,
          field.Count = field.Count,
          Req.Rep.table.field = Req.Rep.table.field
        )
      )
    } else{
      return(list(Req.Rep.table.field = Req.Rep.table.field))
    }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I am reading all files in my directory we set in the begining
(path.final). I was being a bit lazy to copy hard coded paths. I then read those
files in for loop and assigned them to their respective variable names [eg,
gsub(“.csv”,&amp;quot;&amp;quot;,filename.csv)].&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# all.files contains the file name of all csv files in our directory. I have decided to
# read all files from the system rather than reading one file at a time.

# setwd(path.final)
# if using local path
# all.files &amp;lt;- list.files(path.final)

# using from github
all.files &amp;lt;- path.final
all.files &amp;lt;- all.files[grepl(&amp;quot;*.csv&amp;quot;, all.files)]

# # selecting only required files


all.files &amp;lt;- all.files[grepl(&amp;quot;seedA.csv|seedB.csv|seedC.csv|seedD.csv|harvestA.csv|harvestB.csv|harvestC.csv|harvestD.csv&amp;quot;, all.files)]

# # Or choose specifically which data files in path to analyze
# all.files &amp;lt;- all.files &amp;lt;- c(&amp;quot;seedA.csv&amp;quot;, &amp;quot;seedB.csv&amp;quot;, &amp;quot;seedC.csv&amp;quot;, &amp;quot;seedD.csv&amp;quot;)

# Rearranging the vectors, putting seed data first and then harvest data
all.files &amp;lt;-
c(all.files[grepl(&amp;quot;seed&amp;quot;, all.files)], all.files[grepl(&amp;quot;harvest&amp;quot;, all.files)])
# all.files
# [1] &amp;quot;harvestA.csv&amp;quot; &amp;quot;harvestB.csv&amp;quot; &amp;quot;harvestC.csv&amp;quot; &amp;quot;harvestD.csv&amp;quot; &amp;quot;seedA.csv&amp;quot;    &amp;quot;seedB.csv&amp;quot;    &amp;quot;seedC.csv&amp;quot;    &amp;quot;seedD.csv&amp;quot;

#https://raw.githubusercontent.com/achalneupane/data/master/
# reading all csv files within the path and saving them to their respective names.
for (i in 1:length(all.files)) {
assign(
gsub(&amp;quot;.*master/|.csv&amp;quot;, &amp;quot;&amp;quot;, all.files)[i],
read.table(gsub(&amp;quot;blob/&amp;quot;, &amp;quot;&amp;quot;,paste0(&amp;quot;https://raw.githubusercontent.com&amp;quot;,all.files[i])),
header = TRUE,
sep = &amp;quot;,&amp;quot;)
)
print(paste0(&amp;quot;Read file &amp;quot;, all.files[i], &amp;#39; !&amp;#39;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Read file /achalneupane/data/blob/master/seedA.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/seedB.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/seedC.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/seedD.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/harvestA.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/harvestB.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/harvestC.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/harvestD.csv !&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# head(harvestA)
# head(harvestB) # and so on..&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I am using &lt;code&gt;get_question_1_and_2_answers&lt;/code&gt; to extract date information by
looping over all data files we just read above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make a table of date range
range_table &amp;lt;- {}
one.hour.time.difference &amp;lt;- data.frame()
for (i in 1:length(all.files)) {
# file.dat &amp;lt;- eval(sub(&amp;quot;.csv&amp;quot;, &amp;quot;&amp;quot;, all.files, fixed = TRUE)[i])
file.dat &amp;lt;- eval(gsub(&amp;quot;.*master/|.csv&amp;quot;, &amp;quot;&amp;quot;, all.files)[i])
# print(paste0(&amp;quot;Doing time thing for &amp;quot;, file.dat, &amp;quot;...&amp;quot;))
dat &amp;lt;- get_question_1_and_2_answers(eval(parse(text = file.dat)))
tmp.range.of.days &amp;lt;-
cbind(
data = file.dat,
range_from_start_to_end_date_in_days = dat$range_from_start_to_end_date_in_days,
elapsed.dates.each.field = dat$elapsed.dates.each.field,
range_of_start_end_date = dat$range_of_start_end_date,
time.range.each.day = dat$time.range.each.day,
time.elapsed.each.day = dat$time.elapsed.each.day
)
range_table &amp;lt;- rbind(range_table, tmp.range.of.days)

tmp.one.hour.time.difference &amp;lt;- dat$one_hour_gap_time_stamp
tmp.one.hour.time.difference$field &amp;lt;- file.dat

if (dim(as.data.frame(tmp.one.hour.time.difference))[1] &amp;gt;= 2) {
one.hour.time.difference &amp;lt;-
rbind.fill(one.hour.time.difference, tmp.one.hour.time.difference)
} else{
one.hour.time.difference &amp;lt;- one.hour.time.difference
}


}

range_table &amp;lt;- as.data.frame(range_table)
range_table$days_number &amp;lt;-
as.numeric(gsub(
&amp;#39; days|day&amp;#39;,
&amp;quot;&amp;quot;,
range_table$range_from_start_to_end_date_in_days
))

# Range table
range_table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        data range_from_start_to_end_date_in_days elapsed.dates.each.field
## 1     seedA                            0.117 day                        0
## 2     seedB                            0.352 day                        1
## 3     seedB                            0.352 day                        1
## 4     seedC                             0.17 day                        1
## 5     seedC                             0.17 day                        1
## 6     seedD                             0.22 day                        0
## 7  harvestA                          18.824 days                       19
## 8  harvestA                          18.824 days                       19
## 9  harvestA                          18.824 days                       19
## 10 harvestB                            0.363 day                        0
## 11 harvestC                           1.085 days                        1
## 12 harvestC                           1.085 days                        1
## 13 harvestD                          43.214 days                       43
## 14 harvestD                          43.214 days                       43
## 15 harvestD                          43.214 days                       43
##     range_of_start_end_date time.range.each.day.Date time.range.each.day.Time
## 1  2018-05-17 to 2018-05-17               2018-05-17     15:24:19 to 18:13:06
## 2  2018-05-20 to 2018-05-21               2018-05-20     18:50:11 to 21:53:11
## 3  2018-05-20 to 2018-05-21               2018-05-21     02:41:49 to 03:16:59
## 4  2018-05-18 to 2018-05-19               2018-05-18     22:24:15 to 23:59:59
## 5  2018-05-18 to 2018-05-19               2018-05-19     00:00:00 to 02:29:27
## 6  2018-05-20 to 2018-05-20               2018-05-20     13:20:08 to 18:36:20
## 7  2018-10-27 to 2018-11-15               2018-10-27     21:07:30 to 23:06:33
## 8  2018-10-27 to 2018-11-15               2018-10-28     16:22:46 to 22:13:25
## 9  2018-10-27 to 2018-11-15               2018-11-15     16:31:55 to 16:53:40
## 10 2018-11-02 to 2018-11-02               2018-11-02     15:01:21 to 23:43:43
## 11 2018-11-11 to 2018-11-12               2018-11-11     15:34:34 to 23:59:59
## 12 2018-11-11 to 2018-11-12               2018-11-12     00:00:00 to 17:37:01
## 13 2018-10-01 to 2018-11-13               2018-10-01     17:04:25 to 17:32:04
## 14 2018-10-01 to 2018-11-13               2018-11-12     17:42:25 to 23:55:58
## 15 2018-10-01 to 2018-11-13               2018-11-13     15:10:12 to 22:11:56
##    time.elapsed.each.day.Date time.elapsed.each.day.Time_elapsed (minutes)
## 1                  2018-05-17                                   168.78665 
## 2                  2018-05-20                                   183.00582 
## 3                  2018-05-21                                    35.17333 
## 4                  2018-05-18                                    95.73995 
## 5                  2018-05-19                                   149.45087 
## 6                  2018-05-20                                   316.21000 
## 7                  2018-10-27                                   119.06642 
## 8                  2018-10-28                                   350.63592 
## 9                  2018-11-15                                    21.75020 
## 10                 2018-11-02                                   522.35150 
## 11                 2018-11-11                                   505.40003 
## 12                 2018-11-12                                  1057.01712 
## 13                 2018-10-01                                    27.65013 
## 14                 2018-11-12                                   373.55237 
## 15                 2018-11-13                                   421.71987 
##    days_number
## 1        0.117
## 2        0.352
## 3        0.352
## 4        0.170
## 5        0.170
## 6        0.220
## 7       18.824
## 8       18.824
## 9       18.824
## 10       0.363
## 11       1.085
## 12       1.085
## 13      43.214
## 14      43.214
## 15      43.214&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the range table for all fields. I have calculated the range for total
elapsed time (in terms of days) in &lt;code&gt;range_from_start_to_end_date_in_days&lt;/code&gt;
column, total dates elapsed in &lt;code&gt;elapsed.dates.each.field&lt;/code&gt; column, Range in dates
in &lt;code&gt;range_of_start_end_date&lt;/code&gt; column, time range each day (from start to end
time) in &lt;code&gt;time.range.each.day.Time&lt;/code&gt; column, total time spent in the field each
date in &lt;code&gt;time.elapsed.each.day.Time_elapsed (minutes)&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is the list of consecutive rows with more than one hour time gap from all files:
one.hour.time.difference&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       X                Timestamp      LonM      LatM Moisture DISTANCE
## 1 27840 2018-11-12T01:51:27.071Z 208.29634 396.43056    15.33 1.075304
## 2 27849 2018-11-12T16:42:23.002Z  82.56309  62.87237    15.60 5.872164
##   VRYIELDVOL       Date         Time           date_time    field
## 1   133.3961 2018-11-12 01:51:27.071 2018-11-12 01:51:27 harvestC
## 2   119.6537 2018-11-12 16:42:23.002 2018-11-12 16:42:23 harvestC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I was able to find more than an hour gap in consecutive rows in &lt;code&gt;harvestC&lt;/code&gt; data
only&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plotting the total number of days for each field type
ggplot(range_table, aes(x = data, y = days_number)) + geom_point(size = 4) +
  ggtitle(&amp;quot;Plot in terms of elapsed time- \n start to end (in days)&amp;quot;) +
theme_bw() +
theme(
axis.line = element_line(colour = &amp;quot;black&amp;quot;),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
theme(text = element_text(size = 14)) +
scale_y_continuous(breaks = round(c(
1,
max(range_table$days_number) * 1 / 3,
max(range_table$days_number) * 2 / 3,
max(range_table$days_number)
), 2)) +
# to check if any of the data were planted/harvested within one day range
geom_hline(yintercept = 1) +
geom_text((aes(3, 1, label = &amp;quot;Within 1 day (24 hours margin)&amp;quot;, vjust = -1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this plot, we can see that SeedA, seedB, seedC and seedD were all
planted within 24 hours span. We can also check the &lt;code&gt;range_table&lt;/code&gt; above to get
the exact timeframe for this. However, if want to see if they were planted or
harvested on the same date, then we can plot this figure below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If we want to see whether they were planted or harvested the same day (i.e
# Date) and (not necessarily 24 hours margin), we choose range_table$elapsed.dates.each.field


# Now plot them
ggplot(range_table, aes(x = data, y = elapsed.dates.each.field)) +
  geom_point(size = 2) +
  theme_bw() +
  ggtitle(&amp;quot;Plot in terms of elapsed dates&amp;quot;) +
  theme(axis.line = element_line(colour = &amp;quot;black&amp;quot;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  theme(text = element_text(size=14)) +
# to check if any of the data were planted/harvested the same &amp;quot;Date&amp;quot;
geom_hline(yintercept = 1) +
  geom_hline(yintercept = 0, color=&amp;quot;blue&amp;quot;, linetype=&amp;quot;dashed&amp;quot;) +
geom_text((aes(3, 1, label = &amp;quot;Margin outside the \&amp;quot;same\&amp;quot; date&amp;quot;, vjust = -1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot above shows that seedA and seedD were planted on the same date, and
harvestB was harvested also on the same date. Rest of the field data were
harvested/planted for multiple dates. Bubbles within the black solid line
indicates that the fields were, harvested/planted the same date. Only seedA,
seedD and harvestB are within the solid line. Here, Y axis margin of 1 means
next date. Data within the black and dashed blue lines indicate the same date.&lt;/p&gt;
&lt;p&gt;Additionally, if we want to see how many hours were spent on each field in
total, we can plot this below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.new &amp;lt;- range_table[, c(&amp;quot;data&amp;quot;, &amp;quot;time.elapsed.each.day.Time_elapsed (minutes)&amp;quot;, &amp;quot;time.elapsed.each.day.Date&amp;quot;)]
plot.new$time.elapsed.each.day.Time_elapsed &amp;lt;- as.numeric(plot.new$`time.elapsed.each.day.Time_elapsed (minutes)`)
plot.new$time.elapsed.each.day.Time_elapsed.in.hour &amp;lt;- plot.new$time.elapsed.each.day.Time_elapsed/60
plot.new.data &amp;lt;- setNames(aggregate(plot.new$time.elapsed.each.day.Time_elapsed.in.hour, list(range_table$data), sum), c(&amp;quot;group&amp;quot;, &amp;quot;time.spent.in.hours&amp;quot;))

ggplot(plot.new.data, aes(x = group, y = time.spent.in.hours)) +
  geom_point(size = 4) +
  theme_bw() +
  ggtitle(&amp;quot;Plot in terms of total time spent in hours&amp;quot;) +
  theme(axis.line = element_line(colour = &amp;quot;black&amp;quot;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  theme(text = element_text(size=14)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this above plot, it looks like only harvestC and harvestD had more than
10 hours spent on them.&lt;/p&gt;
&lt;p&gt;Now, we can also plot start and end time for everyday record:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;range_table$start_date_time &amp;lt;-
as.POSIXct(paste(
range_table$time.range.each.day.Date ,
gsub(&amp;quot;to.*| &amp;quot;, &amp;quot;&amp;quot;, range_table$time.range.each.day.Time)
), tz = &amp;#39;UTC&amp;#39;)
range_table$end_date_time &amp;lt;-
as.POSIXct(paste(
range_table$time.range.each.day.Date ,
gsub(&amp;quot;.*to| &amp;quot;, &amp;quot;&amp;quot;, range_table$time.range.each.day.Time)
), tz = &amp;#39;UTC&amp;#39;)


tt &amp;lt;- range_table[, c(&amp;quot;data&amp;quot;, &amp;quot;start_date_time&amp;quot;, &amp;quot;end_date_time&amp;quot;)]

tt &amp;lt;- melt(tt, id = &amp;quot;data&amp;quot;)



# Plot for date and time

ggplot(tt, aes(
x = data,
y = as.character.Date(value),
colour = variable
)) +
geom_point(size = 4) +
ylab(&amp;quot;Dates and Time&amp;quot;) +
theme_bw() +
ggtitle(&amp;quot;Start and end time for everyday record&amp;quot;) +
theme(
axis.line = element_line(colour = &amp;quot;black&amp;quot;),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 14)) +
theme(axis.text.y = element_text(angle = 0, hjust = 1, size = 8)) +
# theme(text = element_text(size = 14)) +
geom_hline(yintercept = 1:length(tt$value), linetype = &amp;quot;dotted&amp;quot;) +
labs(color = &amp;#39;Each Day&amp;#39;) +
scale_color_manual(labels = c(&amp;quot;START_time&amp;quot;, &amp;quot;END_time&amp;quot;),
values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
This plot also shows similar result as with previous plot, but with both start
and end time.&lt;/p&gt;
&lt;p&gt;We can also check the moisture level differences in the fields by date and
times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now, let&amp;#39;s work on the effect of time/dates on moiture level
tt1 &amp;lt;- harvestA
tt2 &amp;lt;- harvestB
tt3 &amp;lt;- harvestC
tt4 &amp;lt;- harvestD

tt1$type &amp;lt;- &amp;quot;harvestA&amp;quot;
tt2$type &amp;lt;- &amp;quot;harvestB&amp;quot;
tt3$type &amp;lt;- &amp;quot;harvestC&amp;quot;
tt4$type &amp;lt;- &amp;quot;harvestD&amp;quot;

make_posixct &amp;lt;- function(x) {
  x$Date &amp;lt;- gsub(&amp;quot;T.*&amp;quot;, &amp;quot;&amp;quot;, x$Timestamp)
  x$Date &amp;lt;- as.factor(x$Date)
  x$Time &amp;lt;- gsub(&amp;quot;.*T|Z&amp;quot;, &amp;quot;&amp;quot;, x$Timestamp)
  n &amp;lt;- 2
  pat &amp;lt;- paste0(&amp;#39;^([^:]+(?::[^:]+){&amp;#39;, n - 1, &amp;#39;}).*&amp;#39;)
  x$Time  &amp;lt;- as.factor(sub(pat, &amp;#39;\\1&amp;#39;, x$Time))
  return(x)
}

tt1 &amp;lt;- make_posixct(tt1)
tt2 &amp;lt;- make_posixct(tt2)
tt3 &amp;lt;- make_posixct(tt3)
tt4 &amp;lt;- make_posixct(tt4)
colnames(tt2) &amp;lt;- colnames(tt1)


df &amp;lt;- rbind(tt1, tt2, tt3, tt4)

df$Time &amp;lt;-
  as.POSIXct(strptime(df$Time, format = &amp;quot;%H:%M&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
lims &amp;lt;-
  as.POSIXct(strptime(c(&amp;quot;0:00&amp;quot;, &amp;quot;23:59&amp;quot;), format = &amp;quot;%H:%M&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
p &amp;lt;-
  ggplot(data = df, aes(
    x = Time,
    y = Moisture,
    colour = as.factor(Date)
  )) +
  geom_point() +
  scale_x_datetime(
    limits = lims,
    breaks = date_breaks(&amp;quot;2 hour&amp;quot;),
    labels = date_format(&amp;quot;%H:%M&amp;quot;, tz = &amp;quot;UTC&amp;quot;)
  ) +
  labs(color = &amp;#39;Dates&amp;#39;) +
  facet_grid(. ~ type)
p + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;
Based on this figure above, we can see that the moisture content is very low
from midnight to 2:00pm, but then after, it increases substantially. I don’t
think there is so much difference in moisture in terms of harvest dates though.&lt;/p&gt;
&lt;p&gt;I tried to pull weather data for each observation, however the api I used only
allows 1000 requests per day, so I did not include that data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can use darksky like api&amp;#39;s to pull weather data, however it&amp;#39;s not
# possible as we have so many observations. Only allows 1000 downloads per day.

# install.packages(&amp;quot;darksky&amp;quot;)
library(darksky) darksky_api_key(force = TRUE)
#API: 60b8ca6aaece606b21c0dfec1cb7ec9f
head(seedA)
# field_or_seed &amp;lt;- seedA 
tmp &amp;lt;- get_forecast_for(10.25209, 478.4000, &amp;quot;2013-05-06T12:00:00&amp;quot;)

# Create empty columns and cbind to the field data so you can add new weather
data field_or_seed &amp;lt;- cbind(field_or_seed, setNames(lapply(get.weather.cols,
function(x) x = NA), get.weather.cols)) 
field_or_seed$Date &amp;lt;- gsub(&amp;quot;T.*&amp;quot;, &amp;quot;&amp;quot;, field_or_seed$Timestamp)
field_or_seed$Time &amp;lt;- gsub(&amp;quot;.*T|Z&amp;quot;, &amp;quot;&amp;quot;, field_or_seed$Timestamp) # Range in days field_or_seed$date_time &amp;lt;-
as.POSIXct(paste(field_or_seed$Date, field_or_seed$Time), tz = &amp;quot;UTC&amp;quot;)

field_or_seed_with_weather &amp;lt;- {}
for (i in 1:nrow(field_or_seed)) {
print(paste0(&amp;quot;My iteration is: &amp;quot;, i)) tmp.min &amp;lt;-
get_forecast_for(44.311356, -96.798386, min(field_or_seed$date_time[i]))
tmp.weather.data &amp;lt;- cbind(field_or_seed[i,], tmp.min$daily)
field_or_seed_with_weather &amp;lt;- rbind(field_or_seed_with_weather,
tmp.weather.data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additional work:&lt;/p&gt;
&lt;p&gt;Let’s read our csv files from midterm&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # If using local path
#   setwd(path.midterm)
#   all.files &amp;lt;- list.files(path.midterm)
  # here, instead, I am just using the URL
  all.files &amp;lt;- path.midterm
  all.files &amp;lt;- all.files[grepl(&amp;quot;*.csv&amp;quot;, all.files)]
  # &amp;gt; all.files
  # [1] &amp;quot;fieldA.csv&amp;quot; &amp;quot;fieldB.csv&amp;quot; &amp;quot;fieldC.csv&amp;quot; &amp;quot;fieldD.csv&amp;quot;
  

all.files &amp;lt;- all.files[grepl(&amp;quot;fieldA.csv|fieldB.csv|fieldC.csv|fieldD.csv&amp;quot;, all.files)]  
  
  
# reading all csv files within the path and saving them with their respective
# names
# https://raw.githubusercontent.com/achalneupane/data/master/
# reading all csv files within the path and saving them to their respective names.
for (i in 1:length(all.files)) {
assign(
gsub(&amp;quot;.*master/|.csv&amp;quot;, &amp;quot;&amp;quot;, all.files)[i],
read.table(gsub(&amp;quot;blob/&amp;quot;, &amp;quot;&amp;quot;,paste0(&amp;quot;https://raw.githubusercontent.com&amp;quot;,all.files[i])),
header = TRUE,
sep = &amp;quot;,&amp;quot;)
)
print(paste0(&amp;quot;Read file &amp;quot;, all.files[i], &amp;#39; !&amp;#39;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Read file /achalneupane/data/blob/master/fieldA.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/fieldB.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/fieldC.csv !&amp;quot;
## [1] &amp;quot;Read file /achalneupane/data/blob/master/fieldD.csv !&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Now, let&amp;#39;s merge field and seed data
  
  # head(fieldA)
  # head(seedA)
  
  dataA &amp;lt;-
  merge(
  fieldA,
  seedA,
  by.x = c(&amp;quot;Easting&amp;quot;, &amp;quot;Northing&amp;quot;),
  by.y = c(&amp;quot;LonM&amp;quot;, &amp;quot;LatM&amp;quot;)
  )
  colnames(dataA) &amp;lt;- gsub(&amp;quot;.x&amp;quot;, &amp;quot;&amp;quot;, colnames(dataA))
  dataB &amp;lt;-
  merge(
  fieldB,
  seedB,
  by.x = c(&amp;quot;Easting&amp;quot;, &amp;quot;Northing&amp;quot;),
  by.y = c(&amp;quot;LonM&amp;quot;, &amp;quot;LatM&amp;quot;)
  )
  colnames(dataB) &amp;lt;- gsub(&amp;quot;.x&amp;quot;, &amp;quot;&amp;quot;, colnames(dataB))
  dataC &amp;lt;-
  merge(
  fieldC,
  seedC,
  by.x = c(&amp;quot;Easting&amp;quot;, &amp;quot;Northing&amp;quot;),
  by.y = c(&amp;quot;LonM&amp;quot;, &amp;quot;LatM&amp;quot;)
  )
  colnames(dataC) &amp;lt;- gsub(&amp;quot;.x&amp;quot;, &amp;quot;&amp;quot;, colnames(dataC))
  dataD &amp;lt;-
  merge(
  fieldD,
  seedD,
  by.x = c(&amp;quot;Easting&amp;quot;, &amp;quot;Northing&amp;quot;),
  by.y = c(&amp;quot;LonM&amp;quot;, &amp;quot;LatM&amp;quot;)
  )
  colnames(dataD) &amp;lt;- gsub(&amp;quot;.x&amp;quot;, &amp;quot;&amp;quot;, colnames(dataD))
  
  # Taking 1000 ControlRates interval, we will also compare different dates (i.e,
  # ControlRate and date pairs) for each merged seed and field data.
  
  ControlRateInterval &amp;lt;- 1000
  # field A
  fieldA.data &amp;lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataA, intervals = ControlRateInterval)
  # fieldA.data
  
  
  # field B
  fieldB.data &amp;lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataB, intervals = ControlRateInterval)
  
  # fieldB.data
  
  # From midterm for fieldB, we saw significant difference between all Control Rates of
  # Seeding except between 28000 Vs 29000. The seeding rates of 28000 has the
  # highest Mean yield here.
  
  # field C
  fieldC.data &amp;lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataC, intervals = ControlRateInterval)
  # fieldC.data
  
  # From midterm for fieldC, we saw all Control rates of seeding are significantly diffferent with the highest Mean yeild for 28000.
  
  # field D
  fieldD.data &amp;lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataD, intervals = ControlRateInterval)
 # fieldD.data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on my analysis from midterm, we saw that it is better to compare all
four fields together. Also, I do not want to clutter this report with too many
figures and tables, I am analyzing 1000 Control Rate and all four fields
merged together. This way we can compare the skewed data points if we analyze
with date and ControlRate pairs. I have done this analysis below&lt;/p&gt;
&lt;p&gt;We can now merge all four fields for SD, means and ControlRate level counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  merged.SD.4.plots &amp;lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$fieldSD,
  fieldB.data$fieldSD,
  fieldC.data$fieldSD,
  fieldD.data$fieldSD
  )
  )
  
  
  merged.Mean.4.plots &amp;lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.mean,
  fieldB.data$field.mean,
  fieldC.data$field.mean,
  fieldD.data$field.mean
  )
  )
  
  
  merged.Count.4.plots &amp;lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.Count,
  fieldB.data$field.Count,
  fieldC.data$field.Count,
  fieldD.data$field.Count
  )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need SD pooled for these levels so we can calculate Cohen’s d and Required
Replicates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  levels(merged.SD.4.plots$CR.Date.Levels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;23000*2018-05-17&amp;quot; &amp;quot;24000*2018-05-17&amp;quot; &amp;quot;25000*2018-05-17&amp;quot; &amp;quot;26000*2018-05-17&amp;quot;
##  [5] &amp;quot;27000*2018-05-17&amp;quot; &amp;quot;28000*2018-05-17&amp;quot; &amp;quot;29000*2018-05-17&amp;quot; &amp;quot;24000*2018-05-20&amp;quot;
##  [9] &amp;quot;25000*2018-05-20&amp;quot; &amp;quot;25000*2018-05-21&amp;quot; &amp;quot;26000*2018-05-20&amp;quot; &amp;quot;26000*2018-05-21&amp;quot;
## [13] &amp;quot;27000*2018-05-20&amp;quot; &amp;quot;27000*2018-05-21&amp;quot; &amp;quot;28000*2018-05-20&amp;quot; &amp;quot;28000*2018-05-21&amp;quot;
## [17] &amp;quot;29000*2018-05-20&amp;quot; &amp;quot;29000*2018-05-21&amp;quot; &amp;quot;23000*2018-05-18&amp;quot; &amp;quot;24000*2018-05-18&amp;quot;
## [21] &amp;quot;25000*2018-05-18&amp;quot; &amp;quot;26000*2018-05-18&amp;quot; &amp;quot;26000*2018-05-19&amp;quot; &amp;quot;27000*2018-05-18&amp;quot;
## [25] &amp;quot;27000*2018-05-19&amp;quot; &amp;quot;28000*2018-05-18&amp;quot; &amp;quot;28000*2018-05-19&amp;quot; &amp;quot;23000*2018-05-20&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # First, we merged all merged.SD.4.plots, merged.Mean.4.plots and
  # merged.Count.4.plots from all fields.
  # Therefore, now merging all three dataframes from all four plots for SD, Mean
  # and counts by ControlRate column
  
  Mean_SD_Count.dat &amp;lt;- Reduce(function(...)
  merge(..., by = c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;grp&amp;quot;), all.x = TRUE),
  lapply(
  list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots),
  transform,
  grp = ave(seq_along(CR.Date.Levels), CR.Date.Levels, FUN = seq_along)
  ))
  # head(Mean_SD_Count.dat)
  
  # Now, we can also do Anova on the merged data `Mean_SD_Count.dat`
  
  Mean_SD_Count_merged_for_all_four_plots &amp;lt;- Mean_SD_Count.dat
  Mean_SD_Count_merged_for_all_four_plots$Yield &amp;lt;-
  Mean_SD_Count_merged_for_all_four_plots$Mean
  Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels &amp;lt;-
  Mean_SD_Count_merged_for_all_four_plots$ControlRate
  
  get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = &amp;quot;all four fields&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this plot above, from all fields, we can tell that, Control Rates of
seeding of 23000-25000 has significantly less Mean yield as compared to
26000-29000. It looks like Control Rates of Seeding between 27000 and 28000
has the highest Mean yield. I also think that there is no effect on seeding
dates, I have done some analysis to show the seeding effect only in the last
part of this report and based on that as well, I do not think there is
significant effect of Dates on Yield.&lt;/p&gt;
&lt;p&gt;We now calculate pooled SD for merged 4 plots &lt;code&gt;Mean_SD_Count.dat&lt;/code&gt; to calculate
Effect Size and Required Replicates for all fields (combined analysis) using
pooled SD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pooled.dat &amp;lt;- Mean_SD_Count.dat


# # Pooled sd can be calculated as:
pooled.dat$df &amp;lt;- pooled.dat$Count - 1
# pooled SD is :
# pooledSD &amp;lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) )

# We can calculate our SD pooled using this formula:
#  s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
# We will derrive this in steps as below:

pooled.dat$df &amp;lt;- pooled.dat$Count - 1
pooled.dat$sd.square &amp;lt;- pooled.dat$SD ^ 2

pooled.dat$ss &amp;lt;- pooled.dat$sd.square * pooled.dat$df


# We can use convenience function (aggregate) for splitting and calculating
# the necessary sums.
ds &amp;lt;- aggregate(ss ~ CR.Date.Levels, data = pooled.dat, sum)

# Two different built-in methods for split apply, we could use aggregate for
# both if we wanted. This calculates our degrees of freedom.
ds$df &amp;lt;- tapply(pooled.dat$df, pooled.dat$CR.Date.Levels, sum)
# divide ss by df and then we get sd square
ds$sd.square &amp;lt;- ds$ss / ds$df
# Finally, we can get our sd pooled
ds$SD_pooled &amp;lt;- sqrt(ds$sd.square)
# ds

# However, we could also calculate our sd_pooled as below and get the same results :
# sd_pooled &amp;lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$CR.Date.Levels),
# function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) )

# Now, we calculate Mean (Mean of Means) from the merged table
# `Mean_SD_Count.dat so we can calculate Cohens d and RequiredReplicates for all
# four field combined.

ds.Mean &amp;lt;-
setNames(aggregate(
Mean_SD_Count.dat$Mean,
by = list(Mean_SD_Count.dat$CR.Date.Levels),
FUN = mean
),
c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;Mean&amp;quot;))

ds &amp;lt;- merge(ds, ds.Mean, by.x = &amp;quot;CR.Date.Levels&amp;quot;)
# ds

# Now we calculate the Effect Size and Cohen&amp;#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate

# Now we calculate the Effect Size and Cohen&amp;#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate usinf our function

RequiredReplicates_for_all_fields &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(
field = ds,
intervals = ControlRateInterval,
Single.Field.Analysis = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This table gives us the required replicates and effectSize for all ControlRates-Date pairs&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RequiredReplicates_for_all_fields&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Req.Rep.table.field
##       Group                                  EffectSize          
##  [1,] &amp;quot;23000*2018-05-17 Vs 23000*2018-05-18&amp;quot; &amp;quot;5.76937088213814&amp;quot;  
##  [2,] &amp;quot;23000*2018-05-18 Vs 23000*2018-05-20&amp;quot; &amp;quot;1.77467806243082&amp;quot;  
##  [3,] &amp;quot;23000*2018-05-20 Vs 24000*2018-05-17&amp;quot; &amp;quot;0.319085268435178&amp;quot; 
##  [4,] &amp;quot;24000*2018-05-17 Vs 24000*2018-05-18&amp;quot; &amp;quot;2.23695386227101&amp;quot;  
##  [5,] &amp;quot;24000*2018-05-18 Vs 24000*2018-05-20&amp;quot; &amp;quot;1.4793489792455&amp;quot;   
##  [6,] &amp;quot;24000*2018-05-20 Vs 25000*2018-05-17&amp;quot; &amp;quot;0.764820441835373&amp;quot; 
##  [7,] &amp;quot;25000*2018-05-17 Vs 25000*2018-05-18&amp;quot; &amp;quot;0.749715759825526&amp;quot; 
##  [8,] &amp;quot;25000*2018-05-18 Vs 25000*2018-05-20&amp;quot; &amp;quot;0.631404233619908&amp;quot; 
##  [9,] &amp;quot;25000*2018-05-20 Vs 25000*2018-05-21&amp;quot; &amp;quot;0.0122749797874684&amp;quot;
## [10,] &amp;quot;25000*2018-05-21 Vs 26000*2018-05-17&amp;quot; &amp;quot;1.86649999718639&amp;quot;  
## [11,] &amp;quot;26000*2018-05-17 Vs 26000*2018-05-18&amp;quot; &amp;quot;0.604237639607718&amp;quot; 
## [12,] &amp;quot;26000*2018-05-18 Vs 26000*2018-05-19&amp;quot; &amp;quot;0.335012013191533&amp;quot; 
## [13,] &amp;quot;26000*2018-05-19 Vs 26000*2018-05-20&amp;quot; &amp;quot;0.294694932900609&amp;quot; 
## [14,] &amp;quot;26000*2018-05-20 Vs 26000*2018-05-21&amp;quot; &amp;quot;0.0568859046090692&amp;quot;
## [15,] &amp;quot;26000*2018-05-21 Vs 27000*2018-05-17&amp;quot; &amp;quot;0.450821573333859&amp;quot; 
## [16,] &amp;quot;27000*2018-05-17 Vs 27000*2018-05-18&amp;quot; &amp;quot;0.227990733240137&amp;quot; 
## [17,] &amp;quot;27000*2018-05-18 Vs 27000*2018-05-19&amp;quot; &amp;quot;0.212920132008121&amp;quot; 
## [18,] &amp;quot;27000*2018-05-19 Vs 27000*2018-05-20&amp;quot; &amp;quot;0.186994200485925&amp;quot; 
## [19,] &amp;quot;27000*2018-05-20 Vs 27000*2018-05-21&amp;quot; &amp;quot;0.312513439276243&amp;quot; 
## [20,] &amp;quot;27000*2018-05-21 Vs 28000*2018-05-17&amp;quot; &amp;quot;0.296577835810804&amp;quot; 
## [21,] &amp;quot;28000*2018-05-17 Vs 28000*2018-05-18&amp;quot; &amp;quot;0.114094426452159&amp;quot; 
## [22,] &amp;quot;28000*2018-05-18 Vs 28000*2018-05-19&amp;quot; &amp;quot;2.35237654983648&amp;quot;  
## [23,] &amp;quot;28000*2018-05-19 Vs 28000*2018-05-20&amp;quot; &amp;quot;0.182678706582559&amp;quot; 
## [24,] &amp;quot;28000*2018-05-20 Vs 28000*2018-05-21&amp;quot; &amp;quot;0.703554320633904&amp;quot; 
## [25,] &amp;quot;28000*2018-05-21 Vs 29000*2018-05-17&amp;quot; &amp;quot;0.983655196454225&amp;quot; 
## [26,] &amp;quot;29000*2018-05-17 Vs 29000*2018-05-20&amp;quot; &amp;quot;0.927565354976892&amp;quot; 
## [27,] &amp;quot;29000*2018-05-20 Vs 29000*2018-05-21&amp;quot; &amp;quot;0.341504648673164&amp;quot; 
##       RequiredReplicates
##  [1,] &amp;quot;0&amp;quot;               
##  [2,] &amp;quot;5&amp;quot;               
##  [3,] &amp;quot;154&amp;quot;             
##  [4,] &amp;quot;3&amp;quot;               
##  [5,] &amp;quot;7&amp;quot;               
##  [6,] &amp;quot;27&amp;quot;              
##  [7,] &amp;quot;28&amp;quot;              
##  [8,] &amp;quot;39&amp;quot;              
##  [9,] &amp;quot;104183&amp;quot;          
## [10,] &amp;quot;5&amp;quot;               
## [11,] &amp;quot;43&amp;quot;              
## [12,] &amp;quot;140&amp;quot;             
## [13,] &amp;quot;181&amp;quot;             
## [14,] &amp;quot;4851&amp;quot;            
## [15,] &amp;quot;77&amp;quot;              
## [16,] &amp;quot;302&amp;quot;             
## [17,] &amp;quot;346&amp;quot;             
## [18,] &amp;quot;449&amp;quot;             
## [19,] &amp;quot;161&amp;quot;             
## [20,] &amp;quot;178&amp;quot;             
## [21,] &amp;quot;1206&amp;quot;            
## [22,] &amp;quot;3&amp;quot;               
## [23,] &amp;quot;470&amp;quot;             
## [24,] &amp;quot;32&amp;quot;              
## [25,] &amp;quot;16&amp;quot;              
## [26,] &amp;quot;18&amp;quot;              
## [27,] &amp;quot;135&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot this results&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RequiredReplicates_for_all_fields &amp;lt;-
  as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field)
  
  RequiredReplicates_for_all_fields$Group &amp;lt;-
  factor(RequiredReplicates_for_all_fields$Group)
  
  RequiredReplicates_for_all_fields$RequiredReplicates &amp;lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates))
  
  RequiredReplicates_for_all_fields$EffectSize &amp;lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize))
  
  
  # There are one or two extreme Required replicates and extreme Effect size. We can
  # remove them to plot them, so we can properly visualize the datapoints
  
  RequiredReplicates_for_all_fields &amp;lt;-
  RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$RequiredReplicates &amp;lt; 4000, ]
  RequiredReplicates_for_all_fields &amp;lt;-
  RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$EffectSize &amp;lt; 1.5, ]
  
  my_levels &amp;lt;-
  length(levels(RequiredReplicates_for_all_fields$Group))
  RequiredReplicates_for_all_fields$EffectSize &amp;lt;-
  round(RequiredReplicates_for_all_fields$EffectSize, 2)
  
  # scaleFUN &amp;lt;- function(x) sprintf(&amp;quot;%.2f&amp;quot;, x)
  # Plot RequiredReplicates_for_all_fields
  
  n &amp;lt;- my_levels
  qual_col_pals = brewer.pal.info[brewer.pal.info$category == &amp;#39;qual&amp;#39;, ]
  col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
  
  
  ggplot(
  RequiredReplicates_for_all_fields,
  aes(
  x = EffectSize,
  y = RequiredReplicates,
  shape = as.factor(Group),
  group = 1
  )
  ) +
  ggtitle(&amp;quot;Effect size Vs Required replicates \n for all fields data \n(Combined Effect)&amp;quot;) +
  geom_point(alpha = 0.9,
  size = 3,
  stroke = 1)  +
  scale_shape_manual(values = rep(c(0:2, 5:6, 9:10, 11:12, 14), times =
  4)) +
  scale_color_manual(values = col_vector) +
  geom_line(color = &amp;quot;black&amp;quot;) +
  scale_x_continuous(&amp;quot;EffectSize&amp;quot;,
  breaks = c(0,
  0.2,
  0.5,
  1,
  max(
  as.numeric(RequiredReplicates_for_all_fields$EffectSize)
  ))) +
  theme_bw() +
  theme(
  axis.title = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
  axis.text = element_text(size = 10),
  plot.title = element_text(hjust = 0.5, face = &amp;quot;bold&amp;quot;),
  axis.line = element_line(colour = &amp;quot;black&amp;quot;),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank()
  ) +
  labs(shape = &amp;quot;Comparison groups&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this plot above, we can tell that there is strong negative correlation
between EffectSize and RequiredReplicates.&lt;/p&gt;
&lt;p&gt;So far, we analyzed our data for controlRates-Date pair, now we can also do the
same analysis for Date effect only.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;######################## Date Only Analysis !! ##############################

# If we want to do Date effect only without taking ControlRates into consideration, then we do the following:
  
# field A
fieldA.data &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataA,
intervals = ControlRateInterval,
Date.Only = TRUE)

# fieldA.data


# field B
fieldB.data &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataB,
intervals = ControlRateInterval,
Date.Only = TRUE)

# fieldB.data



# field C
fieldC.data &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataC,
intervals = ControlRateInterval,
Date.Only = TRUE)

# fieldC.data

# field D
fieldD.data &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataD,
intervals = ControlRateInterval,
Date.Only = TRUE)

# fieldD.data


# We can now merge all four fields  for SD, means and ControlRate level counts.

merged.SD.4.plots &amp;lt;-
Reduce(
function(x, y)
merge(x, y, all = TRUE),
list(
fieldA.data$fieldSD,
fieldB.data$fieldSD,
fieldC.data$fieldSD,
fieldD.data$fieldSD
)
)


merged.Mean.4.plots &amp;lt;-
Reduce(
function(x, y)
merge(x, y, all = TRUE),
list(
fieldA.data$field.mean,
fieldB.data$field.mean,
fieldC.data$field.mean,
fieldD.data$field.mean
)
)


merged.Count.4.plots &amp;lt;-
Reduce(
function(x, y)
merge(x, y, all = TRUE),
list(
fieldA.data$field.Count,
fieldB.data$field.Count,
fieldC.data$field.Count,
fieldD.data$field.Count
)
)

# We need SD pooled for these levels so we can calculate Cohen&amp;#39;s d and
# Required Replicates.

levels(merged.SD.4.plots$CR.Date.Levels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2018-05-17&amp;quot; &amp;quot;2018-05-20&amp;quot; &amp;quot;2018-05-21&amp;quot; &amp;quot;2018-05-18&amp;quot; &amp;quot;2018-05-19&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First, we merged all merged.SD.4.plots, merged.Mean.4.plots and
# merged.Count.4.plots from all fields.

# Therefore, now merging all three dataframes from all four plots for SD, Mean
# and counts by ControlRate column

Mean_SD_Count.dat &amp;lt;- Reduce(function(...)
merge(..., by = c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;grp&amp;quot;), all.x = TRUE),
lapply(
list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots),
transform,
grp = ave(seq_along(CR.Date.Levels), CR.Date.Levels, FUN = seq_along)
))

head(Mean_SD_Count.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CR.Date.Levels grp     Mean       SD Count
## 1     2018-05-17   1 234.0993 40.74205  6710
## 2     2018-05-18   1 222.4764 20.50457  3880
## 3     2018-05-19   1 229.3088 10.26760  6524
## 4     2018-05-20   1 229.8301 26.90213  7747
## 5     2018-05-20   2 230.8569 30.22076 12654
## 6     2018-05-21   1 226.4394 19.87301  1574&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now, we can also do Anova on the merged data `Mean_SD_Count.dat`

Mean_SD_Count_merged_for_all_four_plots &amp;lt;- Mean_SD_Count.dat
Mean_SD_Count_merged_for_all_four_plots$Yield &amp;lt;-
Mean_SD_Count_merged_for_all_four_plots$Mean
Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels &amp;lt;-
Mean_SD_Count_merged_for_all_four_plots$ControlRate

Mean_SD_Count_merged_for_all_four_plots$CR.Date.Levels &amp;lt;-
as.factor(gsub(
&amp;quot;-&amp;quot;,
&amp;quot;_&amp;quot;,
Mean_SD_Count_merged_for_all_four_plots$CR.Date.Levels
))


get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = &amp;quot;all four fields&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      Df Sum Sq Mean Sq F value Pr(&amp;gt;F)
## field$CR.Date.Levels  4  78.66  19.665    37.3  0.122
## Residuals             1   0.53   0.527&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this result above, since the residuals from ANOVA model has only 1 degree
of freedom, and the global p-value is not significant, we don’t have to
plot the TUKEY pairwise test. Instead, we only show the Summary anova. This
shows that the date alone doesn’t affect our mean Yield.&lt;/p&gt;
&lt;p&gt;We now calculate pooled SD for merged 4 plots &lt;code&gt;Mean_SD_Count.dat&lt;/code&gt; to calculate
Effect Size and Required Replicates for all fields (combined analysis) using
pooled SD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pooled.dat &amp;lt;- Mean_SD_Count.dat


# # Pooled sd can be calculated as:
pooled.dat$df &amp;lt;- pooled.dat$Count - 1
# pooled SD is :
# pooledSD &amp;lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) )


# We can calculate our SD pooled using this formula:
#  s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
# We will derrive this in steps as below:

pooled.dat$df &amp;lt;- pooled.dat$Count - 1
pooled.dat$sd.square &amp;lt;- pooled.dat$SD ^ 2

pooled.dat$ss &amp;lt;- pooled.dat$sd.square * pooled.dat$df


# We can use convenience function (aggregate) for splitting and calculating
# the necessary sums.
ds &amp;lt;- aggregate(ss ~ CR.Date.Levels, data = pooled.dat, sum)

# Two different built in methods for split apply, we could use aggregate for
# both if we wanted. This calculates our degrees of freedom.
ds$df &amp;lt;- tapply(pooled.dat$df, pooled.dat$CR.Date.Levels, sum)
# divide ss by df and then we get sd square
ds$sd.square &amp;lt;- ds$ss / ds$df
# Finally, we can get our sd pooled
ds$SD_pooled &amp;lt;- sqrt(ds$sd.square)
ds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CR.Date.Levels         ss    df sd.square SD_pooled
## 1     2018-05-17 11136365.7  6709 1659.9144  40.74205
## 2     2018-05-20 17161878.7 20399  841.3098  29.00534
## 3     2018-05-21   621235.0  1573  394.9364  19.87301
## 4     2018-05-18  1630876.2  3879  420.4373  20.50457
## 5     2018-05-19   687678.2  6523  105.4236  10.26760&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# However, we could also calculate our sd_pooled as below and get the same results :
sd_pooled &amp;lt;-
lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$CR.Date.Levels),
function(dd)
sqrt(sum(dd$SD ^ 2 * (dd$Count - 1)) / (sum(dd$Count - 1) - nrow(dd))))

# Now, we calculate Mean (Mean of Means) from the merged table
# `Mean_SD_Count.dat`, so we can calculate Cohens d and RequiredReplicates for
# all four field combined.

ds.Mean &amp;lt;-
setNames(aggregate(
Mean_SD_Count.dat$Mean,
by = list(Mean_SD_Count.dat$CR.Date.Levels),
FUN = mean
),
c(&amp;quot;CR.Date.Levels&amp;quot;, &amp;quot;Mean&amp;quot;))

ds &amp;lt;- merge(ds, ds.Mean, by.x = &amp;quot;CR.Date.Levels&amp;quot;)
ds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CR.Date.Levels         ss    df sd.square SD_pooled     Mean
## 1     2018-05-17 11136365.7  6709 1659.9144  40.74205 234.0993
## 2     2018-05-18  1630876.2  3879  420.4373  20.50457 222.4764
## 3     2018-05-19   687678.2  6523  105.4236  10.26760 229.3088
## 4     2018-05-20 17161878.7 20399  841.3098  29.00534 230.3435
## 5     2018-05-21   621235.0  1573  394.9364  19.87301 226.4394&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we calculate the Effect Size and Cohen&amp;#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate

# Now we calculate the Effect Size and Cohen&amp;#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate usinf our function

# `calculate_field_mean_SD_and_get_RR_EffSize`.

RequiredReplicates_for_all_fields &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(
field = ds,
intervals = ControlRateInterval,
Single.Field.Analysis = FALSE
)

RequiredReplicates_for_all_fields&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Req.Rep.table.field
##      Group                      EffectSize          RequiredReplicates
## [1,] &amp;quot;2018-05-17 Vs 2018-05-18&amp;quot; &amp;quot;0.360381603784739&amp;quot; &amp;quot;121&amp;quot;             
## [2,] &amp;quot;2018-05-18 Vs 2018-05-19&amp;quot; &amp;quot;0.421358834058387&amp;quot; &amp;quot;88&amp;quot;              
## [3,] &amp;quot;2018-05-19 Vs 2018-05-20&amp;quot; &amp;quot;0.047558010269331&amp;quot; &amp;quot;6940&amp;quot;            
## [4,] &amp;quot;2018-05-20 Vs 2018-05-21&amp;quot; &amp;quot;0.157027441353185&amp;quot; &amp;quot;637&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can plot this results for better visualization of pattern.

RequiredReplicates_for_all_fields &amp;lt;-
as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field)

RequiredReplicates_for_all_fields$Group &amp;lt;-
factor(RequiredReplicates_for_all_fields$Group)

RequiredReplicates_for_all_fields$RequiredReplicates &amp;lt;-
as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates))

RequiredReplicates_for_all_fields$EffectSize &amp;lt;-
as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize))


# There are one or two extreme Required replicates and extreme Effect size. We can
# remove them to plot them, so we can properly visualize the datapoints

RequiredReplicates_for_all_fields &amp;lt;-
RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$RequiredReplicates &amp;lt; 4000, ]

RequiredReplicates_for_all_fields &amp;lt;-
RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$EffectSize &amp;lt; 1.5, ]



my_levels &amp;lt;-
length(levels(RequiredReplicates_for_all_fields$Group))
RequiredReplicates_for_all_fields$EffectSize &amp;lt;-
round(RequiredReplicates_for_all_fields$EffectSize, 2)

# Plot RequiredReplicates_for_all_fields
n &amp;lt;- my_levels
qual_col_pals = brewer.pal.info[brewer.pal.info$category == &amp;#39;qual&amp;#39;,]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))


ggplot(
RequiredReplicates_for_all_fields,
aes(
x = EffectSize,
y = RequiredReplicates,
shape = as.factor(Group),
group = 1
)
) +
ggtitle(&amp;quot;Effect size Vs Required replicates for all fields data \n(Combined Effect)&amp;quot;) +
geom_point(alpha = 0.9,
size = 3,
stroke = 1)  +
scale_shape_manual(values = rep(c(0:2, 5:6, 9:10, 11:12, 14), times =
4)) +
scale_color_manual(values = col_vector) +
geom_line(color = &amp;quot;black&amp;quot;) +
scale_x_continuous(&amp;quot;EffectSize&amp;quot;,
breaks = c(0,
0.2,
0.5,
1,
max(
as.numeric(RequiredReplicates_for_all_fields$EffectSize)
))) +
theme_bw() +
theme(
axis.title = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
axis.text = element_text(size = 10),
plot.title = element_text(hjust = 0.5, face = &amp;quot;bold&amp;quot;),
axis.line = element_line(colour = &amp;quot;black&amp;quot;),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
labs(shape = &amp;quot;Comparison groups&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Final_assignment_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;################################ End of codes !! ################################&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discussion: Based on these results, I think that the fields were planted within
24 hours margin. seedB and seedC were planted on the different dates.
Similarly, harvestB was harvested within 24 hours and on the same date, but rest
of the fields were harvested/planted for multiple dates. Additionally, I also
performed ANOVA for ControlRates paired with dates and found no significant
difference to what I found for my midterm. I analyzed these data for Date
effect also and it looks like there is no Date effect on Mean Yield. I thought
the skewned in 28000 interval from my midterm would be addressed by Date effect,
but I could not find significant effect of date here. As in the mid-term, I also
calculated the required replicated for ControlRate-Date pairs as well as Date
only observations, and in both cases, the EffectSize was inversely proportional
to RequiredReplicates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Midterm data analysis</title>
      <link>/achalneupane.github.io/post/midterm_project/</link>
      <pubDate>Fri, 21 Jun 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/midterm_project/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;In this exercise, I will be analyzing four fields as a
statistical consultant. This data comes from four different farm fields and we try to investigate the effects of variable seeding rates in overall yeild.&lt;/p&gt;
&lt;p&gt;The table summarizes data from four corn fields. The top row are target seeding rates, in seeds per acre, the bottom row are
corn yields in bushels per acre.
This table summarizes all four fields at the Control Rate interval of 1000 which was also reproduced below in my anaysis. More importantly, we are interested in exploring the EffectSize and Required Replicates for properly examining any statistical evidence for the yield and variable seeding rates relationships.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Rate&lt;/th&gt;
&lt;th&gt;23000&lt;/th&gt;
&lt;th&gt;24000&lt;/th&gt;
&lt;th&gt;25000&lt;/th&gt;
&lt;th&gt;26000&lt;/th&gt;
&lt;th&gt;27000&lt;/th&gt;
&lt;th&gt;28000&lt;/th&gt;
&lt;th&gt;29000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Yield&lt;/td&gt;
&lt;td&gt;111.4216&lt;/td&gt;
&lt;td&gt;155.0326&lt;/td&gt;
&lt;td&gt;181.1176&lt;/td&gt;
&lt;td&gt;227.5800&lt;/td&gt;
&lt;td&gt;233.4623&lt;/td&gt;
&lt;td&gt;242.1753&lt;/td&gt;
&lt;td&gt;231.3890&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The algorithm to do this analysis is divided into several steps as described below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I have used some external packages like ggplot2 to make the plots.&lt;/li&gt;
&lt;li&gt;I have written several functions to analyze each individual field as well as merged field statistics that calculates cohens effect size, required replicates, and does ANOVA analysis on individual field as well as on merged data from all fields.&lt;/li&gt;
&lt;li&gt;Function &lt;code&gt;calculate_field_mean_SD_and_get_RR_EffSize&lt;/code&gt; also uses mean, sd, and counts from each field to calculate cohens d and Required replicates. It also uses pooled standard deviations and Mean (mean of Means) calculated from all four fields for different control Rates to calculate cohens d and required replicates of combined field data. I have tested this function on all four fields at ControlRate level of 500, 1000, 2000 and 3000 intervals, then based on the results (also merged the output of all four intervals to get the effectSize and Required Replicates compared), I have decided to choose 1000 intervals of ControlRates for further analysis for combined data from all four fields.&lt;/li&gt;
&lt;li&gt;I have also shown ANOVA analysis followed by TUKEY HSD test to show which ControlRates of seeding have significant effect on Yield for each field and then for all fields (combined all four fields).&lt;/li&gt;
&lt;li&gt;Lastly, I have plotted EffectSize Vs RequiredReplicates for each field and also for combined data from four fields.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, we first install ond load some of the packages (“multcompView”, “ggplot2”, “scales”, “data.table”) I will be using for this exercise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First, install missing packages and load them
myPackages &amp;lt;- c(&amp;quot;multcompView&amp;quot;, &amp;quot;ggplot2&amp;quot;, &amp;quot;scales&amp;quot;, &amp;quot;data.table&amp;quot;)
my.installed.packages&amp;lt;- installed.packages()
available.packages &amp;lt;- myPackages %in% my.installed.packages
if (sum(!available.packages) &amp;gt; 0){
  install.packages(myPackages[!available.packages])
}
# Load all required packages 
lapply(myPackages, require, character.only = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: multcompView&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: scales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: data.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] TRUE
## 
## [[2]]
## [1] TRUE
## 
## [[3]]
## [1] TRUE
## 
## [[4]]
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will first write and describe our function here. We will be using these function to manipulate our data, calculate Cohen’s d, required replicates and also do ANOVA and TukeyHSD paired test, then finally to plot our data.
We can calculate EffectSize and RequiredReplicates for these data using functions from previous homework &lt;code&gt;cohen.d&lt;/code&gt; and
&lt;code&gt;required.replicates&lt;/code&gt;. These two functions are then called within another function &lt;code&gt;calculate_field_mean_SD_and_get_RR_EffSize&lt;/code&gt; that analyzes each individual field and also merged data from all four fields to calculate Effect Size, Required Replictaes and then plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cohen.d &amp;lt;- function(m1, s1, m2, s2){
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  return(cohens_d)
}

required.replicates &amp;lt;- function (m1, s1, m2, s2, alpha=0.05, beta=0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We willl also perform ANOVA analysis with Tukey Test for paired comparision of mean for each field data as well as merged data at different ControlRate intervals. This function does Tukey HSD test and generates label for significant outcomes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create function to get the labels for Tukey HSD:
generate_label_df &amp;lt;- function(TUKEY, variable){
  
  # Extract labels and factor levels from Tukey post-hoc 
  Tukey.levels &amp;lt;- TUKEY[[variable]][,4]
  Tukey.labels &amp;lt;- data.frame(multcompLetters(Tukey.levels)[&amp;#39;Letters&amp;#39;])
  
  #I need to put the labels in the same order as in the boxplot :
  Tukey.labels$treatment=rownames(Tukey.labels)
  Tukey.labels=Tukey.labels[order(Tukey.labels$treatment) , ]
  return(Tukey.labels)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function does ANOVA and makes boxplots with Tukey statistics for comparing Mean yield.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_my_box_plot &amp;lt;- function (field, plot_name = &amp;quot;the Field&amp;quot;) {
  model = lm(field$Yield ~ field$ControlRate.Levels)
  ANOVA = aov(model)
  
  # Tukey test to study each pair of treatment :
  TUKEY &amp;lt;- TukeyHSD(x = ANOVA, &amp;#39;field$ControlRate.Levels&amp;#39;, conf.level = 0.95)
  
  # generate labels using function
  labels &amp;lt;- generate_label_df(TUKEY , &amp;quot;field$ControlRate.Levels&amp;quot;)
  
  # rename columns for merging
  names(labels) &amp;lt;- c(&amp;#39;Letters&amp;#39;, &amp;#39;ControlRate.Levels&amp;#39;)
  
  # Obtain letter positions for y axis using means
  yvalue &amp;lt;- aggregate(. ~ ControlRate.Levels, data = field, mean)
  
  final &amp;lt;- merge(labels, yvalue) #merge dataframes
  
  p &amp;lt;- ggplot(field, aes(x = ControlRate.Levels, y = Yield)) +
  geom_blank() +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()) +
  labs(x = &amp;#39;Control Rates&amp;#39;, y = &amp;#39;Mean Yield&amp;#39;) +
  ggtitle(paste0(&amp;quot;ControlRates Vs Mean yield for &amp;quot;, plot_name),
  expression(atop(italic(&amp;quot;(Anova:TukeyHSD)&amp;quot;), &amp;quot;&amp;quot;))) +
  theme(plot.title = element_text(hjust = 0.5, face = &amp;#39;bold&amp;#39;)) +
  geom_boxplot(fill = &amp;#39;grey&amp;#39;, stat = &amp;quot;boxplot&amp;quot;) +
  geom_text(
  data = final,
  aes(x = ControlRate.Levels, y = Yield, label = Letters),
  vjust = -3.5,
  hjust = -.5
  ) +
  geom_vline(aes(xintercept = 4.5), linetype = &amp;quot;dashed&amp;quot;) +
  theme(plot.title = element_text(vjust = -0.6))
  print(p)
  # print(TUKEY)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Function to calculate mean and sd for each field for a given interval of
ControlRate and then calculate Required Replicates and EffectSize. I have made
one function to do all that so I can just use this function to analyze all
four fields. To analyze single field set &lt;code&gt;Single.Field.Analysis = TRUE&lt;/code&gt;, and to analyze all four field together, set &lt;code&gt;Single.Field.Analysis = FALSE&lt;/code&gt;,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calculate_field_mean_SD_and_get_RR_EffSize &amp;lt;- 
  function(
    field, intervals, Single.Field.Analysis = TRUE, printplot = FALSE){
  plot_NAME &amp;lt;- (deparse(substitute(field)))
  if(Single.Field.Analysis &amp;gt; 0){
  # removing rows with NAs in Yield
  sum(is.na(field$Yield))
  # field &amp;lt;- field[!complete.cases(field$Yield), ]
  field &amp;lt;- field[!is.na(field$Yield),]
  # as.factor(fieldA$ControlRate)
  table.field &amp;lt;- table(as.factor(field$ControlRate))
  field$ControlRate.Levels &amp;lt;- as.factor(intervals * ceiling(field$ControlRate/intervals))
  field.Count &amp;lt;- setNames(aggregate(field$ControlRate, 
          by = list(field$ControlRate.Levels), FUN = length), c(&amp;quot;ControlRate&amp;quot;, &amp;quot;Count&amp;quot;))
  # Degree of freedome = n * k - k
  field.Count$degree.Freedom &amp;lt;- 
    (field.Count$Count * length(field.Count$ControlRate)) -length(field.Count$ControlRate)
  
  field.mean &amp;lt;-
    setNames(aggregate(
    field$Yield,
    by = list(field$ControlRate.Levels),
    FUN = mean
    ),
    c(&amp;quot;ControlRate&amp;quot;, &amp;quot;Mean&amp;quot;))
    
    field.SD &amp;lt;-
    setNames(aggregate(
    field$Yield,
    by = list(field$ControlRate.Levels),
    FUN = sd
    ),
    c(&amp;quot;ControlRate&amp;quot;, &amp;quot;SD&amp;quot;))

  # plot individual fields with tukey test We will print box plot only if we
  # want for certain ControlRates intervals. Otherwise we will have too many
  # plots
  if(printplot == 1 ){
  get_my_box_plot(field, plot_name = plot_NAME)
  }
  # Else we work on the merged data with SD pooled; no need to calculate mean
  # and SD as we will be doing it below
  
  } else {
    temp.Field &amp;lt;- field
    colnames(temp.Field)[colnames(temp.Field) == &amp;quot;ControlRate&amp;quot;] &amp;lt;- &amp;quot;ControlRate.Levels&amp;quot;
    colnames(temp.Field)[colnames(temp.Field) == &amp;quot;Mean&amp;quot;] &amp;lt;- &amp;quot;Yield&amp;quot;
    # get_my_box_plot(temp.Field)
    field.SD  &amp;lt;- 
      as.data.frame(cbind(ControlRate = field[&amp;quot;ControlRate&amp;quot;], SD = field[&amp;quot;SD_pooled&amp;quot;]))
    field.mean &amp;lt;- 
      as.data.frame(cbind(ControlRate = field[&amp;quot;ControlRate&amp;quot;], Mean = field[&amp;quot;Mean&amp;quot;]))
  }
  
# Calculate Required replicate and Effect Size from each
# field for ControlRate i vs i+1

# ReqRep_EffectSize_table &amp;lt;- function (field.mean, field.SD){
Req.Rep.table.field &amp;lt;- {}
for (i in 1:nrow(field.SD)){
  if(i+1 &amp;gt; nrow(field.SD) ){
    break
  }
  temp.Effect.size &amp;lt;-
    cohen.d(
    m1 = field.mean$Mean[i],
    s1 = field.SD$SD[i],
    m2 = field.mean$Mean[i + 1],
    s2 = field.SD$SD[i + 1]
    )
    
    tmp.req.reps &amp;lt;-
    required.replicates(
    m1 = field.mean$Mean[i],
    s1 = field.SD$SD[i],
    m2 = field.mean$Mean[i + 1],
    s2 = field.SD$SD[i + 1]
    )
 
    tmp.table &amp;lt;-
    cbind(
    Group = paste0(field.SD$ControlRate[i], &amp;quot; Vs &amp;quot;, field.SD$ControlRate[i +
    1]),
    EffectSize = temp.Effect.size,
    RequiredReplicates = tmp.req.reps
    )
      
      Req.Rep.table.field &amp;lt;- rbind(Req.Rep.table.field, tmp.table)
}
if (Single.Field.Analysis &amp;gt; 0) {
  return(
    list(
      field.mean = field.mean,
      fieldSD = field.SD,
      field.Count = field.Count,
      Req.Rep.table.field = Req.Rep.table.field
    )
  )
} else{
  return(list(Req.Rep.table.field = Req.Rep.table.field))
}
  }

###############################################End of Functions##########&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we read four fields data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fieldA &amp;lt;-
  read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldA.csv&amp;quot;,
  header = TRUE,
  sep = &amp;quot;,&amp;quot;
  )
  # head(fieldA)
  dim(fieldA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6718    6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  tmp.fieldA &amp;lt;- fieldA
  tmp.fieldA$ControlRate &amp;lt;- as.factor(tmp.fieldA$ControlRate)
  # aggregate(fieldA$Yield, by = list(fieldA$ControlRate), FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: There are 40 levels of Control rates in fieldA, we can reduce these levels, so what we can do is relevel them separated by ‘intervals’ of(say 1000) as used in ‘calculate_field_mean_SD’ function above. We can do the same for other fields. However, instead of checking all this one by one, we will be using the functions we described above to analyze these four fields data and get the plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fieldB &amp;lt;-
  read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldB.csv&amp;quot;,
  header = TRUE,
  sep = &amp;quot;,&amp;quot;
  )
  # head(fieldB)
  dim(fieldB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9321    6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # fieldC
  fieldC &amp;lt;-
  read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldC.csv&amp;quot;,
  header = TRUE,
  sep = &amp;quot;,&amp;quot;
  )
  # head(fieldC)
  dim(fieldC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10404     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # fieldD
  fieldD &amp;lt;-
  read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldD.csv&amp;quot;,
  header = TRUE,
  sep = &amp;quot;,&amp;quot;
  )
  # head(fieldD)
  dim(fieldD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12654     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can calculate EffectSize and RequiredReplicates for these four fields using previous homeworks functions Cohen’s d and
required.replicates as described in the begining of this report.&lt;/p&gt;
&lt;p&gt;Here, we can work on each individual field, first for the interval of 500, 1000,
2000 and 3000 ControlRates and decide which ControlRate interval fits the best
for our data. Then we we will work on merged data that we merge after
calculating individual fields (i.e using Mean and SD pooled for each
ControlRate from all fields)&lt;/p&gt;
&lt;p&gt;We chan check how many replicates we need for each field if we compare at
the different ControlRates intervals starting with 500 to 3000. Set &lt;code&gt;eval = TRUE&lt;/code&gt; to check this code.&lt;/p&gt;
&lt;p&gt;At ControlRates interval of 500:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we chan check how many replicates we need for each field if we compare at
# the different ControlRates intervals starting with 500 to 3000:

# At ControlRates interval of 500:
ControlRateInterval &amp;lt;- 500

# field A
fieldA.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )

# field B
fieldB.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )

# field C
fieldC.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )

# field D
fieldD.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )

# Now, we merge all these so we can plot them together later
my_list &amp;lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)

my_list_nms &amp;lt;- setNames(my_list, c(&amp;quot;fieldA&amp;quot;, &amp;quot;fieldB&amp;quot;, &amp;quot;fieldC&amp;quot;, &amp;quot;fieldD&amp;quot;))
Merged.EffectSize.500 &amp;lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &amp;quot;id&amp;quot;))
Merged.EffectSize.500$interval &amp;lt;- 500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At ControlRates interval of 1000:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ControlRateInterval &amp;lt;- 1000

# field A
fieldA.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )

# field C
fieldC.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )

# field D
fieldD.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )

# Now, we merge all these so we can plot them together later
my_list &amp;lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)

my_list_nms &amp;lt;- setNames(my_list, c(&amp;quot;fieldA&amp;quot;, &amp;quot;fieldB&amp;quot;, &amp;quot;fieldC&amp;quot;, &amp;quot;fieldD&amp;quot;))
Merged.EffectSize.1000 &amp;lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &amp;quot;id&amp;quot;))

Merged.EffectSize.1000$interval &amp;lt;- 1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At ControlRates interval of 2000:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ControlRateInterval &amp;lt;- 2000

# field A
fieldA.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )

# field C
fieldC.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )

# field D
fieldD.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )

# Now, we merge all these so we can plot them together later
my_list &amp;lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)

my_list_nms &amp;lt;- setNames(my_list, c(&amp;quot;fieldA&amp;quot;, &amp;quot;fieldB&amp;quot;, &amp;quot;fieldC&amp;quot;, &amp;quot;fieldD&amp;quot;))
Merged.EffectSize.2000 &amp;lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &amp;quot;id&amp;quot;))
Merged.EffectSize.2000$interval &amp;lt;- 2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At ControlRates interval of 3000:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ControlRateInterval &amp;lt;- 3000

# field A
fieldA.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )

# field C
fieldC.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )

# field D
fieldD.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )

# Now, we merge all these so we can plot them together later
my_list &amp;lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)

my_list_nms &amp;lt;- setNames(my_list, c(&amp;quot;fieldA&amp;quot;, &amp;quot;fieldB&amp;quot;, &amp;quot;fieldC&amp;quot;, &amp;quot;fieldD&amp;quot;))
Merged.EffectSize.3000 &amp;lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &amp;quot;id&amp;quot;))
Merged.EffectSize.3000$interval &amp;lt;- 3000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s plot these data for EffectSize Vs RequiredReplicates for each Field and fordifferent ControlRate intervals&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EffectSize.Plot &amp;lt;-
  rbind(
  Merged.EffectSize.500,
  Merged.EffectSize.1000,
  Merged.EffectSize.2000,
  Merged.EffectSize.3000
  )

EffectSize.Plot$id &amp;lt;- factor(EffectSize.Plot$id)
EffectSize.Plot$interval &amp;lt;- factor(EffectSize.Plot$interval)
EffectSize.Plot$Group &amp;lt;- factor(EffectSize.Plot$Group)
EffectSize.Plot$EffectSize &amp;lt;- as.numeric(EffectSize.Plot$EffectSize)
EffectSize.Plot$RequiredReplicates &amp;lt;- as.numeric(EffectSize.Plot$RequiredReplicates)
head(EffectSize.Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       id          Group EffectSize RequiredReplicates interval
## 1 fieldA 23000 Vs 23500 1.03546848                 15      500
## 2 fieldA 23500 Vs 24000 0.73574477                 29      500
## 3 fieldA 24000 Vs 25000 1.48620683                  7      500
## 4 fieldA 25000 Vs 25500 0.41887785                 89      500
## 5 fieldA 25500 Vs 26000 1.10746983                 13      500
## 6 fieldA 26000 Vs 26500 0.07390218               2874      500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaleFUN &amp;lt;- function(x) sprintf(&amp;quot;%.2f&amp;quot;, x)

# To eliminate any outliers and noise, we can plot effect Size between 0-1 and
# Required Replicates upto 5000
EffectSize.Plot &amp;lt;- EffectSize.Plot[EffectSize.Plot$RequiredReplicates&amp;lt;= 5000,]
EffectSize.Plot &amp;lt;- EffectSize.Plot[EffectSize.Plot$EffectSize&amp;lt;= 1,]

EffectSize.Plot$Group &amp;lt;- factor(EffectSize.Plot$Group)

ggplot(EffectSize.Plot, aes(x=EffectSize, y=RequiredReplicates)) + 
  ggtitle(&amp;quot;Effect size Vs Required replicates for each field data&amp;quot;) +
  geom_point(aes(shape = Group), size = 2)  + 
  scale_shape_manual(values=1:nlevels(EffectSize.Plot$Group)) +
  geom_line() + 
  scale_x_continuous(&amp;quot;EffectSize&amp;quot;, breaks=c(0,0.2,0.5,1), labels = scaleFUN) +
  theme_bw() +
  theme(axis.title = element_text(size=14,face=&amp;quot;bold&amp;quot;), axis.text = element_text(size=10), 
        plot.title = element_text(hjust = 0.5, face = &amp;quot;bold&amp;quot;)) +
  facet_wrap(~id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the plot we just plotted above, RequiredReplicates inccrease significantly with decreasing Effect Size.&lt;/p&gt;
&lt;p&gt;Additionally, after running all these &lt;code&gt;ControlRateInterval&lt;/code&gt;, I noticed that the Required replicates decrease for the higher ControlRate intervals. The reason I did not include this in my report is because it gives lots of output. SO, if we consider taking the ControlRate intervals of only 1000 and get the field mean and field SD for each field as below.&lt;/p&gt;
&lt;p&gt;Now, I will only limit my analysis for all fields (merged) for the Control Rates interval of 1000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If we consider taking the ControlRate intervals of 1000 and get the field mean and field
# SD for each field:

ControlRateInterval &amp;lt;- 1000
# field A
fieldA.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, intervals = ControlRateInterval, printplot = TRUE
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fieldA.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $field.mean
##   ControlRate      Mean
## 1       23000  63.01258
## 2       24000 100.63078
## 3       25000 172.57527
## 4       26000 237.45902
## 5       27000 237.33206
## 6       28000 238.59221
## 7       29000 195.73072
## 
## $fieldSD
##   ControlRate       SD
## 1       23000 15.45740
## 2       24000 51.36069
## 3       25000 41.82109
## 4       26000 31.64908
## 5       27000 44.09390
## 6       28000 38.11963
## 7       29000 53.59605
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       23000    24            161
## 2       24000    93            644
## 3       25000    44            301
## 4       26000  4381          30660
## 5       27000   786           5495
## 6       28000  1283           8974
## 7       29000    99            686
## 
## $Req.Rep.table.field
##      Group            EffectSize            RequiredReplicates
## [1,] &amp;quot;23000 Vs 24000&amp;quot; &amp;quot;0.991869049115401&amp;quot;   &amp;quot;16&amp;quot;              
## [2,] &amp;quot;24000 Vs 25000&amp;quot; &amp;quot;1.53614606065372&amp;quot;    &amp;quot;7&amp;quot;               
## [3,] &amp;quot;25000 Vs 26000&amp;quot; &amp;quot;1.74957194606738&amp;quot;    &amp;quot;5&amp;quot;               
## [4,] &amp;quot;26000 Vs 27000&amp;quot; &amp;quot;0.00330802194919998&amp;quot; &amp;quot;1434501&amp;quot;         
## [5,] &amp;quot;27000 Vs 28000&amp;quot; &amp;quot;0.0305749319500618&amp;quot;  &amp;quot;16792&amp;quot;           
## [6,] &amp;quot;28000 Vs 29000&amp;quot; &amp;quot;0.921630680790716&amp;quot;   &amp;quot;18&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the plot we can see that the Mean Yield is significant between 23000 vs all ControlRates , 24000 vs all Control Rates, 25000 Vs all Control rates. Yeild has no significant effect of Seeding from Control Rate between 26000 to 28000. Infact, it starts to decline significantly from 29000 Control Seedings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# field B
fieldB.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval, printplot = TRUE
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fieldB.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $field.mean
##   ControlRate     Mean
## 1       24000 125.8102
## 2       25000 180.9337
## 3       26000 215.8033
## 4       27000 229.8309
## 5       28000 240.5746
## 6       29000 238.9680
## 
## $fieldSD
##   ControlRate       SD
## 1       24000 33.51371
## 2       25000 33.36007
## 3       26000 28.10560
## 4       27000 19.11270
## 5       28000 14.00890
## 6       29000 14.26832
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       24000    36            210
## 2       25000   390           2334
## 3       26000  2455          14724
## 4       27000   798           4782
## 5       28000  5173          31032
## 6       29000   469           2808
## 
## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &amp;quot;24000 Vs 25000&amp;quot; &amp;quot;1.64857878536612&amp;quot;  &amp;quot;6&amp;quot;               
## [2,] &amp;quot;25000 Vs 26000&amp;quot; &amp;quot;1.13048091635421&amp;quot;  &amp;quot;12&amp;quot;              
## [3,] &amp;quot;26000 Vs 27000&amp;quot; &amp;quot;0.583669852812804&amp;quot; &amp;quot;46&amp;quot;              
## [4,] &amp;quot;27000 Vs 28000&amp;quot; &amp;quot;0.641176026096802&amp;quot; &amp;quot;38&amp;quot;              
## [5,] &amp;quot;28000 Vs 29000&amp;quot; &amp;quot;0.113628916568796&amp;quot; &amp;quot;1216&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we see significant difference between all Control Rates of Seeding except between 28000 Vs 29000. The seeding rates of 28000 has the highest Mean yield here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# field C
fieldC.data &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval, printplot = TRUE
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fieldC.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $field.mean
##   ControlRate     Mean
## 1       23000 149.2422
## 2       24000 185.4175
## 3       25000 195.8427
## 4       26000 225.0309
## 5       27000 231.4881
## 6       28000 240.9121
## 
## $fieldSD
##   ControlRate        SD
## 1       23000 14.416699
## 2       24000 15.340123
## 3       25000 13.316623
## 4       26000 14.335465
## 5       27000 11.247829
## 6       28000  4.463365
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       23000    28            162
## 2       24000   170           1014
## 3       25000    69            408
## 4       26000  5697          34176
## 5       27000  4419          26508
## 6       28000    21            120
## 
## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &amp;quot;23000 Vs 24000&amp;quot; &amp;quot;2.43022553971697&amp;quot;  &amp;quot;3&amp;quot;               
## [2,] &amp;quot;24000 Vs 25000&amp;quot; &amp;quot;0.725780226926124&amp;quot; &amp;quot;30&amp;quot;              
## [3,] &amp;quot;25000 Vs 26000&amp;quot; &amp;quot;2.1096712954133&amp;quot;   &amp;quot;4&amp;quot;               
## [4,] &amp;quot;26000 Vs 27000&amp;quot; &amp;quot;0.501162120939759&amp;quot; &amp;quot;63&amp;quot;              
## [5,] &amp;quot;27000 Vs 28000&amp;quot; &amp;quot;1.10136177756912&amp;quot;  &amp;quot;13&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, all Control rates of seeding are significantly diffferent with the highest Mean yeild for 28000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# field D
fieldD.data &amp;lt;- calculate_field_mean_SD_and_get_RR_EffSize(
  field = fieldD, interval = ControlRateInterval, printplot = TRUE
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fieldD.data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $field.mean
##   ControlRate     Mean
## 1       23000 113.4783
## 2       24000 157.5754
## 3       25000 176.8111
## 4       26000 227.5933
## 5       27000 240.8722
## 6       28000 247.0608
## 7       29000 230.0359
## 
## $fieldSD
##   ControlRate       SD
## 1       23000 24.58440
## 2       24000 39.35167
## 3       25000 36.11914
## 4       26000 26.50089
## 5       27000 17.44482
## 6       28000 15.63857
## 7       29000 25.17324
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       23000    50            343
## 2       24000   372           2597
## 3       25000   132            917
## 4       26000  8284          57981
## 5       27000  1167           8162
## 6       28000  2631          18410
## 7       29000    18            119
## 
## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &amp;quot;23000 Vs 24000&amp;quot; &amp;quot;1.34402785417073&amp;quot;  &amp;quot;9&amp;quot;               
## [2,] &amp;quot;24000 Vs 25000&amp;quot; &amp;quot;0.50928632312699&amp;quot;  &amp;quot;61&amp;quot;              
## [3,] &amp;quot;25000 Vs 26000&amp;quot; &amp;quot;1.60311540536477&amp;quot;  &amp;quot;6&amp;quot;               
## [4,] &amp;quot;26000 Vs 27000&amp;quot; &amp;quot;0.591895252495428&amp;quot; &amp;quot;45&amp;quot;              
## [5,] &amp;quot;27000 Vs 28000&amp;quot; &amp;quot;0.373564449867553&amp;quot; &amp;quot;112&amp;quot;             
## [6,] &amp;quot;28000 Vs 29000&amp;quot; &amp;quot;0.812438594873596&amp;quot; &amp;quot;24&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, all Control Rates of seeding are significantly different except for 26000-28000 Vs 29000 with the highest Mean yield for 28000.&lt;/p&gt;
&lt;p&gt;Also, to check if my calculation of Control rates and Yeild mean matches with
the table in instruction (with 1000 intervals), we can check this with the merged field data (
i.e. fieldA, fieldB, fieldC and fieldD) data. We should get the same Mean values in
the instruction table above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.merged.raw.fields &amp;lt;- rbind(fieldA, fieldB, fieldC, fieldD)

check.the.instruction.table &amp;lt;-
calculate_field_mean_SD_and_get_RR_EffSize(
  field = all.merged.raw.fields, intervals = ControlRateInterval
  )

check.the.instruction.table$field.mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   ControlRate     Mean
## 1       23000 111.4216
## 2       24000 155.0326
## 3       25000 181.1176
## 4       26000 227.5779
## 5       27000 233.4717
## 6       28000 242.1698
## 7       29000 231.3890&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now merge all four fields for SD, means and ControlRate level counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged.SD.4.plots &amp;lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$fieldSD,
  fieldB.data$fieldSD,
  fieldC.data$fieldSD,
  fieldD.data$fieldSD
  )
  )
  
  
  merged.Mean.4.plots &amp;lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.mean,
  fieldB.data$field.mean,
  fieldC.data$field.mean,
  fieldD.data$field.mean
  )
  )
  
  
  merged.Count.4.plots &amp;lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.Count,
  fieldB.data$field.Count,
  fieldC.data$field.Count,
  fieldD.data$field.Count
  )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need SD pooled for these levels so we can calculate Cohen’s d and Required
Replicates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(merged.SD.4.plots$ControlRate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;23000&amp;quot; &amp;quot;24000&amp;quot; &amp;quot;25000&amp;quot; &amp;quot;26000&amp;quot; &amp;quot;27000&amp;quot; &amp;quot;28000&amp;quot; &amp;quot;29000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we merged all merged.SD.4.plots, merged.Mean.4.plots and merged.Count.4.plots from all fields.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Therefore, now merging all three dataframes from all four plots for SD, Mean
# and counts by ControlRate column
Mean_SD_Count.dat &amp;lt;- Reduce(function(...)
  merge(..., by = c(&amp;quot;ControlRate&amp;quot;, &amp;quot;grp&amp;quot;), all.x = TRUE),
  lapply(
    list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots),
    transform,
    grp = ave(seq_along(ControlRate), ControlRate, FUN = seq_along)
  ))

head(Mean_SD_Count.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   ControlRate grp      Mean       SD Count degree.Freedom
## 1       23000   1  63.01258 14.41670    24            161
## 2       23000   2 113.47830 15.45740    28            162
## 3       23000   3 149.24222 24.58440    50            343
## 4       24000   1 100.63078 15.34012    36            210
## 5       24000   2 125.81017 33.51371    93            644
## 6       24000   3 157.57538 39.35167   170           1014&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can also do Anova on the merged data &lt;code&gt;Mean_SD_Count.dat&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mean_SD_Count_merged_for_all_four_plots &amp;lt;- Mean_SD_Count.dat
Mean_SD_Count_merged_for_all_four_plots$Yield &amp;lt;-
Mean_SD_Count_merged_for_all_four_plots$Mean
Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels &amp;lt;-
Mean_SD_Count_merged_for_all_four_plots$ControlRate

get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = &amp;quot;all four fields&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this plot, from all fields, we can tell that, Control Rates of seeding of 23000-25000 has significantly less Mean yield as compared to 26000-29000. It looks like Control Rates of Seeding between 27000 and 28000 has the highest Mean yield.&lt;/p&gt;
&lt;p&gt;We now calculate pooled SD for merged 4 plots &lt;code&gt;Mean_SD_Count.dat&lt;/code&gt; to calculate Effect Size and Required Replicates for all fields (combined analysis) using pooled SD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pooled.dat &amp;lt;- Mean_SD_Count.dat


# # Pooled sd can be calculated as: 
pooled.dat$df &amp;lt;- pooled.dat$Count-1
# pooled SD is :
# pooledSD &amp;lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) )


# We can calculate our SD pooled using this formula:
#  s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
# We will derrive this in steps as below:

pooled.dat$df &amp;lt;- pooled.dat$Count-1
pooled.dat$sd.square &amp;lt;- pooled.dat$SD^2 

pooled.dat$ss &amp;lt;- pooled.dat$sd.square * pooled.dat$df


# We can use convenience function (aggregate) for splitting and calculating the necessary sums.
ds &amp;lt;- aggregate(ss ~ ControlRate, data = pooled.dat, sum)

# Two different built in methods for split apply, we could use aggregate for
# both if we wanted. This calculates our degrees of freedom.
ds$df &amp;lt;- tapply(pooled.dat$df, pooled.dat$ControlRate, sum) 
# divide ss by df and then we get sd square
ds$sd.square &amp;lt;- ds$ss / ds$df
# Finally, we can get our sd pooled
ds$SD_pooled &amp;lt;- sqrt(ds$sd.square)
ds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   ControlRate          ss    df sd.square SD_pooled
## 1       23000    40846.73    99  412.5933  20.31239
## 2       24000  1351941.68   667 2026.8991  45.02110
## 3       25000   934566.13   631 1481.0874  38.48490
## 4       26000 16376572.57 20813  786.8434  28.05073
## 5       27000  9357584.58  7166 1305.8309  36.13628
## 6       28000  8410659.15  9104  923.8422  30.39477
## 7       29000  1409909.69   583 2418.3700  49.17693&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# However, we could also calculate our sd_pooled as below and get the same results :
sd_pooled &amp;lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$ControlRate),
         function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we calculate Mean (Mean of Means) from the merged table &lt;code&gt;Mean_SD_Count.dat&lt;/code&gt;, so we can calculate Cohen’s d and RequiredReplicates for all four field combined.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ds.Mean &amp;lt;-
  setNames(aggregate(
  Mean_SD_Count.dat$Mean,
  by = list(Mean_SD_Count.dat$ControlRate),
  FUN = mean
  ),
  c(&amp;quot;ControlRate&amp;quot;, &amp;quot;Mean&amp;quot;))

ds &amp;lt;- merge(ds, ds.Mean, by.x = &amp;quot;ControlRate&amp;quot;)
ds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   ControlRate          ss    df sd.square SD_pooled     Mean
## 1       23000    40846.73    99  412.5933  20.31239 108.5777
## 2       24000  1351941.68   667 2026.8991  45.02110 142.3585
## 3       25000   934566.13   631 1481.0874  38.48490 181.5407
## 4       26000 16376572.57 20813  786.8434  28.05073 226.4716
## 5       27000  9357584.58  7166 1305.8309  36.13628 234.8808
## 6       28000  8410659.15  9104  923.8422  30.39477 241.7849
## 7       29000  1409909.69   583 2418.3700  49.17693 221.5782&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we calculate the Effect Size and Cohen’s D for the combined 4 plots using mean yield and sd pooled for different ControlRate&lt;/p&gt;
&lt;p&gt;Now we calculate the Effect Size and Cohen’s D for the combined 4 plots using mean yield and sd pooled for different ControlRate usinf our function &lt;code&gt;calculate_field_mean_SD_and_get_RR_EffSize&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RequiredReplicates_for_all_fields &amp;lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = ds, intervals = ControlRateInterval, Single.Field.Analysis = FALSE
    )

RequiredReplicates_for_all_fields&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &amp;quot;23000 Vs 24000&amp;quot; &amp;quot;0.967241180364881&amp;quot; &amp;quot;17&amp;quot;              
## [2,] &amp;quot;24000 Vs 25000&amp;quot; &amp;quot;0.935567380483301&amp;quot; &amp;quot;18&amp;quot;              
## [3,] &amp;quot;25000 Vs 26000&amp;quot; &amp;quot;1.33427570681431&amp;quot;  &amp;quot;9&amp;quot;               
## [4,] &amp;quot;26000 Vs 27000&amp;quot; &amp;quot;0.259967383137317&amp;quot; &amp;quot;232&amp;quot;             
## [5,] &amp;quot;27000 Vs 28000&amp;quot; &amp;quot;0.206777477800031&amp;quot; &amp;quot;367&amp;quot;             
## [6,] &amp;quot;28000 Vs 29000&amp;quot; &amp;quot;0.49430440628565&amp;quot;  &amp;quot;64&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot this results for better visualization of pattern.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RequiredReplicates_for_all_fields &amp;lt;-
  as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field)
  
  RequiredReplicates_for_all_fields$Group &amp;lt;-
  factor(RequiredReplicates_for_all_fields$Group)
  
  RequiredReplicates_for_all_fields$RequiredReplicates &amp;lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates))
  
  RequiredReplicates_for_all_fields$EffectSize &amp;lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize))
  
  # Plot RequiredReplicates_for_all_fields
  ggplot(RequiredReplicates_for_all_fields,
  aes(x = EffectSize, y = RequiredReplicates)) +
  ggtitle(&amp;quot;Effect size Vs Required replicates for all fields data \n(Combined Effect)&amp;quot;) +
  geom_point(aes(shape = Group), size = 2)  +
  scale_shape_manual(values = 1:nlevels(EffectSize.Plot$Group)) +
  geom_line() +
  scale_x_continuous(&amp;quot;EffectSize&amp;quot;,
  breaks = c(
  0,
  0.2,
  0.5,
  1,
  max(RequiredReplicates_for_all_fields$EffectSize)
  ),
  labels = scaleFUN) +
  theme_bw() +
  theme(
  axis.title = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
  axis.text = element_text(size = 10),
  plot.title = element_text(hjust = 0.5, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Midterm_project_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the combined effect size (from all four fields), we can see that the Effect size is inversely proportional to Required Replicates. We also see that for the small effect of ~0.2, we need exponentially larger replicates, for medium effect (~0.5), we need about 80 replicates and for larger effect (more than 0.8), we only need about 20 or fewer replicates. Our analysis from all fields also indicates that we have highest mean yeild for controlRates of Seeding og 27000-29000.&lt;/p&gt;
&lt;p&gt;Final discussion and suggestion:&lt;/p&gt;
&lt;p&gt;Based on the results we saw above, if we take individual field (one at a time), we would need thousands of replicates for low effect size. However, for moderate and large effect size, the required replicates number signficantly decreases even if we use individual field data separately. On the other hand, when we combine all four fields, our required replictes decreases for all low, moderate and large effect sizes (Figure: Effect size Vs Required replicates for all fields data (Combined Effect)) as compared to individual fields (Figure: Effect size Vs Required replicates for each field data). I also found that the required replicates decreases for lower rates of seeding (control Rates of 23000 Vs 24000).
Based on these results, we can also conclude that the Required replicates is inversely proportional to the Effect size as seen in all four fields data and merged data.&lt;/p&gt;
&lt;p&gt;Aditionally, the mean yield significantly increases when we increase the seeding rates at control rates of 26000 to 28000 for each individual field. This was also consistent for combined field data (Figure: ControlRates Vs Mean yield for all four fields). So, my final suggestion for this analysis is that we should used all four fields to calculate required replicates and effect size, though we could still use single field data separately to derrive the conclusion that higher seeding rates (~26000-28000) has higher mean yield which was consistent in all four fields. However, making use of all four fields data is recommended.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Processing text</title>
      <link>/achalneupane.github.io/post/processing_text/</link>
      <pubDate>Sat, 15 Jun 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/processing_text/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;There are six exercises below. You are required to provide solutions for at least four of the six. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve five of the sixth problems, mixing the languages as you wish.&lt;/p&gt;
&lt;p&gt;If you choose SAS for an exercise, you may use &lt;code&gt;IML&lt;/code&gt;, &lt;code&gt;DATA&lt;/code&gt; operations or &lt;code&gt;PROC SQL&lt;/code&gt; at your discretion.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Warning&lt;/em&gt; I will continue restricting the use of external libraries in R, particularly &lt;code&gt;tidyverse&lt;/code&gt; libraries. You may choose to use &lt;code&gt;ggplot2&lt;/code&gt;, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the midterm and final projects.&lt;/p&gt;
&lt;div id=&#34;reuse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reuse&lt;/h2&gt;
&lt;p&gt;For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Functions:
# Ex 1
collapse_df &amp;lt;- function(x) {
createPattern &amp;lt;- function(n) {
sprintf(&amp;quot;(%s[^,]+),&amp;quot;, strrep(&amp;quot;[^,]+,&amp;quot;, n - 1))
}

mystring &amp;lt;-
do.call(paste, c(as.list(colnames(x)), sep = &amp;quot;,&amp;quot;, do.call(paste, c(x, sep =
&amp;quot;,&amp;quot;))))
my_pattern &amp;lt;- createPattern(ncol(x))
gsub(my_pattern, &amp;quot;\\1\n&amp;quot;, paste(mystring, collapse = &amp;quot;,&amp;quot;))
}

# Exercise 4
reformat &amp;lt;- function(x) {
x &amp;lt;-  unlist(strsplit(x, split = &amp;#39;|&amp;#39;, fixed = TRUE))
x &amp;lt;- gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, x)[-1]
return(as.numeric(x))
}

# Exercise 5
# Function to calculate R-square
lm_eqn &amp;lt;- function(df) {
m &amp;lt;- lm(Weight2019 ~ Weight2015, df)

eq &amp;lt;-
substitute(
italic(Weight2019) == a + b %.% italic(Weight2015) * &amp;quot;,&amp;quot; ~  ~ italic(r) ^
2 ~ &amp;quot;=&amp;quot; ~ r2,
list(
a = format(unname(coef(m)[1]), digits = 2),
b = format(unname(coef(m)[2]), digits = 2),
r2 = format(summary(m)$r.squared, digits = 3)
)
)
as.character(as.expression(eq))

}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1.&lt;/h1&gt;
&lt;p&gt;Write a loop or a function to convert a matrix to a &lt;code&gt;CSV&lt;/code&gt; compatible string. Given a matrix of the form&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;C1&lt;/th&gt;
&lt;th&gt;C2&lt;/th&gt;
&lt;th&gt;C3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;e&lt;/td&gt;
&lt;td&gt;f&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;g&lt;/td&gt;
&lt;td&gt;h&lt;/td&gt;
&lt;td&gt;i&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;produce a string of the form&lt;/p&gt;
&lt;p&gt;&lt;code&gt;a,b,c\nd,e,f\ng,h,i&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;\n&lt;/code&gt; is the newline character.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my.dat &amp;lt;-
  setNames(as.data.frame(matrix(
  letters[1:9], ncol = 3, byrow = TRUE
  )), c(&amp;quot;C1&amp;quot;, &amp;quot;C2&amp;quot;, &amp;quot;C3&amp;quot;))

  collapse_df(my.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;C1,C2,C3\na,b,c\nd,e,f\ng,h,i&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are only required to convert a matrix to CSV format, but you may choose to write code to convert data tables to CSV; in this case, include column names in the output string. Use &lt;code&gt;NATR332.DAT&lt;/code&gt; as a test case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NATR332.DAT &amp;lt;- data.frame(
  Y1 = c(146,141,135,142,140,143,138,137,142,136),
  Y2 = c(141,143,139,139,140,141,138,140,142,138)
)

# Test our function
collapse_df(NATR332.DAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Y1,Y2\n146,141\n141,143\n135,139\n142,139\n140,140\n143,141\n138,138\n137,140\n142,142\n136,138&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you choose SAS, I’ve include the NATR332 data table and framework code for IML in the template. I used the &lt;code&gt;CATX&lt;/code&gt; function in IML. I found I could do this in one line in R, with judicious use of &lt;code&gt;apply&lt;/code&gt;, but I haven’t found the equivalent in IML. Instead, I used a pair of nested loops to “accumulate” an increasingly longer string.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2.&lt;/h1&gt;
&lt;p&gt;Create an ordered treatment pairs table from the pumpkin data, as described in Homework 7. Before printing the table, iterate over each row to create a vector of row names that are more descriptive. First, use &lt;code&gt;levels&lt;/code&gt; to get the text associated with each &lt;code&gt;Class&lt;/code&gt;, then combine the &lt;code&gt;Class&lt;/code&gt; text to create a row name of the form:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Blue vs Cinderella&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(where &lt;code&gt;Blue&lt;/code&gt; is the Class description for class 1, &lt;code&gt;Cinderella&lt;/code&gt; is the description for class 2. This text should be the row name in the row corresponding to &lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j=2\)&lt;/span&gt;). You may choose to add a column with the specified descriptions, if you wish, but this must be the first column of the printed table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins.dat = read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv&amp;quot;,
  header = T,
  sep = &amp;quot;,&amp;quot;
)


# There are 4 classes, so 4 choose 2:
class.level &amp;lt;- levels(pumpkins.dat$Class)
comb.matrix &amp;lt;- setNames(as.data.frame(t(combn(4,2))), c(&amp;quot;CLASSi&amp;quot;, &amp;quot;CLASSj&amp;quot;))

# Now we loop over the class level indices to get the row names
for (i in 1:nrow(comb.matrix)) {
  rownames(comb.matrix)[i] &amp;lt;-
    paste(class.level[comb.matrix$CLASSi[i]], class.level[comb.matrix$CLASSj[i]], sep = &amp;quot; Vs &amp;quot;)
}

# This is how the combination matrix would look like
comb.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      CLASSi CLASSj
## Blue Vs Cinderella        1      2
## Blue Vs Howden            1      3
## Blue Vs Pie               1      4
## Cinderella Vs Howden      2      3
## Cinderella Vs Pie         2      4
## Howden Vs Pie             3      4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# In addition to this, if we also want to see actual class levels on the same matrix:
for (i in 1:length(comb.matrix$V1)) {
comb.matrix$CLASSi.Names[i] &amp;lt;-   class.level[comb.matrix$CLASSi[i]]
comb.matrix$col2j.Names[i] &amp;lt;-   class.level[comb.matrix$CLASSj[i]]
}

comb.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      CLASSi CLASSj CLASSi.Names col2j.Names
## Blue Vs Cinderella        1      2         Blue  Cinderella
## Blue Vs Howden            1      3         Blue  Cinderella
## Blue Vs Pie               1      4         Blue  Cinderella
## Cinderella Vs Howden      2      3         Blue  Cinderella
## Cinderella Vs Pie         2      4         Blue  Cinderella
## Howden Vs Pie             3      4         Blue  Cinderella&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3.&lt;/h1&gt;
&lt;p&gt;Calculate MSW, MSB, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for the data from Wansink Table 1 (Homework 4, Exercise 6) where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSB = \frac{\sum_i n_i(x_i-\bar{x})^2}{k-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSW =  \frac{\sum_i (n_i-1)s_i^2}{N-k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Start with the strings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Means &amp;lt;- &amp;quot;268.1 271.1 280.9 294.7 285.6 288.6 384.4&amp;quot;
StandardDeviations &amp;lt;- &amp;quot;124.8 124.2 116.2 117.7 118.3 122.0 168.3&amp;quot;
SampleSizes &amp;lt;- &amp;quot;18 18 18 18 18 18 18&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tokenize the strings, then convert the tokens to a create vectors of numeric values. Use these vectors to compute and print &lt;span class=&#34;math inline&#34;&gt;\(MSW\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(MSB\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Means&amp;lt;-strsplit(Means, split = &amp;quot; &amp;quot;)
x&amp;lt;-as.numeric(Means[[1]])
Sd&amp;lt;- strsplit(StandardDeviations, split = &amp;quot; &amp;quot;)
s&amp;lt;-as.numeric(Sd[[1]])
n&amp;lt;-strsplit(SampleSizes, split= &amp;quot; &amp;quot;)
n&amp;lt;-as.numeric(n[[1]])
k&amp;lt;-length(n)
N&amp;lt;-18*k

## MSB iteration  
sum.x=0
for (i in 1:k) {
  sum.x=sum.x + x[i]
}
x_mean=sum.x/k

ss.x=0 
for (i in 1:k){
    ss.x=ss.x + n[i]*(x[i] - x_mean)^2
}
MSB=ss.x/(k-1)
MSB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 28815.96&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## MSW iteration  
ss.w=0 
for (i in 1:k){
  ss.w=ss.w + ((n[i]-1)*(s[i])^2)
}
MSW=ss.w/(N-k)
MSW&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16508.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F_ratio = MSB/MSW
F_ratio&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.745512&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;P_value = pf(F_ratio, df1=k-1, df2=N-k, lower.tail=FALSE)
P_value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1163133&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use SAS, I’ve provided macro variables that can be tokenized in either macro language or using SAS functions. You can mix and match macro, DATA, IML or SQL processing as you wish, but you must write code to convert the text into numeric tokens before processing.&lt;/p&gt;
&lt;p&gt;Compare your results from previous homework, or to the resource given in previous homework, to confirm that the text was correctly converted to numeric values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4.&lt;/h1&gt;
&lt;p&gt;Repeat the regression analysis from Homework 4, Exercise 5, but start with the text&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Rate &amp;lt;- &amp;quot;Rate   | 23000   | 24000   | 25000   | 26000    | 27000   | 28000   | 29000&amp;quot;
Yield &amp;lt;- &amp;quot;Yield  | 111.4216 | 155.0326 | 181.1176 | 227.5800 | 233.4623 | 242.1753 | 231.3890&amp;quot;
Rate &amp;lt;- reformat(Rate)
Yield &amp;lt;- reformat(Yield)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that by default, &lt;code&gt;strsplit&lt;/code&gt; in R will read &lt;code&gt;split&lt;/code&gt; as a regular expression, and &lt;code&gt;|&lt;/code&gt; is a special character in regular expressions. You will need to change one of the default parameters for this exercise.&lt;/p&gt;
&lt;p&gt;Tokenize these strings and convert to numeric vectors, then use these vectors to define&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y =\left( \begin{array}{c}
111.4216 \\
155.0326 \\
\vdots \\
231.3890 
 \end{array}\right) 
 =
 \left(\begin{array}{rr}
 1 &amp;amp; 23000 \\
 1 &amp;amp; 24000  \\
\vdots &amp;amp; \vdots \\
 1 &amp;amp; 29000
 \end{array}\right) 
 \left(\begin{array}{c}
 \beta_1 \\
 \beta_2
 \end{array}\right)^t = \mathbf{X} \mathbf{\beta}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Solve for and print &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you use SAS, I’ve provided macro variables that can be tokenized in either macro language or using SAS functions. You can mix and match macro, DATA, IML or SQL processing as you wish, but you must write code to convert the text into numeric tokens before processing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- as.vector( Yield)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 111.4216 155.0326 181.1176 227.5800 233.4623 242.1753 231.3890&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#creating a matrix for bias term
bias=rep(1:1, length.out=length(y))
bias&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cx &amp;lt;- Rate
X=matrix(c(bias,cx), ncol = 2)
X&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]  [,2]
## [1,]    1 23000
## [2,]    1 24000
## [3,]    1 25000
## [4,]    1 26000
## [5,]    1 27000
## [6,]    1 28000
## [7,]    1 29000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#multplication of transpose of x and x
tX=t(X)
tX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]
## [1,]     1     1     1     1     1     1     1
## [2,] 23000 24000 25000 26000 27000 28000 29000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Xm=tX%*%X
Xm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]     [,2]
## [1,]      7 1.82e+05
## [2,] 182000 4.76e+09&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A=solve(Xm)

hat.beta=A%*%(tX%*%y)
hat.beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               [,1]
## [1,] -347.18307857
## [2,]    0.02094758&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare your results from previous homework, or to the resource given in previous homework, to confirm that the text was correctly converted to numeric values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5.&lt;/h1&gt;
&lt;p&gt;Use the file &lt;code&gt;openmat2015.csv&lt;/code&gt; from D2L. These data are from &lt;a href=&#34;https://news.theopenmat.com/high-school-wrestling/high-school-wrestling-rankings/final-2015-clinch-gear-national-high-school-wrestling-individual-rankings/57136&#34; class=&#34;uri&#34;&gt;https://news.theopenmat.com/high-school-wrestling/high-school-wrestling-rankings/final-2015-clinch-gear-national-high-school-wrestling-individual-rankings/57136&lt;/a&gt;. This is a list of top-ranked high school wrestlers in 2015, their high &lt;code&gt;School&lt;/code&gt;, &lt;code&gt;Weight&lt;/code&gt; class and in some cases the &lt;code&gt;College&lt;/code&gt; where they expected to enroll and compete after high school.&lt;/p&gt;
&lt;p&gt;We wish to know how many went on to compete in the national championship in 2019, so we will merge this table with the data from Homework 7, &lt;code&gt;ncaa2019.csv&lt;/code&gt;. The &lt;code&gt;openmat2015.csv&lt;/code&gt; data contains only a single column, &lt;code&gt;Name&lt;/code&gt;. You will need to split the text in this column to create the columns &lt;code&gt;First&lt;/code&gt; and &lt;code&gt;Last&lt;/code&gt; required to merge with &lt;code&gt;ncaa2019.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do not print these tables in the submitted work&lt;/strong&gt; Instead, print a contingency table comparing &lt;code&gt;Weight&lt;/code&gt; for 2015 and &lt;code&gt;Weight&lt;/code&gt; for 2019. What is the relationship between high school and college weight classes? You may instead produce a scatter plot or box-whisker plot, using high school weight class as the independent variable.&lt;/p&gt;
&lt;p&gt;If you do this in SAS, use the &lt;code&gt;openmat2015SAS.csv&lt;/code&gt; file, it will import &lt;code&gt;College&lt;/code&gt; correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;openmat2015 &amp;lt;-
  read.delim2(
    &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/openmat2015.csv&amp;quot;,
    header = T,
    sep = &amp;quot;,&amp;quot;
  )
openmat2015$Weight2015 &amp;lt;- openmat2015$Weight

openmat2015$First &amp;lt;- sapply(strsplit(as.character(openmat2015$Name),&amp;#39; &amp;#39;), &amp;quot;[&amp;quot;, 1)
# sapply(strsplit(as.character(openmat2015$Name),&amp;#39; &amp;#39;), function(x){x[1]})
openmat2015$Last &amp;lt;- sapply(strsplit(as.character(openmat2015$Name),&amp;#39; &amp;#39;), &amp;quot;[&amp;quot;, 2)
# sapply(strsplit(as.character(openmat2015$Name),&amp;#39; &amp;#39;), function(x){x[2]})
head(openmat2015)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weight Rank              Name Year           School State College Previous
## 1    106    1       Cade Olivas  Fr.   St. John Bosco    CA                1
## 2    106    2 Roman Bravo-Young  Fr.        Sunnyside    AZ                2
## 3    106    3    Gavin Teasdale  Fr. Jefferson-Morgan    PA                3
## 4    106    4       Drew Mattin  So.            Delta    OH                5
## 5    106    5        Real Woods  Fr. Montini Catholic    IL                6
## 6    106    6     Jacori Teemer  Fr.       Long Beach    NY                7
##   Weight2015  First        Last
## 1        106   Cade      Olivas
## 2        106  Roman Bravo-Young
## 3        106  Gavin    Teasdale
## 4        106   Drew      Mattin
## 5        106   Real       Woods
## 6        106 Jacori      Teemer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncaa2019.dat = read.table(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2019.csv&amp;quot;,
  header = T,
  sep = &amp;quot;,&amp;quot;
)
ncaa2019.dat$Weight2019 &amp;lt;- ncaa2019.dat$Weight

head(ncaa2019.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weight       Last     First Finish Weight2019
## 1    125        Lee   Spencer      1        125
## 2    125    Mueller      Jack      2        125
## 3    125     Rivera Sebastian      3        125
## 4    125     Arujau    Vitali      4        125
## 5    125 Piccininni  Nicholas      5        125
## 6    125      Glory       Pat      6        125&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged.dat &amp;lt;- merge(x = openmat2015, y = ncaa2019.dat, by = c(&amp;quot;First&amp;quot;, &amp;quot;Last&amp;quot;), all = TRUE)
head(merged.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   First    Last Weight.x Rank          Name Year        School State
## 1  A.C. Headlee      132    3  A.C. Headlee  Sr.    Waynesburg    PA
## 2 Aaron Burkett      106   18 Aaron Burkett  Jr. Chesnut Ridge    PA
## 3  Adam  Hudson      160   17   Adam Hudson  Sr.   Shelbyville    IL
## 4    AJ Nevills       NA   NA          &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;
## 5    Al Beattie      285   15    Al Beattie  Sr.       Burrell    PA
## 6  Alan    Hart      113   20     Alan Hart  So.        Edward    OH
##          College Previous Weight2015 Weight.y  Finish Weight2019
## 1 North Carolina        4        132       NA    &amp;lt;NA&amp;gt;         NA
## 2                      NR        106       NA    &amp;lt;NA&amp;gt;         NA
## 3                      NR        160       NA    &amp;lt;NA&amp;gt;         NA
## 4           &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         NA      285 cons 32        285
## 5 Pitt Johnstown       18        285       NA    &amp;lt;NA&amp;gt;         NA
## 6                      17        113       NA    &amp;lt;NA&amp;gt;         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged.dat &amp;lt;- merged.dat[!is.na(merged.dat$Weight2015) &amp;amp; !is.na(merged.dat$Weight2019),]


weight_contingency &amp;lt;-
  table(
    merged.dat$Weight2015,
    merged.dat$Weight2019,
    dnn = c(&amp;quot;Weight for 2015&amp;quot;, &amp;quot;Weight for 2019&amp;quot;)
  )

weight_contingency&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Weight for 2019
## Weight for 2015 125 133 141 149 157 165 174 184 197 285
##             106   2   1   0   0   1   0   0   0   0   0
##             113   1   1   1   0   0   0   0   0   0   0
##             120   5   3   1   1   0   0   0   0   0   0
##             126   0   1   1   0   0   0   0   0   0   0
##             132   2   2   4   1   0   0   0   0   0   0
##             138   0   0   2   1   1   0   0   0   0   0
##             145   0   0   1   1   4   0   0   0   0   0
##             152   0   0   0   1   1   2   2   0   0   0
##             160   0   0   0   0   0   4   4   0   0   0
##             170   0   0   0   0   0   2   2   2   0   0
##             182   0   0   0   0   0   0   2   2   3   1
##             195   0   0   0   0   0   0   0   2   1   1
##             220   0   0   0   0   0   0   0   0   0   5
##             285   0   0   0   0   0   0   0   0   0   1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
p &amp;lt;- ggplot(merged.dat, aes(x=Weight2015, y=Weight2019)) +
  geom_point( size=1, shape=21, fill=&amp;quot;white&amp;quot;) 
  # geom_abline()


# Now we paste the R-square value to our relationship plot
p + geom_text(x = 150, y = 300, label = lm_eqn(merged.dat), parse = TRUE) +
  xlab(&amp;quot;HighSchool Weight&amp;quot;) +
  ylab(&amp;quot;College Weight&amp;quot;) + 
  geom_smooth(method = &amp;#39;lm&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Processing_Text_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The relationship between highschool weight and college weight is almost linear.
# Exercise 6&lt;/p&gt;
&lt;p&gt;Use file &lt;code&gt;openmat2015.csv&lt;/code&gt; from Exercise 6, and use partial text matching to answer these questions. To show your results, print only the rows from the table that match the described text patterns, but to save space, print only &lt;code&gt;Name&lt;/code&gt;, &lt;code&gt;School&lt;/code&gt; and &lt;code&gt;College&lt;/code&gt;. Each of these can be answered in a single step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;openmat2015 &amp;lt;-
  read.delim2(
  &amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/openmat2015.csv&amp;quot;,
  header = T,
  sep = &amp;quot;,&amp;quot;
  )

# head(openmat2015)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Which wrestlers come from a &lt;code&gt;School&lt;/code&gt; with a name starting with &lt;code&gt;St.&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;openmat2015[grepl(&amp;quot;St.&amp;quot;, openmat2015$School), c(&amp;quot;Name&amp;quot;, &amp;quot;School&amp;quot;, &amp;quot;College&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  Name                  School        College
## 1         Cade Olivas          St. John Bosco               
## 17        John Tropea     St. Joseph Montvale               
## 30        Mitch Moore        St. Paris Graham               
## 37         Joey Prata       St. Christopher&amp;#39;s               
## 50       Eli Stickley        St. Paris Graham      Wisconsin
## 64     Mitchell McKee St. Michael-Albertville  Minnesota &amp;#39;16
## 67        Eli Seipel         St. Paris Graham     Pittsburgh
## 76       Ben Lamantia            St. Anthonys       Michigan
## 82         Kaid Brock              Stillwater Oklahoma State
## 94    Austin O&amp;#39;Connor                St. Rita               
## 99    Hunter Ladnier               St. Edward               
## 128       Brent Moore        St. Paris Graham               
## 134     Tristan Moran              Stillwater Oklahoma State
## 153       Kyle Lawson        St. Paris Graham               
## 161    Alex Marinelli        St. Paris Graham       Iowa &amp;#39;16
## 182  Anthony Valencia          St. John Bosco  Arizona State
## 183       Logan Massa               St. Johns       Michigan
## 185         Joe Smith              Stillwater Oklahoma State
## 201    Zahid Valencia          St. John Bosco  Arizona State
## 217     Jordan Joseph St. Michael-Albertville               
## 251 Christian Colucci        St. Peter&amp;#39;s Prep         Lehigh
## 255   Ian Butterbrodt          St. Johns Prep          Brown&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Which wrestlers were intending to attend an Iowa &lt;code&gt;College&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;openmat2015[grepl(&amp;quot;Iowa&amp;quot;, openmat2015$College), c(&amp;quot;Name&amp;quot;, &amp;quot;School&amp;quot;, &amp;quot;College&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Name                 School       College
## 21     Justin Mejia                 Clovis      Iowa &amp;#39;17
## 24   Jason Renteria  Oak Park-River Forest      Iowa &amp;#39;17
## 65   Markus Simmons           Broken Arrow    Iowa State
## 121 Michael Kemerer      Franklin Regional          Iowa
## 122     Max Thomsen        Union Community Northern Iowa
## 155     Kaleb Young           Punxsutawney      Iowa &amp;#39;16
## 161  Alex Marinelli       St. Paris Graham      Iowa &amp;#39;16
## 166   Bryce Steiert     Waverly-Shell Rock Northern Iowa
## 176     Paden Moore Jackson County Central Northern Iowa
## 194   Isaiah Patton       Dowling Catholic Northern Iowa
## 196 Jacob Holschlag                  Union Northern Iowa
## 197 Colston DiBlasi              Park Hill    Iowa State
## 204    Taylor Lujan             Carrollton Northern Iowa
## 233     Cash Wilcke                OA-BCIG          Iowa
## 244    Ryan Parmely       Maquoketa Valley    Upper Iowa&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Which wrestlers were intending to start &lt;code&gt;College&lt;/code&gt; in 2016 or 2017 (&lt;code&gt;College&lt;/code&gt; will end with 16 or 17)?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;openmat2015[grepl(&amp;quot;16$|17$&amp;quot;, openmat2015$College), c(&amp;quot;Name&amp;quot;, &amp;quot;School&amp;quot;, &amp;quot;College&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Name                  School           College
## 21    Justin Mejia                  Clovis          Iowa &amp;#39;17
## 24  Jason Renteria   Oak Park-River Forest          Iowa &amp;#39;17
## 45   Kyle Norstrem                 Brandon Virginia Tech &amp;#39;16
## 46    Jack Mueller        Wyoming Seminary      Virginia &amp;#39;16
## 51      Ty Agaisse               Delbarton     Princeton &amp;#39;16
## 64  Mitchell McKee St. Michael-Albertville     Minnesota &amp;#39;16
## 126  Hayden Hidlay          Mifflin County      NC State &amp;#39;16
## 145   Jake Wentzel              South Park          Pitt &amp;#39;16
## 155    Kaleb Young            Punxsutawney          Iowa &amp;#39;16
## 161 Alex Marinelli        St. Paris Graham          Iowa &amp;#39;16
## 186    Nick Reenan        Wyoming Seminary  Northwestern &amp;#39;16&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Which wrestlers are intending compete in a sport other than wrestling? (look for a sport in parenthesis in the &lt;code&gt;College&lt;/code&gt; column. Note - &lt;code&gt;(&lt;/code&gt; is a special character in regular expressions, so to match the exact character, it needs to be preceded by the escape character &lt;code&gt;\&lt;/code&gt;. However, &lt;code&gt;\&lt;/code&gt; in strings is a special character, so itself must be preceded by the escape character.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# all.sports.brackets &amp;lt;-
#   openmat2015[grepl(&amp;quot;\\(&amp;quot;, openmat2015$College), ]

openmat2015[grepl(&amp;quot;\\(&amp;quot;, openmat2015$College), ][!grepl(&amp;quot;Wrestling&amp;quot;, openmat2015[grepl(&amp;quot;\\(&amp;quot;, openmat2015$College), ]$College, ignore.case = TRUE), c(&amp;quot;Name&amp;quot;, &amp;quot;School&amp;quot;, &amp;quot;College&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Name           School                      College
## 218    Chase Osborn             Penn         Minnesota (Baseball)
## 225   Tevis Barlett    Cheyenne East              Washington (FB)
## 230     Jan Johnson Governor Mifflin                    Akron(FB)
## 261 Michael Johnson Montini Catholic              Yale (Football)
## 264   Gage Cervenka          Emerald           Clemson (Football)
## 267     Jake Marnin   Southeast Polk Southern Illinois (Football)
## 277     Que Overton            Jenks          Oklahoma (Football)
## 279  Norman Oglesby   Benjamin Davis        Cincinnati (Football)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data manipulation</title>
      <link>/achalneupane.github.io/post/data_manipulation/</link>
      <pubDate>Wed, 12 Jun 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/data_manipulation/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;There are six exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve five of the sixth problems, mixing the languages as you wish.&lt;/p&gt;
&lt;p&gt;If you choose SAS for an exercise, you may use &lt;code&gt;IML&lt;/code&gt;, &lt;code&gt;DATA&lt;/code&gt; operations or &lt;code&gt;PROC SQL&lt;/code&gt; at your discretion.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Warning&lt;/em&gt; I will continue restricting the use of external libraries in R, particularly &lt;code&gt;tidyverse&lt;/code&gt; libraries. You may choose to use &lt;code&gt;ggplot2&lt;/code&gt;, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the midterm and final projects.&lt;/p&gt;
&lt;div id=&#34;reuse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reuse&lt;/h2&gt;
&lt;p&gt;For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1.&lt;/h1&gt;
&lt;div id=&#34;part-a.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Repeat the table from Homework 5, Exercise 2. The table will contain 30 rows, each corresponding to a unique combination of CV from &lt;span class=&#34;math inline&#34;&gt;\(8, 12, ..., 28\)&lt;/span&gt; and Diff from &lt;span class=&#34;math inline&#34;&gt;\(5,10, ... , 25\)&lt;/span&gt;. However, for this exercise you only need to calculate one column for required replicates (&lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta=0.2\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;Define the table in the space below. &lt;em&gt;Do not print this table&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata.dat &amp;lt;- data.frame(CV = rep(seq(8,28,4), each = 5), Diff = rep(seq(5,25,5),6))
newdata.dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CV Diff
## 1   8    5
## 2   8   10
## 3   8   15
## 4   8   20
## 5   8   25
## 6  12    5
## 7  12   10
## 8  12   15
## 9  12   20
## 10 12   25
## 11 16    5
## 12 16   10
## 13 16   15
## 14 16   20
## 15 16   25
## 16 20    5
## 17 20   10
## 18 20   15
## 19 20   20
## 20 20   25
## 21 24    5
## 22 24   10
## 23 24   15
## 24 24   20
## 25 24   25
## 26 28    5
## 27 28   10
## 28 28   15
## 29 28   20
## 30 28   25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combined &amp;lt;- function (cv, percent.diff, alpha = 0.05, beta = 0.2){
  cv &amp;lt;- cv
  percent.diff &amp;lt;- percent.diff
  n &amp;lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  n &amp;lt;- round(n,0)
  value &amp;lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0))
  return(value)
}
value &amp;lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff)

# Adding RequiredReplicates column
newdata.dat$RequiredReplicates &amp;lt;- value$RequiredReplicates
# Print the table
# newdata.dat&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Create two subset tables, one that contains the combinations of CV and Diff that require the five largest number of replicates and one the contains the combinations of CV and Diff the five smallest number of replicates. You can determine the subset by ranking or sorting by required replicates. You can add a rank column to your table if you wish. Call one table &lt;code&gt;LargestFive&lt;/code&gt; and one table &lt;code&gt;SmallestFive&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sort the table by decreasing Replicate values
ordered.new.dat &amp;lt;- newdata.dat[order(newdata.dat$RequiredReplicates,decreasing = TRUE),]

# Creating the required subset tables
LargestFive &amp;lt;- head(ordered.new.dat, 5)


# Now for smallest
SmallestFive &amp;lt;- tail(ordered.new.dat, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Print &lt;code&gt;LargestFive&lt;/code&gt; sorted by required replicates in descending order, and print &lt;code&gt;SmallestFive&lt;/code&gt; in ascending order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adding rank for LargestFive table. Largest value gets rank 1
LargestFive$Rank &amp;lt;- rank(order(LargestFive$RequiredReplicates, decreasing = TRUE))
# printing larget five in descending order, Largest value gets the first rank
LargestFive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CV Diff RequiredReplicates Rank
## 26 28    5                492    1
## 21 24    5                362    2
## 16 20    5                251    3
## 11 16    5                161    4
## 27 28   10                123    5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adding rank for SmallestFive table. Smallest value gets rank 1
SmallestFive &amp;lt;- SmallestFive[order(SmallestFive$RequiredReplicates, decreasing = FALSE),]
SmallestFive$Rank &amp;lt;- rank((SmallestFive$RequiredReplicates),ties.method=&amp;quot;min&amp;quot;)
# printing smallest five in ascending order, smallest value gets the first rank
SmallestFive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CV Diff RequiredReplicates Rank
## 5   8   25                  2    1
## 4   8   20                  3    2
## 3   8   15                  4    3
## 10 12   25                  4    3
## 15 16   25                  6    5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2&lt;/h1&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a&lt;/h3&gt;
&lt;p&gt;Go to &lt;a href=&#34;http://www.itl.nist.gov/div898/strd/anova/SiRstv.html&#34; class=&#34;uri&#34;&gt;http://www.itl.nist.gov/div898/strd/anova/SiRstv.html&lt;/a&gt; and use the data listed under &lt;code&gt;Data File in Table Format&lt;/code&gt; (&lt;a href=&#34;https://www.itl.nist.gov/div898/strd/anova/SiRstvt.dat&#34; class=&#34;uri&#34;&gt;https://www.itl.nist.gov/div898/strd/anova/SiRstvt.dat&lt;/a&gt;). You may reuse the file from Homework 6. Load the data into a table below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SiRstvt.dat &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/SiRstvt.dat&amp;quot;,
                          header = FALSE, skip = 59)
SiRstvt.dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         V1       V2       V3       V4       V5
## 1 196.3052 196.3042 196.1303 196.2795 196.2119
## 2 196.1240 196.3825 196.2005 196.1748 196.1051
## 3 196.1890 196.1669 196.2889 196.1494 196.1850
## 4 196.2569 196.3257 196.0343 196.1485 196.0052
## 5 196.3403 196.0422 196.1811 195.9885 196.2090&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;Reshape or transpose this table from the wide format to the long format. Make sure the resulting table has two columns - &lt;code&gt;Resistance&lt;/code&gt; and &lt;code&gt;Instrument&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.SiRstvt.dat &amp;lt;- with(reshape(SiRstvt.dat, idvar = &amp;quot;id&amp;quot;, varying = list(1:5),
             v.names = &amp;quot;Resistance&amp;quot;, timevar = &amp;quot;Instrument&amp;quot;, direction = &amp;quot;long&amp;quot;), 
             data.frame(Instrument, Resistance))

# the desired table in two column format:
long.SiRstvt.dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Instrument Resistance
## 1           1   196.3052
## 2           1   196.1240
## 3           1   196.1890
## 4           1   196.2569
## 5           1   196.3403
## 6           2   196.3042
## 7           2   196.3825
## 8           2   196.1669
## 9           2   196.3257
## 10          2   196.0422
## 11          3   196.1303
## 12          3   196.2005
## 13          3   196.2889
## 14          3   196.0343
## 15          3   196.1811
## 16          4   196.2795
## 17          4   196.1748
## 18          4   196.1494
## 19          4   196.1485
## 20          4   195.9885
## 21          5   196.2119
## 22          5   196.1051
## 23          5   196.1850
## 24          5   196.0052
## 25          5   196.2090&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c&lt;/h3&gt;
&lt;p&gt;To confirm that the table was reshaped correctly, use &lt;code&gt;aggregate&lt;/code&gt; or &lt;code&gt;tapply&lt;/code&gt; to calculate mean &lt;code&gt;Resistance&lt;/code&gt; grouped by &lt;code&gt;Instrument&lt;/code&gt; from the long table, and use &lt;code&gt;apply&lt;/code&gt; or &lt;code&gt;colMeans&lt;/code&gt; to calculate column means from the wide table. Print and compare the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Resistance mean grouped by Instrument from long format
tapply(long.SiRstvt.dat$Resistance, long.SiRstvt.dat$Instrument, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5 
## 196.2431 196.2443 196.1670 196.1481 196.1432&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregate(long.SiRstvt.dat$Resistance, by = list(long.SiRstvt.dat$Instrument), mean)

# Resistance mean from wide format
colMeans(SiRstvt.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       V1       V2       V3       V4       V5 
## 196.2431 196.2443 196.1670 196.1481 196.1432&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the reshaped table should be equivalent to the file linked under ‘Data File in Two-Column Format’.&lt;/p&gt;
&lt;p&gt;Yes, the reshaped table is equivalent to the linked data file in Two-column format&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;p&gt;Create an ordered treatment pairs table from the &lt;code&gt;pumpkin.csv&lt;/code&gt;. In the submitted work print the table only once at the end of the exercise.&lt;/p&gt;
&lt;div id=&#34;part-a.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Read the pumpkin data and compute mean &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt;, standard deviation &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt; and count &lt;span class=&#34;math inline&#34;&gt;\(n_i\)&lt;/span&gt; for each level &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; of &lt;code&gt;Class&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins.dat =read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv&amp;quot;,
                         header = T, sep = &amp;quot;,&amp;quot;)
# pumpkin.table &amp;lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class),
# FUN = function(x) c(Mean = mean(x), SD = sd(x), Count.n = length(x)))

pumpkin.mean &amp;lt;- setNames(aggregate(pumpkins.dat$Price, 
       by = list(pumpkins.dat$Class), FUN =  mean), c(&amp;quot;Class&amp;quot;, &amp;quot;Mean&amp;quot;))
pumpkin.sd &amp;lt;- setNames(aggregate(pumpkins.dat$Price, 
       by = list(pumpkins.dat$Class), FUN =  sd), c(&amp;quot;Class&amp;quot;, &amp;quot;SD&amp;quot;))
pumpkin.count &amp;lt;- setNames(aggregate(pumpkins.dat$Price, 
       by = list(pumpkins.dat$Class), FUN =  length), c(&amp;quot;Class&amp;quot;, &amp;quot;Counts&amp;quot;))


merged.class &amp;lt;- Reduce(function(x, y) merge(x, y, all=TRUE), 
                 list(pumpkin.mean, pumpkin.sd, pumpkin.count))

# This table includes mean, sd and counts for all Class
merged.class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Class     Mean        SD Counts
## 1       Blue 175.0000  0.000000      6
## 2 Cinderella 218.4286 17.924445      7
## 3     Howden 127.9000  3.695342     10
## 4        Pie 212.0000 18.565200      7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;Create a table over all possible pairs &lt;span class=&#34;math inline&#34;&gt;\(i,j\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; Classes from these data. Let one table column be &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and another column be &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, \dots (k-1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j = i+1, i+2, \dots k\)&lt;/span&gt;. There will be &lt;span class=&#34;math inline&#34;&gt;\((k \times (k-1))/2\)&lt;/span&gt; rows in this table. I usually create an empty table, then fill the table using a pair of nested loops, the outer loop over &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and the inner loop over &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Use a counter variable to keep track of the current row and increment the counter in each step of the inner loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# There are 4 classes, 4 choose 2:
class.level &amp;lt;- levels(pumpkins.dat$Class)
comb.matrix &amp;lt;- as.data.frame(t(combn(4,2)))
# This is how the combination matrix would look like
comb.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2
## 1  1  2
## 2  1  3
## 3  1  4
## 4  2  3
## 5  2  4
## 6  3  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating an empty matrix so we could fill in with the required values
collect &amp;lt;- data.frame(col1 = numeric(6), col2 = numeric(6))
for (i in 1:length(comb.matrix$V1)){
collect$col1[i] &amp;lt;-   class.level[comb.matrix$V1[i]]
collect$col2[i] &amp;lt;-   class.level[comb.matrix$V2[i]]
}
# The possible pairs table
collect&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         col1       col2
## 1       Blue Cinderella
## 2       Blue     Howden
## 3       Blue        Pie
## 4 Cinderella     Howden
## 5 Cinderella        Pie
## 6     Howden        Pie&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we merge the possible pairs (collect) table with the merged.class table with mean,
# sd and counts values we created above
collect &amp;lt;- merge(collect, merged.class, by.x = &amp;quot;col1&amp;quot;, by.y = &amp;quot;Class&amp;quot; )
collect &amp;lt;- merge(collect, merged.class, by.x = &amp;quot;col2&amp;quot;, by.y = &amp;quot;Class&amp;quot; )
collect&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         col2       col1   Mean.x      SD.x Counts.x   Mean.y      SD.y Counts.y
## 1 Cinderella       Blue 175.0000  0.000000        6 218.4286 17.924445        7
## 2     Howden       Blue 175.0000  0.000000        6 127.9000  3.695342       10
## 3     Howden Cinderella 218.4286 17.924445        7 127.9000  3.695342       10
## 4        Pie       Blue 175.0000  0.000000        6 212.0000 18.565200        7
## 5        Pie Cinderella 218.4286 17.924445        7 212.0000 18.565200        7
## 6        Pie     Howden 127.9000  3.695342       10 212.0000 18.565200        7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Calculate Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; for each Class pair in this table. Use a pooled standard deviation given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may add Class means to the table if you wish. Sort the table by &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; in descending order and print the table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cohen.d &amp;lt;- function(m1,s1,m2,s2){
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
return(cohens_d)
}


for(i in 1:nrow(collect)) {
  collect$cohens_d[i] &amp;lt;-
  cohen.d(
  m1 = collect$Mean.x[i],
  s1 = collect$SD.x[i],
  m2 = collect$Mean.y[i],
  s2 = collect$SD.y[i]
  )
}

collect &amp;lt;- collect[order(collect$cohens_d, decreasing = TRUE),]
# This  is the final table we need
collect&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         col2       col1   Mean.x      SD.x Counts.x   Mean.y      SD.y Counts.y
## 2     Howden       Blue 175.0000  0.000000        6 127.9000  3.695342       10
## 3     Howden Cinderella 218.4286 17.924445        7 127.9000  3.695342       10
## 6        Pie     Howden 127.9000  3.695342       10 212.0000 18.565200        7
## 1 Cinderella       Blue 175.0000  0.000000        6 218.4286 17.924445        7
## 4        Pie       Blue 175.0000  0.000000        6 212.0000 18.565200        7
## 5        Pie Cinderella 218.4286 17.924445        7 212.0000 18.565200        7
##     cohens_d
## 2 18.0252467
## 3  6.9954609
## 6  6.2831022
## 1  3.4264534
## 4  2.8184938
## 5  0.3522961&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4.&lt;/h1&gt;
&lt;div id=&#34;part-a.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Download the two files from D2L &lt;code&gt;ncaa2018.csv&lt;/code&gt; and &lt;code&gt;ncaa2019.csv&lt;/code&gt;, and read into data frames or tables. &lt;code&gt;ncaa2018.csv&lt;/code&gt; comes from the same source as &lt;code&gt;elo.csv&lt;/code&gt; from Homework 5, while &lt;code&gt;ncaa2019.csv&lt;/code&gt; is the corresponding more recent data. These tables do not contain identical sets of columns, but we will be able to merge &lt;code&gt;Finish&lt;/code&gt; by individual wrestlers. &lt;em&gt;Do not print these tables&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncaa2018.dat =read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2018.csv&amp;quot;,
                         header = T, sep = &amp;quot;,&amp;quot;)
head(ncaa2018.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weight     Last  First Conference  Finish     ELO
## 1    125   Atkins Thayer        ACC      NQ 1297.06
## 2    125  Bentley     LJ        ACC      NQ 1343.66
## 3    125    Fausz   Sean        ACC cons 24 1380.84
## 4    125    Hayes  Louie        ACC cons 12 1404.51
## 5    125 Norstrem   Kyle        ACC cons 24 1348.79
## 6    125  Bianchi   Paul     Big 12 cons 32 1312.73&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncaa2019.dat =read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/ncaa2019.csv&amp;quot;,
                         header = T, sep = &amp;quot;,&amp;quot;)
head(ncaa2019.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weight       Last     First Finish
## 1    125        Lee   Spencer      1
## 2    125    Mueller      Jack      2
## 3    125     Rivera Sebastian      3
## 4    125     Arujau    Vitali      4
## 5    125 Piccininni  Nicholas      5
## 6    125      Glory       Pat      6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged.ncaa &amp;lt;- merge(ncaa2018.dat, ncaa2019.dat, by = c(&amp;quot;Last&amp;quot;, &amp;quot;First&amp;quot;), all = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;The tables list the wrestlers qualifying for the NCAA 2018 and 2019 National Champions, respectively. Merge the tables into a single table that contains only those wrestlers who qualified for both tournaments. Use the columns &lt;code&gt;Last&lt;/code&gt; and &lt;code&gt;First&lt;/code&gt; to merge on; neither is unique for all wrestlers.&lt;/p&gt;
&lt;p&gt;The merged table should have columns corresponding to &lt;code&gt;Finish&lt;/code&gt; 2018 and &lt;code&gt;Finish&lt;/code&gt; 2019 - you can leave the column names as the defaults produced by R or SAS. To check the merge, print the number of rows in the table, and determine if there are any missing values in either &lt;code&gt;Finish&lt;/code&gt; column (&lt;code&gt;sum&lt;/code&gt; or &lt;code&gt;any&lt;/code&gt; are sufficient. &lt;em&gt;Do not print the table&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged.common.players &amp;lt;- merge(ncaa2018.dat, ncaa2019.dat, by=c(&amp;quot;Last&amp;quot;,&amp;quot;First&amp;quot;), all = FALSE) 
# numer of rows
print(nrow(merged.common.players))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 198&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(merged.common.players$Finish.x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Print a contingency table comparing &lt;code&gt;Weight&lt;/code&gt; for 2018 and &lt;code&gt;Weight&lt;/code&gt; for 2019. The sum of all cells in this table will be equal to the total number of wrestlers that competed in both tournaments; the sum of the main diagonal will be the number of wrestlers that competed in the same weight class for both. How many wrestlers changed weight classes?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight_contingency &amp;lt;-
  table(
  merged.common.players$Weight.x,
  merged.common.players$Weight.y,
  dnn = c(&amp;quot;Weight for 2018&amp;quot;, &amp;quot;Weight for 2019&amp;quot;)
  )

weight_contingency&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Weight for 2019
## Weight for 2018 125 133 141 149 157 165 174 184 197 285
##             125  20   2   0   0   0   0   0   0   0   0
##             133   2  17   6   0   0   0   0   0   0   0
##             141   0   2  14   5   0   0   0   0   0   0
##             149   0   0   2  12   4   0   0   0   0   0
##             157   0   0   0   3  12   2   0   0   0   0
##             165   0   0   0   0   1  17   3   0   0   0
##             174   0   0   0   0   0   1  18   1   0   0
##             184   0   0   0   0   0   0   0  14   4   0
##             197   0   0   0   0   0   0   0   4  15   0
##             285   0   0   0   0   0   0   0   0   0  17&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The sum of all cells in this table will be equal to the total number of
# wrestlers that competed in both tournaments
sum(weight_contingency)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 198&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Diagonal sum
sum(diag(weight_contingency))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 156&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many wrestlers changed weight classes? This gives:
sum(weight_contingency)-sum(diag(weight_contingency))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sum of all cells in this table will be equal to the total number of wrestlers that competed in both tournaments: which is 198&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5&lt;/h1&gt;
&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;I’m working on software that produces a repeated measures analysis. To test my code, I use published data and compare results. For one analysis, I used data from &lt;strong&gt;Contemporary Statistical Models for the Plant and Soil Sciences&lt;/strong&gt;, Oliver Schabenberger and Francis J. Pierce, 2001. These data are measurements of the diameter of individual apples from selected apple trees.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-a.-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a.&lt;/h2&gt;
&lt;p&gt;Download the &lt;code&gt;AppleData.csv&lt;/code&gt; if you choose R, the SAS data is included in the SAS template. Note the file include comments for the data; you may need to specify comment character in import. &lt;em&gt;Do not print this table&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To simplify this exercise, create a subset of the &lt;code&gt;AppleData&lt;/code&gt; including only trees number 3, 7 and 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AppleData=read.csv(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/AppleData.csv&amp;quot;, 
                   header = TRUE, comment.char = &amp;#39;#&amp;#39;)

#subsetting data to include tree 3, 7 and 10
AppleData&amp;lt;- AppleData[AppleData$tree %in% c(3,7,10),]
AppleData&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     tree apple time diam
## 111    3     1    1 2.91
## 112    3     1    2 3.00
## 113    3     1    3 3.02
## 114    3     1    4 3.03
## 115    3    10    1 2.81
## 116    3    10    2 2.89
## 117    3    10    3 2.87
## 118    3    10    4 2.93
## 119    3    10    5 2.93
## 120    3    10    6 2.94
## 121    3    16    1 2.95
## 122    3    16    2 3.00
## 123    3    16    3 3.03
## 124    3    16    4 3.03
## 125    3    16    5 3.06
## 126    3    16    6 3.08
## 127    3    17    1 2.79
## 128    3    17    2 2.83
## 129    3    17    3 2.86
## 130    3    17    4 2.87
## 131    3    17    5 2.87
## 132    3    17    6 2.93
## 133    3    18    1 2.98
## 134    3    18    2 3.03
## 135    3    18    3 3.06
## 136    3    18    4 3.09
## 137    3    18    5 3.09
## 138    3    18    6 3.09
## 139    3    20    1 2.76
## 140    3    20    2 2.82
## 141    3    20    3 2.83
## 142    3    20    4 2.85
## 143    3    20    5 2.86
## 144    3    20    6 2.88
## 145    3    22    1 2.76
## 146    3    22    2 2.82
## 147    3    22    3 2.85
## 148    3    22    4 2.87
## 149    3    22    5 2.90
## 150    3    22    6 2.90
## 151    3    23    1 2.76
## 152    3    23    2 2.78
## 153    3    23    3 2.77
## 154    3    23    4 2.79
## 155    3    23    5 2.79
## 156    3    23    6 2.79
## 157    3    24    1 2.80
## 158    3    24    2 2.85
## 159    3    24    3 2.87
## 160    3    24    4 2.87
## 161    3    24    5 2.89
## 162    3    24    6 2.92
## 317    7     2    1 2.79
## 318    7     2    2 2.89
## 319    7     2    3 2.89
## 320    7     2    4 2.91
## 321    7     2    5 2.91
## 322    7     2    6 2.95
## 323    7     4    1 2.80
## 324    7     4    2 2.81
## 325    7     4    3 2.85
## 326    7     4    4 2.91
## 327    7     4    5 2.92
## 328    7     4    6 2.96
## 329    7     9    1 3.06
## 330    7     9    2 3.15
## 331    7     9    3 3.15
## 332    7     9    4 3.23
## 333    7     9    5 3.27
## 334    7     9    6 3.31
## 335    7    25    1 2.84
## 336    7    25    2 2.86
## 337    7    25    3 2.88
## 338    7    25    4 2.93
## 339    7    25    5 2.93
## 340    7    25    6 2.96
## 399   10     2    1 2.92
## 400   10     2    2 2.95
## 401   10     2    3 3.00
## 402   10     2    4 3.01
## 403   10     2    5 3.07
## 404   10     5    1 2.87
## 405   10     5    2 2.89
## 406   10     5    3 2.94
## 407   10     5    4 2.95
## 408   10     5    5 3.01
## 409   10     5    6 3.02
## 410   10     8    1 2.76
## 411   10     8    2 2.81
## 412   10     8    3 2.86
## 413   10     8    4 2.90
## 414   10     9    1 2.91
## 415   10     9    2 3.01
## 416   10     9    3 3.07
## 417   10     9    4 3.09
## 418   10     9    5 3.11
## 419   10    10    1 2.88
## 420   10    10    2 2.88
## 421   10    10    3 2.92
## 422   10    10    4 2.97
## 423   10    10    5 2.97
## 424   10    10    6 2.99
## 425   10    17    1 3.00
## 426   10    17    2 3.05
## 427   10    17    3 3.05
## 428   10    17    4 3.06
## 429   10    17    5 3.11
## 430   10    18    1 2.85
## 431   10    18    2 2.87
## 432   10    18    3 2.91
## 433   10    18    4 2.95
## 434   10    18    5 2.98
## 435   10    18    6 3.00
## 436   10    21    1 2.76
## 437   10    21    2 2.83
## 438   10    21    3 2.84
## 439   10    21    4 2.87
## 440   10    21    5 2.88
## 441   10    21    6 2.91
## 442   10    22    1 3.25
## 443   10    22    2 3.34
## 444   10    22    3 3.34
## 445   10    22    4 3.38
## 446   10    22    5 3.47
## 447   10    23    1 3.00
## 448   10    23    2 3.06
## 449   10    23    3 3.08
## 450   10    23    4 3.14
## 451   10    23    5 3.18&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b.&lt;/h2&gt;
&lt;p&gt;Reshape or transpose this data from the long form to the wide form. Call this data &lt;code&gt;AppleWide&lt;/code&gt;. This table should have one column for &lt;code&gt;Tree&lt;/code&gt;, one column for &lt;code&gt;Apple&lt;/code&gt; and six columns, &lt;code&gt;diam.1&lt;/code&gt; - &lt;code&gt;diam.6&lt;/code&gt;. The values in the time columns come from &lt;code&gt;diam&lt;/code&gt; in &lt;code&gt;AppleData&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AppleData1&amp;lt;- as.data.frame(AppleData)

AppleWide&amp;lt;- reshape(AppleData1, direction = &amp;quot;wide&amp;quot;, idvar = c(&amp;quot;apple&amp;quot;,&amp;quot;tree&amp;quot;), timevar = &amp;quot;time&amp;quot;)
AppleWide&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     tree apple diam.1 diam.2 diam.3 diam.4 diam.5 diam.6
## 111    3     1   2.91   3.00   3.02   3.03     NA     NA
## 115    3    10   2.81   2.89   2.87   2.93   2.93   2.94
## 121    3    16   2.95   3.00   3.03   3.03   3.06   3.08
## 127    3    17   2.79   2.83   2.86   2.87   2.87   2.93
## 133    3    18   2.98   3.03   3.06   3.09   3.09   3.09
## 139    3    20   2.76   2.82   2.83   2.85   2.86   2.88
## 145    3    22   2.76   2.82   2.85   2.87   2.90   2.90
## 151    3    23   2.76   2.78   2.77   2.79   2.79   2.79
## 157    3    24   2.80   2.85   2.87   2.87   2.89   2.92
## 317    7     2   2.79   2.89   2.89   2.91   2.91   2.95
## 323    7     4   2.80   2.81   2.85   2.91   2.92   2.96
## 329    7     9   3.06   3.15   3.15   3.23   3.27   3.31
## 335    7    25   2.84   2.86   2.88   2.93   2.93   2.96
## 399   10     2   2.92   2.95   3.00   3.01   3.07     NA
## 404   10     5   2.87   2.89   2.94   2.95   3.01   3.02
## 410   10     8   2.76   2.81   2.86   2.90     NA     NA
## 414   10     9   2.91   3.01   3.07   3.09   3.11     NA
## 419   10    10   2.88   2.88   2.92   2.97   2.97   2.99
## 425   10    17   3.00   3.05   3.05   3.06   3.11     NA
## 430   10    18   2.85   2.87   2.91   2.95   2.98   3.00
## 436   10    21   2.76   2.83   2.84   2.87   2.88   2.91
## 442   10    22   3.25   3.34   3.34   3.38   3.47     NA
## 447   10    23   3.00   3.06   3.08   3.14   3.18     NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part c.&lt;/h2&gt;
&lt;p&gt;To confirm that you’ve reshaped correctly, print column means for the wide data set and use an aggregate or apply function to compute &lt;code&gt;time&lt;/code&gt; means for the long format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanWide= colMeans(AppleWide, na.rm = T)
meanWide&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      tree     apple    diam.1    diam.2    diam.3    diam.4    diam.5    diam.6 
##  6.739130 14.173913  2.878696  2.931304  2.953913  2.983913  3.009524  2.976875&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanLong = tapply(AppleData1$diam, AppleData1$time, mean, na.rm=T)
meanLong&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5        6 
## 2.878696 2.931304 2.953913 2.983913 3.009524 2.976875&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-d.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part d.&lt;/h2&gt;
&lt;p&gt;I choose this example for a test case because it shows a case where the best repeated measures model is an auto-regressive model - each measure is correlated with the preceding measure. We can estimate the degree of using the following R code. You don’t need to evaluate this code for this exercise; it’s provided as a motivation for reshaping the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mult.lm &amp;lt;- lm(cbind(diam.1, diam.2, diam.3, diam.4, diam.5, diam.6) ~ tree, data=AppleWide)
mult.manova &amp;lt;- manova(mult.lm)
print(cov2cor(estVar(mult.lm)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 6&lt;/h1&gt;
&lt;p&gt;This is an exercise in computing the Wilcoxon Signed Rank test. We will be using an example from NIST (&lt;code&gt;NATR332.DAT&lt;/code&gt;). See &lt;a href=&#34;https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/signrank.htm&#34; class=&#34;uri&#34;&gt;https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/signrank.htm&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;The data are provided:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NATR332.DAT &amp;lt;- data.frame(
  Y1 = c(146,141,135,142,140,143,138,137,142,136),
  Y2 = c(141,143,139,139,140,141,138,140,142,138)
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;part-a.-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a.&lt;/h2&gt;
&lt;p&gt;Add a column &lt;code&gt;Difference&lt;/code&gt; that is the difference between &lt;code&gt;Y1&lt;/code&gt; and &lt;code&gt;Y2&lt;/code&gt;. For further analysis, exclude any rows where the difference is 0.&lt;/p&gt;
&lt;p&gt;Next add add the column &lt;code&gt;Rank&lt;/code&gt;, which will be the rank of the absolute value of &lt;code&gt;Difference&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#adding column Difference in the data frame 
NATR332.DAT$Difference &amp;lt;- NATR332.DAT$Y1-NATR332.DAT$Y2
NATR332.DAT&amp;lt;- NATR332.DAT[NATR332.DAT$Difference != 0,]
NATR332.DAT&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Y1  Y2 Difference
## 1  146 141          5
## 2  141 143         -2
## 3  135 139         -4
## 4  142 139          3
## 6  143 141          2
## 8  137 140         -3
## 10 136 138         -2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Next, we rank and add a column in the data frame 
NATR332.DAT$Rank &amp;lt;- rank(abs(NATR332.DAT$Difference))
NATR332.DAT&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Y1  Y2 Difference Rank
## 1  146 141          5  7.0
## 2  141 143         -2  2.0
## 3  135 139         -4  6.0
## 4  142 139          3  4.5
## 6  143 141          2  2.0
## 8  137 140         -3  4.5
## 10 136 138         -2  2.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part c.&lt;/h2&gt;
&lt;p&gt;Add the column &lt;code&gt;SignedRank&lt;/code&gt; by applying the sign (+ or -) of &lt;code&gt;Difference&lt;/code&gt;, to to &lt;code&gt;Rank&lt;/code&gt; (that is, if &lt;code&gt;Difference&lt;/code&gt; is &amp;lt; 0, then &lt;code&gt;SignedRank&lt;/code&gt; is &lt;code&gt;-Rank&lt;/code&gt;, otherwise &lt;code&gt;SignedRank&lt;/code&gt; is &lt;code&gt;Rank&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SignedRank &amp;lt;- ifelse(NATR332.DAT$Difference &amp;lt; 0, - NATR332.DAT$Rank, + NATR332.DAT$Rank)
 
NATR332.DAT$SignedRank&amp;lt;- round(SignedRank, 0)
NATR332.DAT&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Y1  Y2 Difference Rank SignedRank
## 1  146 141          5  7.0          7
## 2  141 143         -2  2.0         -2
## 3  135 139         -4  6.0         -6
## 4  142 139          3  4.5          4
## 6  143 141          2  2.0          2
## 8  137 140         -3  4.5         -4
## 10 136 138         -2  2.0         -2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-d.-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part d.&lt;/h2&gt;
&lt;p&gt;Compute the sum of the positive ranks, and the absolute value of the sum of the negative ranks. Let &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; be the minimum of these two sums. Print &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;positive.sum = sum(NATR332.DAT$SignedRank[NATR332.DAT$SignedRank &amp;gt; 0])
positive.sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs.negative.sum = abs(sum(NATR332.DAT$SignedRank[NATR332.DAT$SignedRank &amp;lt; 0]))
abs.negative.sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let $W$ be the minimum of these two sums.
W = min(abs.negative.sum, positive.sum)
W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The expected mean of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is calculated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_W = N_r*(N_r+1)/4\]&lt;/span&gt;
with a standard deviation of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma_W = \sqrt{\frac{N_r(N_r+1)(2N_r+1)}{6}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(N_r\)&lt;/span&gt; is the number of ranked values (excluding differences of 0). Calculate a &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; score by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z_W = (W - \mu_W)/\sigma_W\]&lt;/span&gt;
Print both &lt;span class=&#34;math inline&#34;&gt;\(\mu_W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_W\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_W = (7*(7+1))/4
mu_W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma_W = sqrt((7*(7+1)*(2*7+1))/6)
sigma_W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11.83216&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z_W = (W - mu_W)/sigma_W
z_W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.08451543&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;- pnorm(z_W, lower.tail = TRUE)
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4663233&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value matches with the value calculated using Wilcoxon test below.&lt;/p&gt;
&lt;p&gt;The NIST page gives a p-values based on the continuity correction. We are not computing this correction. You can compute the &lt;span class=&#34;math inline&#34;&gt;\(P(z&amp;gt;z_W)\)&lt;/span&gt; of your &lt;span class=&#34;math inline&#34;&gt;\(z_W\)&lt;/span&gt; (using the normal distribution) and compare it to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, correct = FALSE, alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, :
## cannot compute exact p-value with ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon signed rank test
## 
## data:  NATR332.DAT$Y1 and NATR332.DAT$Y2
## V = 13.5, p-value = 0.466
## alternative hypothesis: true location shift is less than 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the corrected p-values are given by&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, correct = TRUE, alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, :
## cannot compute exact p-value with ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  NATR332.DAT$Y1 and NATR332.DAT$Y2
## V = 13.5, p-value = 0.5
## alternative hypothesis: true location shift is less than 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, correct = TRUE, alternative = &amp;quot;greater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(NATR332.DAT$Y1, NATR332.DAT$Y2, paired = TRUE, :
## cannot compute exact p-value with ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  NATR332.DAT$Y1 and NATR332.DAT$Y2
## V = 13.5, p-value = 0.5677
## alternative hypothesis: true location shift is greater than 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data tables</title>
      <link>/achalneupane.github.io/post/data_tables/</link>
      <pubDate>Sat, 08 Jun 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/data_tables/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;exercise-1.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1.&lt;/h1&gt;
&lt;p&gt;This exercise will repeat Exercise 1 from Homework 4, but using a data table.&lt;/p&gt;
&lt;div id=&#34;part-a.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Create a data table or frame with 3 defined columns:
- Let &lt;code&gt;M1&lt;/code&gt; be a sequence of means from 320-420, incremented by 10.
- Let &lt;code&gt;M2&lt;/code&gt; be 270.
- Let &lt;code&gt;SD&lt;/code&gt; be a pooled standard deviation of 150.&lt;/p&gt;
&lt;p&gt;Define and print the tabke in the space below. Do not create individual vectors for this exercise, outside of the data frame, if you use R. In SAS, you may use IML to create a matrix and save the matrix as a data table, or define a sequence (&lt;code&gt;DO&lt;/code&gt;) in the DATA step. I’ve included framework code in the SAS template.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M.table &amp;lt;- data.frame(M1= seq(320, 420, 10), M2= 270, SD = 150)
M.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     M1  M2  SD
## 1  320 270 150
## 2  330 270 150
## 3  340 270 150
## 4  350 270 150
## 5  360 270 150
## 6  370 270 150
## 7  380 270 150
## 8  390 270 150
## 9  400 270 150
## 10 410 270 150
## 11 420 270 150&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add to the data table a column containing required replicates, letting &lt;span class=&#34;math inline&#34;&gt;\(s_i = s_j = s_{pooled}\)&lt;/span&gt;. Also add a column containing Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To show results, either print the table or plot required replicates versus &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as in the previous homework.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using combined function from last week to calculate effect size and Required replicates
combined &amp;lt;- function (m1,m2 = 270, s_pooled = 150, alpha = 0.05, beta = 0.2){
  cv &amp;lt;- (s_pooled)/((m1+m2)/2)
  percent.diff &amp;lt;- ((m1-m2)/((m1+m2)/2))
  cohens_d &amp;lt;-(abs(m1-m2)/(s_pooled))
  n &amp;lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  n &amp;lt;- round(n,0)
  value &amp;lt;- (list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0), EffectSize = cohens_d))
  return(value)
}

data &amp;lt;- combined(m1 = M.table$M1, s_pooled = M.table$SD)
M.table$RequiredReplicates &amp;lt;- data$RequiredReplicates
M.table$EffectSize &amp;lt;- data$EffectSize
# Wanted table
M.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     M1  M2  SD RequiredReplicates EffectSize
## 1  320 270 150                141  0.3333333
## 2  330 270 150                 98  0.4000000
## 3  340 270 150                 72  0.4666667
## 4  350 270 150                 55  0.5333333
## 5  360 270 150                 44  0.6000000
## 6  370 270 150                 35  0.6666667
## 7  380 270 150                 29  0.7333333
## 8  390 270 150                 25  0.8000000
## 9  400 270 150                 21  0.8666667
## 10 410 270 150                 18  0.9333333
## 11 420 270 150                 16  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now plotting
# attach(M.table)
# cal.lm &amp;lt;- lm(M.table$RequiredReplicates ~ M.table$EffectSize)
# plot(RequiredReplicates ~ EffectSize)
plot(M.table$RequiredReplicates ~ M.table$EffectSize)
# abline(cal.lm)
abline(v = 0.5, col= &amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2&lt;/h1&gt;
&lt;div id=&#34;part-a.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;You will repeat the calculations from Homework 4, Ex 2, but this time, using a data table. However, instead of a &lt;span class=&#34;math inline&#34;&gt;\(5 \times 6\)&lt;/span&gt; matrix, the result with be a table with 30 rows, each corresponding to a unique combination of CV from &lt;span class=&#34;math inline&#34;&gt;\(8, 12, ..., 28\)&lt;/span&gt; and Diff from &lt;span class=&#34;math inline&#34;&gt;\(5,10, ... , 25\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The table should look something like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left\{
 \begin{array}{cc}
     CV &amp;amp; Diff \\
     8 &amp;amp; 5  \\
     8 &amp;amp; 10  \\
     8 &amp;amp; 15  \\
     \vdots &amp;amp; \vdots \\
     12 &amp;amp; 5  \\
     12 &amp;amp; 10  \\
     12 &amp;amp; 15  \\
     \vdots &amp;amp; \vdots \\
     28 &amp;amp; 5  \\
     28 &amp;amp; 10  \\
     28 &amp;amp; 15  \\
   \end{array}
   \right\}
\]&lt;/span&gt;
Test your required replicates calculations by calculating required replicates for each combination of CV and Diff using the default values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Name this column &lt;code&gt;Moderate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Calculate required replicaes again, but this time let &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.01\)&lt;/span&gt; and let &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.1\)&lt;/span&gt;. Label this column &lt;code&gt;Conservative&lt;/code&gt;. Repeat the calculations, but this time let &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.1\)&lt;/span&gt; and let &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you choose SAS, you can use the framework code from the first exercise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata.dat &amp;lt;- data.frame(CV = rep(seq(8,28,4), each = 5), Diff = rep(seq(5,25,5),6))
newdata.dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CV Diff
## 1   8    5
## 2   8   10
## 3   8   15
## 4   8   20
## 5   8   25
## 6  12    5
## 7  12   10
## 8  12   15
## 9  12   20
## 10 12   25
## 11 16    5
## 12 16   10
## 13 16   15
## 14 16   20
## 15 16   25
## 16 20    5
## 17 20   10
## 18 20   15
## 19 20   20
## 20 20   25
## 21 24    5
## 22 24   10
## 23 24   15
## 24 24   20
## 25 24   25
## 26 28    5
## 27 28   10
## 28 28   15
## 29 28   20
## 30 28   25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combined &amp;lt;- function (cv, percent.diff, alpha = 0.05, beta = 0.2){
  cv &amp;lt;- cv
  percent.diff &amp;lt;- percent.diff
  n &amp;lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  n &amp;lt;- round(n,0)
  value &amp;lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0))
  return(value)
}
value &amp;lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff)

# Adding Moderate column
newdata.dat$Moderate &amp;lt;- value$RequiredReplicates

# Adding Conservative column
value &amp;lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff, alpha = 0.01, beta = 0.1)

newdata.dat$Conservative &amp;lt;- value$RequiredReplicates

# Repeat the calculations, but this time let $\alpha = 0.1$ and let $\beta =
# 0.2$.

# Adding Liberal column
value &amp;lt;- combined(cv = newdata.dat$CV, percent.diff = newdata.dat$Diff, alpha = 0.1, beta = 0.2)

newdata.dat$Liberal &amp;lt;- value$RequiredReplicates
# Print the table
# newdata.dat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To show your work, some ideas for graphs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plot required replicates vs CV and Diff, using different colors or symbols for &lt;code&gt;Moderate&lt;/code&gt;,&lt;code&gt;Conservative&lt;/code&gt; and &lt;code&gt;Liberal&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(newdata.dat)

# Effect size Vs Required replicates
plot(CV, Moderate, col = &amp;quot;black&amp;quot;, type = &amp;#39;b&amp;#39;, pch = 8, xlab=&amp;quot;Effect Size&amp;quot;, ylab=&amp;quot;Required replicates&amp;quot;, ylim = c(min(Moderate, Conservative, Liberal),max(Moderate, Conservative, Liberal)))
points(CV, Conservative, type = &amp;#39;b&amp;#39;, pch = 2,  col = &amp;quot;red&amp;quot;)
points(CV, Liberal, type = &amp;#39;b&amp;#39;, pch = 20,  col = &amp;quot;blue&amp;quot;)
legend(&amp;quot;topleft&amp;quot;, 
       legend = c(&amp;quot;Moderate&amp;quot;, &amp;quot;Conservative&amp;quot;, &amp;quot;Liberal&amp;quot;), 
       col = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;),
       bty = &amp;quot;o&amp;quot;, 
       pch = c(8, 2, 20),
       pt.cex = 2, 
       cex = 1.2, 
       text.col = &amp;quot;black&amp;quot;, 
       horiz = F , 
       inset = c(0.1, 0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Percent diff vs Required replicates
plot(Diff, Moderate, col = &amp;quot;black&amp;quot;, type = &amp;#39;b&amp;#39;,pch = 8, xlab=&amp;quot;Percent diff&amp;quot;, ylab=&amp;quot;Required replicates&amp;quot;, ylim = c(min(Moderate, Conservative, Liberal),max(Moderate, Conservative, Liberal)))
points(Diff, Conservative, type = &amp;#39;b&amp;#39;, pch = 2,  col = &amp;quot;red&amp;quot;)
points(Diff, Liberal, type = &amp;#39;b&amp;#39;, pch = 20,  col = &amp;quot;blue&amp;quot;)
legend(&amp;quot;topright&amp;quot;, 
       legend = c(&amp;quot;Moderate&amp;quot;, &amp;quot;Conservative&amp;quot;, &amp;quot;Liberal&amp;quot;), 
       col = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;),
       bty = &amp;quot;o&amp;quot;, 
       pch = c(8, 2, 20),
       pt.cex = 2, 
       cex = 1.2, 
       text.col = &amp;quot;black&amp;quot;, 
       horiz = F , 
       inset = c(0.1, 0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plot &lt;code&gt;Conservative&lt;/code&gt; vs &lt;code&gt;Moderate&lt;/code&gt; and &lt;code&gt;Liberal&lt;/code&gt; vs &lt;code&gt;Moderate&lt;/code&gt;, including a line with slope 1 and intercept 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(newdata.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from newdata.dat (pos = 3):
## 
##     Conservative, CV, Diff, Liberal, Moderate&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conservative Vs Moderate
plot(Conservative ~ Moderate, type = &amp;#39;b&amp;#39;, col = &amp;quot;black&amp;quot;)
abline(a=1, b = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Liberal Vs Moderate
plot(Liberal ~ Moderate, type = &amp;#39;b&amp;#39;, col = &amp;quot;black&amp;quot;)
abline(a=1, b = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;p&gt;You’ll work with data from U.S. Wholesale price for pumpkins 2018 (&lt;a href=&#34;https://www.ers.usda.gov/newsroom/trending-topics/pumpkins-background-statistics/&#34; class=&#34;uri&#34;&gt;https://www.ers.usda.gov/newsroom/trending-topics/pumpkins-background-statistics/&lt;/a&gt;, Table 1)&lt;/p&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a&lt;/h2&gt;
&lt;p&gt;Download the file &lt;code&gt;pumpkins.csv&lt;/code&gt; from D2L and read the file into a data frame. Print a summary of the table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv&amp;quot;, header = TRUE, sep = &amp;quot;,&amp;quot;)
pumpkins &amp;lt;- data.frame(pumpkins)
attach(pumpkins)
print(summary(pumpkins))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Month         Week              Class        Size        Price      
##  October  :15   Min.   :1.000   Blue      : 6   Large :18   Min.   :121.0  
##  September:15   1st Qu.:3.000   Cinderella: 7   Medium:12   1st Qu.:130.2  
##                 Median :4.500   Howden    :10               Median :175.0  
##                 Mean   :4.433   Pie       : 7               Mean   :178.1  
##                 3rd Qu.:6.000                               3rd Qu.:216.2  
##                 Max.   :7.000                               Max.   :257.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b&lt;/h2&gt;
&lt;p&gt;To show that the data was read correctly, create three plots. Plot
1. Price vs Week
2. Price vs Class
3. Size vs Class&lt;/p&gt;
&lt;p&gt;These three plots should reproduce the three types of plots shown in the &lt;code&gt;RegressionEtcPlots&lt;/code&gt; video, &lt;strong&gt;Categorical vs Categorical&lt;/strong&gt;, &lt;strong&gt;Continuous vs Continuous&lt;/strong&gt; and &lt;strong&gt;Continuous vs Categorical&lt;/strong&gt;. Add these as titles to your plots, as appropriate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(pumpkins)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from pumpkins (pos = 3):
## 
##     Class, Month, Price, Size, Week&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(Price ~ Week, main = &amp;quot;**Continuous vs Continuous**&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(Price ~ Class, main = &amp;quot;**Continuous vs Categorical**&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(Size ~ Class, main = &amp;quot;**Categorical vs Categorical**&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-7-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From these plots, you should be able to answer these questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Are some Weeks missing Price observations?
Yes some Weeks are missing some Price observations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do Prices vary more for some Classes?
Yes, prices vary more for some classes, such as Pie.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do all Classes have the same Sizes?
No, they are of different sizes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;p&gt;Calculate a one-way analysis of variance from the pumpkin data in Exercise 3.&lt;/p&gt;
&lt;div id=&#34;option-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Option A&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; be the &lt;code&gt;Price&lt;/code&gt;. Let the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; treatments be &lt;code&gt;Class&lt;/code&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; be the &lt;code&gt;Price&lt;/code&gt; total for &lt;code&gt;Class&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and let &lt;span class=&#34;math inline&#34;&gt;\(r_i\)&lt;/span&gt; be the number of observations for &lt;code&gt;Class&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Denote the total number of observations &lt;span class=&#34;math inline&#34;&gt;\(N = \sum r_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;part-a-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a&lt;/h3&gt;
&lt;p&gt;Find the treatment totals &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T} = T_1 \dots T_k\)&lt;/span&gt; and replicates per treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{r} = r_1 \dots r_k\)&lt;/span&gt; from the pumpkin data, using group summary functions and compute a grand total &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; for &lt;code&gt;Price&lt;/code&gt;. Print &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{r}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; below. In SAS, you can use &lt;code&gt;proc summary&lt;/code&gt; or &lt;code&gt;proc means&lt;/code&gt; to compute &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; adn &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and output a summary table. I find the rest is easier in IML (see &lt;code&gt;use&lt;/code&gt; to access data tables in IML).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins.dat &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv&amp;quot;, header = TRUE, sep = &amp;quot;,&amp;quot;)
head(pumpkins.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Month Week Class   Size Price
## 1 September    1   Pie Medium   175
## 2 September    2   Pie Medium   199
## 3 September    3   Pie Medium   224
## 4 September    4   Pie Medium   224
## 5   October    5   Pie Medium   219
## 6   October    6   Pie Medium   219&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pumpkins.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- length(levels(pumpkins.dat$Class))
k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- tapply(pumpkins.dat$Price, pumpkins.dat$Class, sum)
# can also use aggregate 
# T &amp;lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = sum)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Blue Cinderella     Howden        Pie 
##       1050       1529       1279       1484&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- table(pumpkins.dat$Class)
# can also use aggregate
# aggregate(pumpkins.dat$Class, by = list(pumpkins.dat$Class), FUN = length)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##       Blue Cinderella     Howden        Pie 
##          6          7         10          7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- sum(r)
G &amp;lt;- sum(pumpkins.dat$Price)
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5342&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;Calculate sums of squares as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{Correction Factor : } C &amp;amp;= \frac{G^2}{N} \\
\text{Total SS : } &amp;amp;= \sum y^2 - C \\
\text{Treatments SS : }  &amp;amp;= \sum \frac{T_i^2}{r_i} -C \\
\text{Error SS : }  &amp;amp;= \text{Total SS} - \text{Treatments SS} \\
\end{aligned}
\]&lt;/span&gt;
and calcute &lt;span class=&#34;math inline&#34;&gt;\(MSB = (\text{Treatments SS})/(k-1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(MSW = (\text{Error SS})/(N-k)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;C &amp;lt;- G^2/N
C&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 951232.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TotalSS &amp;lt;- sum((pumpkins.dat$Price)^2)-C
TotalSS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 48805.87&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TreatmentsSS &amp;lt;- sum(T^2/r) - C
TreatmentsSS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 44687.25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ErrorSS &amp;lt;- TotalSS - TreatmentsSS
ErrorSS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4118.614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSB &amp;lt;- TreatmentsSS/(k-1)
MSB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14895.75&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSW &amp;lt;- ErrorSS/(N-k)
MSW&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 158.4082&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Calculate an F-ratio and a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for this &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, using the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; distribution with &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N-k\)&lt;/span&gt; degrees of freedom. Use &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F.ratio &amp;lt;- MSB/MSW
F.ratio&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 94.03394&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- k-1
df2 &amp;lt;- N-k
p.value &amp;lt;- pf(F.ratio, df1, df2, lower.tail = FALSE)
p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.421291e-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check your work, use &lt;code&gt;aov&lt;/code&gt; as illustated in the chunk below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(aov(Price ~ Class, data=pumpkins.dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## Class        3  44687   14896   94.03 4.42e-14 ***
## Residuals   26   4119     158                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;option-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Option B&lt;/h2&gt;
&lt;p&gt;You may resue code from Exercise 6, Homework 4. Use group summary functions to calculate means, standard deviations and replicates from the pumpkin data, then calculate &lt;span class=&#34;math inline&#34;&gt;\(MSW\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(MSB\)&lt;/span&gt; as previously. Report the F-ratio and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; value as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean.groups &amp;lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = mean)
mean.groups&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Group.1        x
## 1       Blue 175.0000
## 2 Cinderella 218.4286
## 3     Howden 127.9000
## 4        Pie 212.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean.groups &amp;lt;- mean.groups$x
sd.groups &amp;lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = sd)
sd.groups&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Group.1         x
## 1       Blue  0.000000
## 2 Cinderella 17.924445
## 3     Howden  3.695342
## 4        Pie 18.565200&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd.groups &amp;lt;- sd.groups$x

k &amp;lt;- length(mean.groups)
k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length.groups &amp;lt;- aggregate(pumpkins.dat$Price, by = list(pumpkins.dat$Class), FUN = length)

# We have n samples in each pumpkin.dat Class; so our population size is N:
n &amp;lt;- length.groups$x
n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  6  7 10  7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- sum(n)
N&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 30&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calculating MSB 
mean.mean.groups &amp;lt;- mean(mean.groups)
MSB = (sum(n*(mean.groups-mean.mean.groups)^2))/(k-1)
MSB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15173&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSW &amp;lt;- sum((n-1) * sd.groups^2)/(N-k)
MSW&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 158.4082&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F.ratio &amp;lt;- MSB/MSW
F.ratio&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 95.78418&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- k-1
df2 &amp;lt;- N-k
p.value &amp;lt;- pf(F.ratio, df1, df2, lower.tail = FALSE)
p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.551828e-14&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5&lt;/h1&gt;
&lt;div id=&#34;part-a-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a&lt;/h2&gt;
&lt;p&gt;Go to &lt;a href=&#34;http://www.itl.nist.gov/div898/strd/anova/SiRstv.html&#34; class=&#34;uri&#34;&gt;http://www.itl.nist.gov/div898/strd/anova/SiRstv.html&lt;/a&gt; and use the data listed under &lt;code&gt;Data File in Table Format&lt;/code&gt; (&lt;a href=&#34;https://www.itl.nist.gov/div898/strd/anova/SiRstvt.dat&#34; class=&#34;uri&#34;&gt;https://www.itl.nist.gov/div898/strd/anova/SiRstvt.dat&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b&lt;/h2&gt;
&lt;p&gt;Edit this into a file that can be read into R or SAS, or find an appropriate function that can read the file as-is. You will need to upload the edited file to D2L along with your Rmd/SAS files. Provide a brief comment on changes you make, or assumptions about the file needed for you file to be read into R/SAS. Read the data into a data frame or data table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Here we read the .dat file first skipping the 59 lines as indicated in SiRstvt.dat file.
df &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/SiRstvt.dat&amp;quot;, header = FALSE, skip = 59)
# read file as dataframe
df &amp;lt;- as.data.frame(df)
# We need to change the column names as alpha-numeric to work with ease
names(df) &amp;lt;- paste0(&amp;quot;col_&amp;quot;, seq(1,5,1))
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      col_1    col_2    col_3    col_4    col_5
## 1 196.3052 196.3042 196.1303 196.2795 196.2119
## 2 196.1240 196.3825 196.2005 196.1748 196.1051
## 3 196.1890 196.1669 196.2889 196.1494 196.1850
## 4 196.2569 196.3257 196.0343 196.1485 196.0052
## 5 196.3403 196.0422 196.1811 195.9885 196.2090&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part c&lt;/h2&gt;
&lt;p&gt;There are 5 columns in these data. Calculate mean and sd and sample size for each column in this data, using column summary functions. Print the results below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculating the mean (or summary)
sapply(df, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    col_1    col_2    col_3    col_4    col_5 
## 196.2431 196.2443 196.1670 196.1481 196.1432&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summary(df, digits = 7)
print(data.frame(sapply(df, summary)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            col_1    col_2    col_3    col_4    col_5
## Min.    196.1240 196.0422 196.0343 195.9885 196.0052
## 1st Qu. 196.1890 196.1669 196.1303 196.1485 196.1051
## Median  196.2569 196.3042 196.1811 196.1494 196.1850
## Mean    196.2431 196.2443 196.1670 196.1481 196.1432
## 3rd Qu. 196.3052 196.3257 196.2005 196.1748 196.2090
## Max.    196.3403 196.3825 196.2889 196.2795 196.2119&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculating length for all columns
sapply(df, length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## col_1 col_2 col_3 col_4 col_5 
##     5     5     5     5     5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculating sd for all columns
sapply(df, sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      col_1      col_2      col_3      col_4      col_5 
## 0.08747329 0.13797498 0.09372413 0.10422674 0.08844797&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Determine the largest and smallest means, and their corresponding standard deviations, and calculate an effect size and required replicates to experimentally detect this effect.&lt;/p&gt;
&lt;p&gt;If you defined functions in the previous exercises, you should be able to call them here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt&amp;lt;- as.data.frame(sapply(df, summary))
# Assigning smallest mean as m1 (Col_5)
m1 &amp;lt;- min(tt[rownames(tt)==&amp;quot;Mean&amp;quot;,])
m1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 196.1432&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Assigning largest mean as m2 (col_2)
m2 &amp;lt;- max(tt[rownames(tt)==&amp;quot;Mean&amp;quot;,])
m2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 196.2443&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now, getting sd for column 2 (largest mean) and 5 (smallest mean)
sd &amp;lt;- sapply(df, sd)
# SD from smallest mean column and largest mean column
# SD for column 5 (smallest mean)
s1 &amp;lt;- as.numeric(sd[5])
s1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08844797&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# SD for column 2 (largest mean)
s2 &amp;lt;- as.numeric(sd[2])
s2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.137975&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculating the effect size
cohen.d &amp;lt;- function(m1,s1,m2,s2){
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  return(cohens_d)
}

cohen.d(m1 = m1, s1 = s1, m2 = m2, s2 = s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8720476&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculating the Required replicates
required.replicates &amp;lt;- function (m1,s1, m2,s2, alpha=0.05, beta=0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}

# required replicates
required.replicates(m1 = m1, s1 = s1, m2 = m2, s2 = s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 6&lt;/h1&gt;
&lt;p&gt;There is a web site (&lt;a href=&#34;https://www.wrestlestat.com/rankings/starters&#34; class=&#34;uri&#34;&gt;https://www.wrestlestat.com/rankings/starters&lt;/a&gt;) that ranks college wrestlers using an ELO scoring system (&lt;a href=&#34;https://en.wikipedia.org/wiki/Elo_rating_system&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Elo_rating_system&lt;/a&gt;). I was curious how well the rankings predicted performance, so I gathered data from th 2018 NCAA Wrestling Championships (&lt;a href=&#34;https://i.turner.ncaa.com/sites/default/files/external/gametool/brackets/wrestling_d1_2018.pdf&#34; class=&#34;uri&#34;&gt;https://i.turner.ncaa.com/sites/default/files/external/gametool/brackets/wrestling_d1_2018.pdf&lt;/a&gt;). Part of the data are on D2L in the file &lt;code&gt;elo.csv&lt;/code&gt;. You will need to download the file to your computer for this exercise.&lt;/p&gt;
&lt;p&gt;Read the dzta below and print a summary. The dzta were created by writing a data frame from R to csv (&lt;code&gt;write.csv&lt;/code&gt;), so the first column is row number and does not have a header entry (the header name is an empty string).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elo.dat &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/elo.csv&amp;quot;, header = TRUE, row.names = 1, sep = &amp;quot;,&amp;quot;)
# elo.dat
print(summary(elo.dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Weight        Conference      ELO        ActualFinish    ExpectedFinish
##  Min.   :125.0   Big Ten:87   Min.   :1228   AA     :80    E[AA]     :80    
##  1st Qu.:141.0   EIWA   :55   1st Qu.:1342   cons 12:40    E[cons 12]:36    
##  Median :157.0   Big 12 :52   Median :1372   cons 16:40    E[cons 16]:36    
##  Mean   :170.9   ACC    :40   Mean   :1379   cons 24:79    E[cons 24]:66    
##  3rd Qu.:184.0   MAC    :34   3rd Qu.:1410   cons 32:80    E[cons 32]:46    
##  Max.   :285.0   Pac 12 :25   Max.   :1584   cons 33:10    E[NQ]     :65    
##                  (Other):36&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row corresponds to an individual wrestler, his weight class and collegiate conference. The WrestleStat ELO score is listed, along with his tournament finish round (i.e. &lt;code&gt;AA&lt;/code&gt; = 1-8 place, &lt;code&gt;cons 12&lt;/code&gt; = lost in the final consolation round, etc.). I calculated an expected finish based on his ELO ranking within the weight class, where &lt;code&gt;E[AA]&lt;/code&gt; = top 8 ranked, expected to finish as AA, etc.&lt;/p&gt;
&lt;p&gt;Produce group summaries or plots to answer the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the mean and standard deviations of ELO by Expected Finish and by Actual Finish?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# By both groups
# mean.ExActFinish &amp;lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ExpectedFinish, elo.dat$ActualFinish), mean)
# mean.ExActFinish

# First By Expected Finish:
mean.ExFinish &amp;lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ExpectedFinish), FUN = mean)
mean.ExFinish&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Group.1        x
## 1      E[AA] 1451.336
## 2 E[cons 12] 1395.442
## 3 E[cons 16] 1379.404
## 4 E[cons 24] 1357.369
## 5 E[cons 32] 1334.704
## 6      E[NQ] 1332.821&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SD.ExFinish &amp;lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ExpectedFinish), FUN = sd)
SD.ExFinish&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Group.1        x
## 1      E[AA] 41.04978
## 2 E[cons 12] 17.77768
## 3 E[cons 16] 13.11593
## 4 E[cons 24] 16.02282
## 5 E[cons 32] 18.02051
## 6      E[NQ] 52.69272&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now, by Actual Finish
mean.ActFinish &amp;lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ActualFinish), FUN = mean)
mean.ActFinish&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Group.1        x
## 1      AA 1444.556
## 2 cons 12 1400.708
## 3 cons 16 1371.745
## 4 cons 24 1355.130
## 5 cons 32 1333.270
## 6 cons 33 1343.795&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SD.ActFinish &amp;lt;- aggregate(elo.dat$ELO, by = list(elo.dat$ActualFinish), FUN = sd)
SD.ActFinish&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Group.1        x
## 1      AA 50.93285
## 2 cons 12 29.22633
## 3 cons 16 34.28861
## 4 cons 24 30.95125
## 5 cons 32 34.08563
## 6 cons 33 28.30588&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Do all conferences have similar quality, or might we suspect one or more conferences have better wrestlers than the rest? (You don’t need to perform an analysis, just argue, based on the summary, if a deeper analysis is warranted).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(elo.dat$Conference, by = list(elo.dat$ExpectedFinish, elo.dat$ActualFinish), FUN = summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Group.1 Group.2 x.ACC x.Big 12 x.Big Ten x.EIWA x.EWL x.MAC x.Pac 12
## 1       E[AA]      AA     7        4        30      8     1     3        4
## 2  E[cons 12]      AA     2        3         1      1     1     1        0
## 3  E[cons 16]      AA     1        1         4      0     0     0        0
## 4  E[cons 24]      AA     0        1         0      0     0     0        0
## 5  E[cons 32]      AA     0        1         0      0     0     0        0
## 6       E[NQ]      AA     0        1         2      0     0     2        1
## 7       E[AA] cons 12     2        3         2      4     1     1        0
## 8  E[cons 12] cons 12     1        1         4      4     1     0        1
## 9  E[cons 16] cons 12     1        1         1      0     0     1        0
## 10 E[cons 24] cons 12     0        2         2      1     0     0        1
## 11      E[NQ] cons 12     0        1         1      0     0     1        0
## 12      E[AA] cons 16     2        0         2      1     0     0        2
## 13 E[cons 12] cons 16     0        0         1      0     2     0        1
## 14 E[cons 16] cons 16     0        1         4      1     0     0        1
## 15 E[cons 24] cons 16     0        1         3      3     0     1        0
## 16 E[cons 32] cons 16     1        1         2      2     0     0        0
## 17      E[NQ] cons 16     1        1         2      0     1     0        2
## 18      E[AA] cons 24     1        0         1      1     0     0        0
## 19 E[cons 12] cons 24     1        1         1      4     0     1        0
## 20 E[cons 16] cons 24     5        2         1      1     0     1        1
## 21 E[cons 24] cons 24     8        4         5      4     3     4        1
## 22 E[cons 32] cons 24     0        3         3      5     1     3        1
## 23      E[NQ] cons 24     0        2         4      2     1     2        1
## 24 E[cons 12] cons 32     0        0         0      2     0     0        0
## 25 E[cons 16] cons 32     0        2         1      1     1     0        1
## 26 E[cons 24] cons 32     1        5         3      2     2     2        1
## 27 E[cons 32] cons 32     2        4         3      4     3     5        1
## 28      E[NQ] cons 32     3        6         3      2     4     4        4
## 29 E[cons 16] cons 33     0        0         1      0     0     0        0
## 30 E[cons 24] cons 33     0        0         0      1     0     2        1
## 31 E[cons 32] cons 33     0        0         0      1     0     0        0
## 32      E[NQ] cons 33     1        0         0      0     0     0        0
##    x.SoCon
## 1        0
## 2        0
## 3        0
## 4        0
## 5        0
## 6        0
## 7        0
## 8        1
## 9        1
## 10       0
## 11       0
## 12       0
## 13       0
## 14       0
## 15       0
## 16       0
## 17       1
## 18       0
## 19       0
## 20       0
## 21       0
## 22       0
## 23       0
## 24       0
## 25       0
## 26       1
## 27       0
## 28       7
## 29       0
## 30       1
## 31       0
## 32       2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregate(elo.dat$Conference, by = list(elo.dat$ExpectedFinish, elo.dat$ActualFinish), length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this table, Big Ten seems to have better wrestlers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How well does ELO predict finish? Use a contingency table or mosaic plot to show how often, say, and &lt;code&gt;AA&lt;/code&gt; finish corresponds to an &lt;code&gt;E[AA]&lt;/code&gt; finish.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contingency.table &amp;lt;- table(elo.dat$ExpectedFinish, elo.dat$ActualFinish)
# Contingency table
contingency.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##              AA cons 12 cons 16 cons 24 cons 32 cons 33
##   E[AA]      57      13       7       3       0       0
##   E[cons 12]  9      13       4       8       2       0
##   E[cons 16]  6       5       7      11       6       1
##   E[cons 24]  1       6       8      29      17       5
##   E[cons 32]  1       0       6      16      22       1
##   E[NQ]       6       3       8      12      33       3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # can convert table to dataframe
# contingency.table.dat &amp;lt;- as.data.frame.matrix(contingency.table)
# Also, plot
attach(elo.dat)
plot(ActualFinish ~ ExpectedFinish)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/data_tables_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;
Based on the contingency table, &lt;code&gt;E[AA]&lt;/code&gt; &lt;code&gt;AA&lt;/code&gt; are associated 57 times&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does this data set include non-qualifiers? (The NCAA tournament only allows 33 wreslers per weight class).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(elo.dat$Weight)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 125 133 141 149 157 165 174 184 197 285 
##  33  33  33  33  33  33  33  33  32  33&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this, all weight class have 33 wrestlers with only 32 wrestlers in 197 weigh class&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Control Structures</title>
      <link>/achalneupane.github.io/post/control_structures/</link>
      <pubDate>Wed, 05 Jun 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/control_structures/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;There are six exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve five of the sixth problems, mixing the languages as you wish.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Warning&lt;/em&gt; I will continue restricting the use of external libraries in R, particularly &lt;code&gt;tidyverse&lt;/code&gt; libraries. You may choose to use &lt;code&gt;ggplot2&lt;/code&gt;, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the midterm and final projects.&lt;/p&gt;
&lt;div id=&#34;experimental&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Experimental&lt;/h4&gt;
&lt;p&gt;Again, you will be allowed to provide one solution using Python. Elaborate on the similarities and differences between Ptyhon function definitions and R or IML or Macro language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reuse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reuse&lt;/h2&gt;
&lt;p&gt;For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here. I’m also including data vectors that can be used in some exercises.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CaloriesPerServingMean &amp;lt;- c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4)
CaloriesPerServingSD &amp;lt;- c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1&lt;/h1&gt;
&lt;p&gt;Write a general Cohen &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; function to be more generally useful, accepting a wider range of arguments. For convenience, name this &lt;code&gt;general.d&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The new function should accept two parameters, &lt;code&gt;m&lt;/code&gt;, &lt;code&gt;s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In your function, check for these condititions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;code&gt;m&lt;/code&gt; is of length 1 and &lt;code&gt;s&lt;/code&gt; is length 1, then simply divide &lt;code&gt;m/s&lt;/code&gt; - that is, proceed with the calculations as if &lt;code&gt;m = %Diff&lt;/code&gt; and &lt;code&gt;s = CV&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;m&lt;/code&gt; is of length 2, then calculate the difference and proceed with the calculations.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;code&gt;m&lt;/code&gt; is of length greater than 2, find the difference between the min and max of &lt;code&gt;m&lt;/code&gt; and proceed with the calculations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If s is of length greater than 1 calculate pooled sd as&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s^2_{pooled} = \sqrt{\frac{\sum_i^k s_i^2}{k}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating the general.d function with all possible combinations for m and s
# conditions:
general.d &amp;lt;- function(m, s){
  if(length(m) == 1 &amp;amp;&amp;amp; length(s) == 1){
    Diff = m
    CV = s
    print(&amp;quot;EffectSize calculated for length(m) == 1, and length(s) == 1&amp;quot;)
  }else if (length(m) == 2 &amp;amp;&amp;amp; length(s) &amp;gt; 1){
    Diff = abs(m[1]-m[2])
    CV = sqrt(sum((s^2))/length(s))
    print(&amp;quot;EffectSize calculated for length(m) == 2, and length(s) &amp;gt; 1&amp;quot;)
  }else if (length(m) == 2 &amp;amp;&amp;amp; length(s) == 1){
    Diff = abs(m[1]-m[2])
    CV = s
    print(&amp;quot;EffectSize calculated for length(m) == 2, and length(s) == 1&amp;quot;)
  }else if (length(m) &amp;gt; 2 &amp;amp;&amp;amp; length(s) &amp;gt; 1){
    Diff = max(m)-min(m)
    CV = sqrt(sum((s^2))/length(s))
    print(&amp;quot;EffectSize calculated for length(m) &amp;gt; 2, and length(s) &amp;gt; 1&amp;quot;)
  }else if (length(m) &amp;gt; 2 &amp;amp;&amp;amp; length(s) == 1){
  Diff = max(m)-min(m)
  CV = s
  print(&amp;quot;EffectSize calculated for length(m) &amp;gt; 2, and length(s) == 1&amp;quot;)
}
  EffectSize &amp;lt;- Diff/CV
  return(EffectSize)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test your function with the three cases:&lt;/p&gt;
&lt;div id=&#34;effect-size&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 Effect Size&lt;/h4&gt;
&lt;p&gt;Use just the mean from 1936 and the associated standard deviation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;general.d(268.1, 124.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;EffectSize calculated for length(m) == 1, and length(s) == 1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.148237&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;p&gt;This should duplicate results from prior exercises.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;general.d(c(268.1,384.4), c(124.8,168.3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;EffectSize calculated for length(m) == 2, and length(s) &amp;gt; 1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, it matches the result from Homework 4&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;all-of-calories-per-serving.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;All of Calories per Serving.&lt;/h4&gt;
&lt;p&gt;Use the vectors from the Reuse chunk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;general.d(CaloriesPerServingMean,CaloriesPerServingSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;EffectSize calculated for length(m) &amp;gt; 2, and length(s) &amp;gt; 1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9051585&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are these three scenarios sufficient to test every path through &lt;code&gt;general.d&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;Yes, these three scenarious are sufficient to test for all conditions mentioned above.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2&lt;/h1&gt;
&lt;p&gt;Previously, we’ve calculated required replicates based on the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; distribution. In this exercise, you will calculate required replicates based on the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; disttibution. You must implement one of two algorithms given below. For both algorithmss, alculate degrees of freedom as &lt;span class=&#34;math inline&#34;&gt;\(\nu = n*k-k\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the current estimate for required replicates and let &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;algorithm-1-from-cochran-and-cox&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Algorithm 1 (from Cochran and Cox, )&lt;/h3&gt;
&lt;p&gt;Use the formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
n \ge 2\times \left( \frac{CV}{\%Diff} \right)^2 \times \left(t_{\alpha/2,\nu}+ t_{\beta,\nu} \right)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with a small &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, say, 2.&lt;/li&gt;
&lt;li&gt;Calculate critical &lt;span class=&#34;math inline&#34;&gt;\(t_\alpha/2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_\beta\)&lt;/span&gt; quantiles with &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; d.f, then calculate required replicates. Label this &lt;span class=&#34;math inline&#34;&gt;\(n_{current}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(n_{current}\)&lt;/span&gt;, then recalculate critical values and required replicates.
Label this &lt;span class=&#34;math inline&#34;&gt;\(n_{next}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(n_{current}=n_{next}\)&lt;/span&gt; then the algorithm has converged.
Otherwise, set &lt;span class=&#34;math inline&#34;&gt;\(n_{current}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(n_{next}\)&lt;/span&gt;, and repeat 2-3.&lt;/li&gt;
&lt;li&gt;If after some sufficiently large number (say, 20), the algorithm hasn’t converged, print a message and return the largest of &lt;span class=&#34;math inline&#34;&gt;\(n_{current}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{next}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates.t &amp;lt;- function (m1, s1, m2, s2,n = 2, k = 2, alpha = 0.05, beta = 0.20){
  temp.required.replicates.t &amp;lt;- function(...){
    sd.pooled &amp;lt;- function(...){
    sqrt((s1^2 + s2^2)/2)
    }
    d &amp;lt;- (sd.pooled(s1,s2))/abs(m1-m2)
    ceiling(2*d^2*(qt(1-alpha/2, n*k-k) + qt(1-beta, n*k-k))^2)
  }
  
  
  # n &amp;lt;- 2
  i &amp;lt;- 1
  ncurrent.indexed &amp;lt;- {
  }
  nnext.indexed &amp;lt;- {
  }
  success &amp;lt;- FALSE
  # while (!success) {
  while (i &amp;lt; 20 &amp;amp;&amp;amp; !success) {
    ncurrent &amp;lt;- n
    ncurrent.indexed[i] &amp;lt;- ncurrent # so we can keep track of ncurrent
    nnext &amp;lt;- temp.required.replicates.t(m1, s1, m2, s2, n = ncurrent)
    nnext.indexed[i] &amp;lt;- nnext # so we can keep track of nnext
    success &amp;lt;- nnext == ncurrent
    if (!success &amp;amp;&amp;amp; i == 20) {
      # if (success ){
      print(&amp;quot;The algorithm DO NOT converge!!!!!&amp;quot;)
      print(paste0(&amp;quot;The largest of ncurrent is : &amp;quot;, max(ncurrent.indexed)))
      print(paste0(&amp;quot;The nnext is: &amp;quot;, nnext))
    } else if (success) {
      print(paste0(&amp;quot;The algorithm DO converge at iteration &amp;quot; , i , &amp;quot; !!!!!&amp;quot;))
      break
    }
    n &amp;lt;- nnext
    i &amp;lt;- i + 1
  }
  values &amp;lt;-
    list(n.current = ncurrent.indexed, n.next = nnext.indexed)
  return(values)
}

# Copying required.replicates function from last homework
required.replicates &amp;lt;- function (m1,m2, s1,s2, alpha=0.05, beta=0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;algorithm-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Algorithm 2&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with a small &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, say, 2.&lt;/li&gt;
&lt;li&gt;Calculate critical &lt;span class=&#34;math inline&#34;&gt;\(t_\alpha\)&lt;/span&gt; quantile using the central &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution with &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; d.f.&lt;/li&gt;
&lt;li&gt;Estimate Type IIShow in New WindowClear OutputExpand/Collapse Output error (p-value) under the alternate hypothesis using the non-central &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution with &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; d.f, at the critical &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; from 2. Calculate non-centrality parameter as
&lt;span class=&#34;math display&#34;&gt;\[
NCP = \frac{\%Diff}{CV} \sqrt{\frac{n}{2}}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If the resulting error is less than &lt;span class=&#34;math inline&#34;&gt;\(1-\beta\)&lt;/span&gt;, accept the current value of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Otherwise increment &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and repeate 2-3.&lt;/li&gt;
&lt;li&gt;If desired power is not achieved after a large number of iterations (say, 1000), terminate the calculations and return NA.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Implement the algorithm as a function or macro named &lt;code&gt;required.replicates.t&lt;/code&gt;, with parameters &lt;code&gt;mu&lt;/code&gt;, &lt;code&gt;sigma&lt;/code&gt; and an optional parameter &lt;code&gt;k&lt;/code&gt;. Test your function by comoparing with required replicates from prior exercises for calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006.&lt;/p&gt;
&lt;p&gt;For either algoorithm, you might consider starting with an initial value of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; calculated using the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; critical values as before. Can you be certain that the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; formula will not estimate more required replicates than the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; algorithm?&lt;/p&gt;
&lt;div id=&#34;versus-2006-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates(268.1, 124.8, 384.4, 168.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 67&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates.t(268.1, 124.8, 384.4, 168.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The algorithm DO converge at iteration 4 !!!!!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $n.current
## [1]  2 94 26 27
## 
## $n.next
## [1] 94 26 27 27&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-1997&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 1997&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates(268.1, 124.8, 288.6, 122.0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 38&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates.t(268.1, 124.8, 288.6, 122.0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The algorithm DO converge at iteration 3 !!!!!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $n.current
## [1]    2 2085  570
## 
## $n.next
## [1] 2085  570  570&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1997 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates(384.4,168.3,288.6,122.0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;required.replicates.t(384.4,168.3,288.6,122.0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The algorithm DO converge at iteration 3 !!!!!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $n.current
## [1]   2 136  38
## 
## $n.next
## [1] 136  38  38&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might find it useful to reproduce the plots from Homework 4, Ex. 3. Plot the central and non-central &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distributions over the range &lt;span class=&#34;math inline&#34;&gt;\(-3,4\)&lt;/span&gt;, and produce plots for seledted &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;p&gt;Calculate a cumulative probability value from the normal pdf, using the Newton-Cotes formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\int _{x_0} ^{x_n} f(x) dx \approx \sum _{i=0} ^n h f(x_i)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1, ..., x_n\)&lt;/span&gt; are a sequence of evenly spaced numbers from &lt;span class=&#34;math inline&#34;&gt;\(-2 \dots 2\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(x_i = x_0 + h i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the sequence and step size &lt;span class=&#34;math inline&#34;&gt;\(h = (x_n -x_0)/n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We will calculate this integral by calculating successive approximations of &lt;span class=&#34;math inline&#34;&gt;\(f = L(x;0,1)\)&lt;/span&gt; &lt;code&gt;= norm.pdf&lt;/code&gt; over series of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with increasingly smaller step sizes.&lt;/p&gt;
&lt;div id=&#34;part-a.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a.&lt;/h2&gt;
&lt;p&gt;Calculate &lt;span class=&#34;math inline&#34;&gt;\(L_0\)&lt;/span&gt; by summing over &lt;span class=&#34;math inline&#34;&gt;\(L(\bf{X_0})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X_0\)&lt;/span&gt; is a series from &lt;span class=&#34;math inline&#34;&gt;\(x_0=-2, \dots, x_n=2\)&lt;/span&gt; incremented by &lt;span class=&#34;math inline&#34;&gt;\(h_0=0.1\)&lt;/span&gt;. Multiply this sum by &lt;span class=&#34;math inline&#34;&gt;\(h_0\)&lt;/span&gt; for an approximate &lt;span class=&#34;math inline&#34;&gt;\(\int _{x_0} ^{x_n} L(x) dx\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h0_val &amp;lt;- 0.1
x0_val&amp;lt;-{}
x0_val &amp;lt;- seq(-2,2,h0_val)
result0 &amp;lt;- sum(x0_val)
result0 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.662937e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normal.pdf.0 &amp;lt;- result0 *h0_val
normal.pdf.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.662937e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Think of this as the sum of a series of rectangles, each &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; wide and a height given by the normal pdf.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b.&lt;/h2&gt;
&lt;p&gt;Create a second series &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; by setting &lt;span class=&#34;math inline&#34;&gt;\(h_1 = h_0/2\)&lt;/span&gt;. Compute &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt; from this series as in part a. Let &lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt;
You now have the are of twice as many rectangles as part a, but each is half as wide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now, Create a second series  X_1
h1_val &amp;lt;- h0_val/2
x1_val&amp;lt;-{}
x1_val &amp;lt;- seq(-2,2,h1_val)
result1 &amp;lt;- sum(x1_val)
result1 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.881784e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normal.pdf.1 &amp;lt;- result1 *h1_val
normal.pdf.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.440892e-16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part c.&lt;/h2&gt;
&lt;p&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(\delta=|L_i - L_{i-1}|\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;lt; 0.0001\)&lt;/span&gt;, your sequence of iterations has converged on a solution for &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;. Finish with Part d. Otherwise, increment &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(h_i = h_{i-1}/2\)&lt;/span&gt;. Create the next series &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; and compute the next &lt;span class=&#34;math inline&#34;&gt;\(L_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Hint: code this first as a for loop of a small number of &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; until you know your code will converge toward a solution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h_val&amp;lt;- 0
seq1&amp;lt;-{}
seq2&amp;lt;-{}
for (x in 1:10){
  
  if (x == 1 ){
    hx &amp;lt;- 0.1
    h_val[x] &amp;lt;- hx
  }
  
  else {
    hx &amp;lt;- h_val[x]
  }
  
  y &amp;lt;- x +1
  hy &amp;lt;- hx/2
  h_val[y] &amp;lt;- hy
  
  seq1 &amp;lt;- seq(-2,2,hx)
  sumSeq1 &amp;lt;- sum(seq1)
  pdf1 &amp;lt;- sumSeq1*hx
  seq2 &amp;lt;- seq(-2,2,hy)
  sumSeq2 &amp;lt;- sum(seq2)
  pdf2 &amp;lt;- sumSeq2*hy  
  del &amp;lt;- abs(sumSeq2 - sumSeq1)  
  if (del &amp;lt; 0.0001){
    print(&amp;quot;The sequence has been converged&amp;quot;)
    break
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The sequence has been converged&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-d&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part d&lt;/h1&gt;
&lt;p&gt;Report &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h_val[y]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(seq2)-length(seq1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check your results, compare your final &lt;span class=&#34;math inline&#34;&gt;\(L_i\)&lt;/span&gt; to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(-2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is your estimate within &lt;span class=&#34;math inline&#34;&gt;\(0.0001\)&lt;/span&gt; of this value?&lt;/p&gt;
&lt;p&gt;You might find it useful to produce staircase plots for the first 2-4 iterations (plot &lt;span class=&#34;math inline&#34;&gt;\(L_i\)&lt;/span&gt; vs &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; on one graph). You might also find it interesting to plot &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;. You can create vectors to hold the intermediate steps - 10 iterations should be enough. How many iterations might it take to get within 0.000001 of the expected value from R?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;div id=&#34;part-a.-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a.&lt;/h2&gt;
&lt;p&gt;Write a function to compute mean, standard deviation, skewness and kurtosis from a single vector of numeric values. You can use the built-in mean function, but must use one (and only one) for loop to compute the rest. Be sure to include a check for missing values. Note that computationally efficient implementations of moments calculations take advantage of &lt;span class=&#34;math inline&#34;&gt;\((Y_i-\bar{Y})^4 = (Y_i-\bar{Y}) \times (Y_i-\bar{Y})^3\)&lt;/span&gt;, etc.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm&#34; class=&#34;uri&#34;&gt;https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm&lt;/a&gt; for formula for skewness and kurtosis. This reference gives several definitions for both skewness and kurtosis, you only need to implement one formula for each. Note that for computing skewness and kurtosis, standard deviation is computed using &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; as a divisor, not &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Your function should return a list with &lt;code&gt;Mean&lt;/code&gt;, &lt;code&gt;SD&lt;/code&gt;, &lt;code&gt;Skewness&lt;/code&gt; and &lt;code&gt;Kurtosis&lt;/code&gt;. If you use IML, you will need to implement this as a subroutie and use call by reference; include these variables in parameter list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; ss_x=0 
  temp.kurtosis=0
  temp.skewness=0
  i=1
  k=0
  # length.x=length(x)
  # creating a function for calculating mean, sd, skewness and kurtosis
  kurtskw=function(x){
    # while (i &amp;lt;= length.x) {
    for (i in 1:length(x)){
      if(!is.na(x[i])){
        meanX=mean(x,na.rm = TRUE)
        ss_x=ss_x + ((x[i]- meanX)^2)
        temp.kurtosis= temp.kurtosis + ((x[i] - meanX)^4)
        temp.skewness=temp.skewness + ((x[i] - meanX)^3)
        k=k+1
      }
      # i=i+1
    }   
    meanX=mean(x,na.rm = TRUE)
    Sd=sqrt(ss_x/k)
    kurtosis = temp.kurtosis/(k*(Sd^4))
    skewness =  temp.skewness/((Sd^3)*k)
    return(list(meanX=meanX,Sd=Sd,kurtosis=kurtosis,skewness=skewness))
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b.&lt;/h2&gt;
&lt;p&gt;Test your function by computing moments for &lt;code&gt;Price&lt;/code&gt; from &lt;code&gt;pumpkins.csv&lt;/code&gt;, for &lt;code&gt;ELO&lt;/code&gt; from &lt;code&gt;elo.csv&lt;/code&gt; or the combine observations from &lt;code&gt;SiRstvt&lt;/code&gt;. If find that &lt;code&gt;ELO&lt;/code&gt; shows both skewness and kurtosis, &lt;code&gt;Price&lt;/code&gt; is kurtotic but not skewed, while &lt;code&gt;SiRstvt&lt;/code&gt; are approximately normal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins.dat =read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/pumpkins.csv&amp;quot;, header = T, sep = &amp;quot;,&amp;quot;)
ELO.dat =read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master/elo.csv&amp;quot;, header = T, sep = &amp;quot;,&amp;quot;)
SiRstvt.dat &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/achalneupane/data/master//SiRstvt.dat&amp;quot;, header = FALSE, skip = 59)
x = pumpkins.dat$Price
y = ELO.dat$ELO
# z = vector(SiRstvt.dat)
library(reshape2)
SiRstvt.dat &amp;lt;- suppressWarnings(melt(SiRstvt.dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No id variables; using all as measure variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z = SiRstvt.dat$value
print(kurtskw(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $meanX
## [1] 178.0667
## 
## $Sd
## [1] 40.33438
## 
## $kurtosis
## [1] 1.670802
## 
## $skewness
## [1] -0.04981076&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(kurtskw(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $meanX
## [1] 1378.776
## 
## $Sd
## [1] 56.42003
## 
## $kurtosis
## [1] 3.684744
## 
## $skewness
## [1] 0.6822352&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(kurtskw(z))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $meanX
## [1] 196.1892
## 
## $Sd
## [1] 0.1034955
## 
## $kurtosis
## [1] 2.337141
## 
## $skewness
## [1] -0.1456679&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also found that &lt;code&gt;ELO&lt;/code&gt; shows both skewness and kurtosis, &lt;code&gt;Price&lt;/code&gt; is kurtotic but not skewed, while &lt;code&gt;SiRstvt&lt;/code&gt; are approximately normal.&lt;/p&gt;
&lt;p&gt;Note:
&lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm&#34; class=&#34;uri&#34;&gt;https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm&lt;/a&gt;
A fundamental task in many statistical analyses is to characterize the location and variability of a data set. A further characterization of the data includes skewness and kurtosis.
Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.&lt;/p&gt;
&lt;p&gt;Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.
The values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George &amp;amp; Mallery, 2010). George, D., &amp;amp; Mallery, M. (2010). SPSS for Windows Step by Step: A Simple Guide and Reference, 17.0 update (10a ed.) Boston: Pearson.&lt;/p&gt;
&lt;p&gt;Also,
If skewness is not close to zero, then your data set is not normally distributed.
If skewness is less than -1 or greater than 1, the distribution is highly skewed.
If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.
If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(moments)
kurtosis(x, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.670802&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skewness(x, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04981076&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kurtosis(y, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.684744&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skewness(y, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6822352&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kurtosis(z, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.337141&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skewness(z, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.1456679&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5&lt;/h1&gt;
&lt;p&gt;In this exercise, we will use run-time profiling and timing to compare the speed of execution for different functons or calculations. In the general, the algorithm will be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Write a loop to execute a large number of iterations.
I find &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; to be useful; you might start with a smaller number as you develop your code.&lt;/li&gt;
&lt;li&gt;In this loop, call a function or perform a calculation.
You don’t need to use or print the results, just assign the result to a local variable.&lt;/li&gt;
&lt;li&gt;Repeat 1 and 2, but with a different function or formula.&lt;/li&gt;
&lt;li&gt;Repeat steps 1-3 10 times, saving the time of execution for each pair of the 10 tests. Calculate mean, standard deviation and effect size for the two methods tested.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you choose R, I’ve included framework code using &lt;code&gt;Rprof&lt;/code&gt;; I’ve included framework code for IML in the SAS template.&lt;/p&gt;
&lt;p&gt;Test options
- In homework 3, you were given two formula for the Poisson pmf,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x;\lambda) &amp;amp; = \frac{e^{-\lambda} \lambda^x}{x!} \\
             &amp;amp; = exp(-\lambda)(\frac{1}{x!}) exp[x\times log(\lambda)] \\
\end{aligned}
\]&lt;/span&gt;
Compare the computationally efficiency of these two formula.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a sequence &lt;code&gt;x&lt;/code&gt; of numbers -3 to 3 of length &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; or so. In the first test, detetermine the amoung of time it takes to compute &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; estimates of &lt;code&gt;norm.pdf&lt;/code&gt; by visiting each element of &lt;code&gt;x&lt;/code&gt; in a loop. In the second test, simply pass &lt;code&gt;x&lt;/code&gt; as an argument to &lt;code&gt;norm.pdf&lt;/code&gt;. Does R or IML optimize vector operations?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A
x0 = -2
xn = 2
h0 &amp;lt;- 0.1 # spaced by 0.1
# Xi = x_0+h
X0 &amp;lt;- seq(x0,xn,h0)
X0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6
## [16] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
## [31]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;L0 &amp;lt;- sum(X0)
L0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.662937e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normal_pdf0 &amp;lt;- L0*h0
normal_pdf0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.662937e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# B



h1 &amp;lt;- h0/2
X1 &amp;lt;- seq(-2,2,h1)

L1 &amp;lt;- sum(X1)
L1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.881784e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normal_pdf1 &amp;lt;- L1*h1
normal_pdf1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.440892e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# C 



# Computing delta to meet the condition
H &amp;lt;- 0

for (i in 1:1){
  
  if (i == 1 ){
    hi &amp;lt;- 0.1
    H[i] &amp;lt;- hi
  } else {
    hi &amp;lt;- H[i]
  }
  # Set j as i+1
  j &amp;lt;- i +1
  
  hj &amp;lt;- hi/2
  H[j] &amp;lt;- hj
  
  Xi &amp;lt;- seq(-2,2,hi)
  Li &amp;lt;- sum(Xi)
  normal_pdf.i &amp;lt;- Li*hi
  
  xj &amp;lt;- seq(-2,2,hj)
  Lj &amp;lt;- sum(xj)
  normal_pdf.j &amp;lt;- Lj*hj
  
  delta &amp;lt;- abs(Lj - Li)
  
  # If converge print
  if (delta &amp;lt; 0.0001){
    print(paste0(&amp;quot;The sequence has converged at iteration i: &amp;quot;, i, &amp;quot; and iteration j: &amp;quot;, j))
    break
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The sequence has converged at iteration i: 1 and iteration j: 2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The mathematical statement &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{x}\)&lt;/span&gt; can be coded as either &lt;code&gt;sqrt(x)&lt;/code&gt; or &lt;code&gt;x^(1/2)&lt;/code&gt;. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(e^x\)&lt;/span&gt; can be written as &lt;span class=&#34;math inline&#34;&gt;\(exp(1)^x\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(exp(x)\)&lt;/span&gt;. These pairs are mathematically equivalent, but are they computationally equivalent. Write two test loops to compare formula with either &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{x}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(e^x\)&lt;/span&gt; of some form (the normal pdf, perhaps).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I find &lt;span class=&#34;math inline&#34;&gt;\(10^5 - 10^7\)&lt;/span&gt; give useful results, and that effect size increases with the number iterations; there is some overhead in the loop itself that contributes relatively less with increasing interations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Rprof(&amp;quot;test1&amp;quot;)
## iteration code 1
Rprof(NULL)
Rprof(&amp;quot;test2&amp;quot;)
## iteration code 2
Rprof(NULL)
#execution time for test 1
summaryRprof(&amp;quot;test1&amp;quot;)$sampling.time
#execution time for test 2
summaryRprof(&amp;quot;test2&amp;quot;)$sampling.time
#functions called in test 1
summaryRprof(&amp;quot;test1&amp;quot;)$by.self
#functions called in test 2
summaryRprof(&amp;quot;test2&amp;quot;)$by.self&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 6&lt;/h1&gt;
&lt;p&gt;Write an improved Poisson pmf function, call this function &lt;code&gt;smart.pois&lt;/code&gt;, using the same parameters &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;lamba&lt;/code&gt; as before, but check &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; for the following conditions.
1. If &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is negative, return a missing value (&lt;code&gt;NA&lt;/code&gt;, &lt;code&gt;.&lt;/code&gt;).
2. If &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is non-integer, truncate &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; then proceed.
3. If &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is too large for the factorial function, return the smallest possible numeric value for your machine. What &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is too large? You could test the return value of &lt;code&gt;factorial&lt;/code&gt; against &lt;code&gt;Inf&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can reuse previously tested code writing this function as a wrapper for a previously written &lt;code&gt;pois.pmf&lt;/code&gt; and call that function only after testing the for specified conditions.&lt;/p&gt;
&lt;p&gt;Test this function by repeating the plots from Homework 4, Ex 4. How is the function different than &lt;code&gt;dpois&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; You may not be able to call this new function exactly as in the last exercise (Hint - what are the rules for conditions in &lt;code&gt;if&lt;/code&gt; statements?). Instead, you might need to create a matrix or data table and use &lt;code&gt;apply&lt;/code&gt; functions, or write a loop for visit each element in a vector of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(warn=-1)

smart.pois &amp;lt;- function(x, lambda){
  if(x&amp;lt;0){
    poisson.d &amp;lt;- NA
    # poisson.d &amp;lt;- &amp;quot;NA check&amp;quot;
  }else if(class(x) != &amp;quot;integer&amp;quot; &amp;amp; suppressWarnings(factorial(x)) != Inf){
    x &amp;lt;- trunc(x)
    poisson.d &amp;lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda)))
    # poisson.d &amp;lt;- &amp;quot;integer.check&amp;quot;
  }else if(suppressWarnings(factorial(x)) == Inf){
    smallest.value &amp;lt;- .Machine
    poisson.d &amp;lt;- smallest.value$double.xmin
    # poisson.d &amp;lt;- &amp;quot;factorial.check&amp;quot;
  }
  return(poisson.d)
}



x_a &amp;lt;- seq(-5, 5, 0.1)

smart.poisson.probability.x_a &amp;lt;- {}
lambda &amp;lt;- 1
for(i in 1:length(x_a)){
out &amp;lt;- smart.pois(x_a[i], lambda)
smart.poisson.probability.x_a &amp;lt;- c(smart.poisson.probability.x_a, out)
}
smart.poisson.probability.x_a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1]          NA          NA          NA          NA          NA          NA
##   [7]          NA          NA          NA          NA          NA          NA
##  [13]          NA          NA          NA          NA          NA          NA
##  [19]          NA          NA          NA          NA          NA          NA
##  [25]          NA          NA          NA          NA          NA          NA
##  [31]          NA          NA          NA          NA          NA          NA
##  [37]          NA          NA          NA          NA          NA          NA
##  [43]          NA          NA          NA          NA          NA          NA
##  [49]          NA          NA 0.367879441 0.367879441 0.367879441 0.367879441
##  [55] 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441
##  [61] 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441 0.367879441
##  [67] 0.367879441 0.367879441 0.367879441 0.367879441 0.183939721 0.183939721
##  [73] 0.183939721 0.183939721 0.183939721 0.183939721 0.183939721 0.183939721
##  [79] 0.183939721 0.183939721 0.061313240 0.061313240 0.061313240 0.061313240
##  [85] 0.061313240 0.061313240 0.061313240 0.061313240 0.061313240 0.061313240
##  [91] 0.015328310 0.015328310 0.015328310 0.015328310 0.015328310 0.015328310
##  [97] 0.015328310 0.015328310 0.015328310 0.015328310 0.003065662&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.pdf &amp;lt;- function(x,mu=0,sigma=1){
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}
normal.liklihood.x_a &amp;lt;- norm.pdf(x_a)


plot(x_a,normal.liklihood.x_a,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_a,smart.poisson.probability.x_a,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Control_Structures_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using base functions for norm.pdf and poisson probability for x_a
base.normal.liklihood.x_a &amp;lt;- dnorm(x_a,0, 1)
base.poiss.x_a &amp;lt;- dpois(x=x_a, lambda = lambda)

plot(x_a,base.normal.liklihood.x_a,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_a,base.poiss.x_a,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Control_Structures_files/figure-html/unnamed-chunk-22-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now
# Using mean and standard deviation for servings per recipe from 1936:
mu = 12.9; sigma = 13.3

# Now we find the upper and lower bounds as:
x.lower &amp;lt;- floor(mu-5*sigma)
x.upper &amp;lt;- ceiling(mu+5*sigma)

# Now taking the length of x_a as a reference, we create the equeally spaced
# sequence from lower to upper bound as followed:
spacer &amp;lt;- (x.upper - x.lower)/(length(x_a) - 1)

x_b &amp;lt;- seq(x.lower, x.upper, spacer)
# To show both x_a and x_b &amp;#39;s lenghts are equal:
length(x_a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(x_b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.pdf &amp;lt;- function(x, mu = 12.9, sigma = 13.3){
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}
normal.liklihood.x_b &amp;lt;- norm.pdf(x_b)


# Calculate poisson probability for x_b using smart.pois
#using sigma = 13.3 to compare the difference with the first plot

smart.poisson.probability.x_b &amp;lt;- {}
lambda &amp;lt;- 12
for(i in 1:length(x_a)){
  out &amp;lt;- smart.pois(x_a[i], lambda)
  smart.poisson.probability.x_b &amp;lt;- c(smart.poisson.probability.x_b, out)
}
# # smart.pois output for x_b
# smart.poisson.probability.x_b


# Repeating the plot from homework 4 Ex 4
plot(x_b,normal.liklihood.x_b,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_b,smart.poisson.probability.x_b,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Control_Structures_files/figure-html/unnamed-chunk-22-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using base functions for x_b
base.normal.liklihood.x_b &amp;lt;- dnorm(x_b,0, 1)
base.poiss.x_b &amp;lt;- dpois(x=x_b, lambda = 12)

plot(x_b,base.normal.liklihood.x_b,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_b,base.poiss.x_b,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Control_Structures_files/figure-html/unnamed-chunk-22-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# FInally, we can also check how smart.pois and dpois functions behave for the following numerical values
# Testing three conditions with smart.pois
smart.pois(-1, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smart.pois(2, 12) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0004423833&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smart.pois(2.233, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0004423833&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smart.pois(300, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.225074e-308&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#comparing same parameters with dpois
dpois(-1, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dpois(2, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0004423833&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dpois(2.233, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dpois(300, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.140347e-296&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Smart pois is different from dpois as smart.pois gives NAs for negative values and dpois gives, zero. In other word, we can say that dpois cannot handle non-integer, but smart.pois can.Also, for non-integer, dpois gives zero. Also, I couldn’t find any similar pattern in the output of smart.poisson.probability.x_a and base.poiss.x_a&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Functions</title>
      <link>/achalneupane.github.io/post/functions/</link>
      <pubDate>Thu, 30 May 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/functions/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;exercise-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1&lt;/h1&gt;
&lt;p&gt;Implement Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as a function of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = f(m_1, s_1, m_2, s_2) = \frac{|m_1-m_2|}{s_{pooled}}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(s_{pooled}\)&lt;/span&gt; is a pooled standard deviation. Use the formula &lt;span class=&#34;math inline&#34;&gt;\(s_{pooled} = \sqrt{(s_1^2 + s_2^2)/2}\)&lt;/span&gt;. You may implement pooled standard deviation as a function as well.&lt;/p&gt;
&lt;p&gt;Calculate the effect size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; for the differences among calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006. Use the values from Wansink, Table 1 as given in Homework 1 or in the course outline. Name this function &lt;code&gt;cohen.d&lt;/code&gt; (or similar if using SAS)&lt;/p&gt;
&lt;div id=&#34;answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function definition
# Variables m1 and m2 are means, and s1 and s2 are standard deviations
# for two dates of comparison among calories per serving we are interested in, respectively. 
cohen.d &amp;lt;- function(m1,s1,m2,s2){
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  return(cohens_d)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;versus-2006&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=384.4
s1=124.8
s2=168.3
cohen.d(m1=m1,s1=s1,m2=m2,s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-1997&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 1997&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=288.6
s1=124.8
s2=122.0
cohen.d(m1=m1,s1=s1,m2=m2,s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1661157&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1997 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=288.6
m2=384.4
s1=122.0
s2=168.3
cohen.d(m1=m1,s1=s1,m2=m2,s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6517694&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check your work by comparing with the previous homework.
-Answers match with previous homework!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2.&lt;/h1&gt;
&lt;p&gt;Implement the required replicates calculation as a function of &lt;span class=&#34;math inline&#34;&gt;\(m_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(m_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_2\)&lt;/span&gt; as required parameters, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; as optional parameters. Let &lt;code&gt;alpha=0.05&lt;/code&gt; and &lt;code&gt;beta=0.2&lt;/code&gt;, so you’ll need to compute quantiles for &lt;code&gt;1-alpha/2&lt;/code&gt; and &lt;code&gt;1-beta&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Your function should return an integer &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
n \ge 2\times \left( \frac{CV}{\%Diff} \right)^2 \times \left(z_{\alpha/2}+ z_\beta \right)^2
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\%Diff = \frac{m_1 - m_2}{(m_1 + m_2)/2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CV = \frac{sd_{pooled}}{(m_1 + m_2)/2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You may use the pooled standarad deviation function from Ex. 1 (if you defined such a function).&lt;/p&gt;
&lt;p&gt;Name this function &lt;code&gt;required.replicates&lt;/code&gt; (or similar if using SAS)&lt;/p&gt;
&lt;div id=&#34;answer-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function definition
# Variables m1 and m2 are means, and s1 and s2 are standard deviations
# for two dates of comparison among calories per serving we are interested in, respectively. 
required.replicates &amp;lt;- function (m1,m2, s1,s2, alpha=0.05, beta=0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;versus-2006-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=384.4
s1=124.8
s2=168.3
required.replicates(m1=m1, m2=m2, s1=s1, s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-1997-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 1997&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=288.6
s1=124.8
s2=122.0
required.replicates(m1=m1, m2=m2, s1=s1, s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 569&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1997 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=288.6
m2=384.4
s1=122.0
s2=168.3
required.replicates(m1=m1, m2=m2, s1=s1, s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 37&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check your work by comparing with the previous homework.
-Answers match with previous homework!&lt;/p&gt;
&lt;p&gt;Note:
for Alpha=0.05 , we can use the r function qnorm(1-alpha/2) assuming u=0 and sd=1,
As for Beta, we need additional information.
z-score is the a standardized value of the value the hypothesized x.
and alpha is about rejecting the value x when its true.
but beta is about x failing to reject in when it is false… which means there is other value of x which we don’t have in the formula z=(x-u)/sd.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;p&gt;Implement the likelihood formula as a function or macro.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L (x ; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}^{}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Define &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; as optional parameters, taking values &lt;code&gt;mu=0&lt;/code&gt; and &lt;code&gt;sigma=1&lt;/code&gt;. Name this function &lt;code&gt;norm.pdf&lt;/code&gt;&lt;/p&gt;
&lt;div id=&#34;answer-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function definition
# Function to calcuate the values for log liklihood from above equation.
# First, we define the values for sigma as variance, 
# mu as mean of a normal population to be used for a liklihood of a x observation.
norm.pdf &amp;lt;- function(x,mu=0,sigma=1){
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;x-0.1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=-0.1\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x=-0.1
norm.pdf(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3969525&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;x0.0&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=0.0\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x=0.0
norm.pdf(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3989423&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;x0.1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=0.1\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x=0.1
norm.pdf(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3969525&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check your work by comparing with the previous homework.
-Answers match with previous homework!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;p&gt;The probability mass function for value &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; from Poisson data with a mean and variance &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(x;\lambda) = \frac{e^{-\lambda} \lambda^x}{x!} = exp(-\lambda)(\frac{1}{x!}) exp[x\times log(\lambda)]
\]&lt;/span&gt;
Write a function &lt;code&gt;pois.pmf&lt;/code&gt; that accepts two parameters, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;lambda&lt;/code&gt;. Use the built in &lt;code&gt;factorial&lt;/code&gt; function for &lt;span class=&#34;math inline&#34;&gt;\(x!\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; should be an integer value, so call a rounding function inside your function.
Test your function with &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 12\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 8,12,16\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;answer-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Define your function(s) in the code chunk below, then call the function with appropriate arguments in the following sections&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function definition
# The function to calculate probability mass function for poisson 
# data with a mean and variance lambda. 
  pois.pmf &amp;lt;- function(x, lambda){
    poisson.d &amp;lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda)))
  return(poisson.d)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;x4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=4\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda=12
x=4
pois.pmf(x=x, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.005308599&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;x12&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=12\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda=12
x=12
pois.pmf(x=x, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1143679&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;x20&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=20\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda=12
x=20
pois.pmf(x=x, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.009682032&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can check your work against the built in Poisson distribution functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using built-in function &amp;#39;dpois&amp;#39;, we can check our answers:
# for x=4
x= 4
lambda=12
dpois(x=x, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.005308599&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# #or
# ppois(x,lambda)-ppois(x-1,lambda)

# for x =12
x= 12
lambda=12
dpois(x=x, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1143679&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for x=20
x= 20
lambda=12
dpois(x=x, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.009682032&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which was correct for all three x&amp;#39;s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something to ponder. Note that there are two formula given. Can you implement both forms in R/IML/Macro language? Would there be a difference in computational speed or efficiency?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Implementation of the first formula
pois.pmf.first &amp;lt;- function (x, lambda){
  poisson.d &amp;lt;- (exp(-lambda)*((lambda^(round(x,0))))/(factorial(round(x,0))))
  return(poisson.d)
}
# To test the execution time of two formulas:
library(microbenchmark)
lambda =12
x=20

mbm &amp;lt;- microbenchmark(&amp;quot;Using first formula&amp;quot; = pois.pmf.first(x=x, lambda = lambda), 
                      &amp;quot;Using second formula&amp;quot; = pois.pmf(x=x, lambda = lambda))
mbm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: microseconds
##                  expr   min     lq     mean median    uq      max neval cld
##   Using first formula 1.889 1.9730 65.01413 2.0045 2.050 6287.776   100   a
##  Using second formula 1.845 1.9295  2.34223 1.9940 2.048   30.670   100   a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
autoplot(mbm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coordinate system already present. Adding new coordinate system, which will replace the existing one.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Functions_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;
Based on the execution time, looks like using the first formula takes a bit longer time to execute.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5&lt;/h1&gt;
&lt;p&gt;Write a function, &lt;code&gt;stat.power&lt;/code&gt; that combines calculations from Exercises 1 and 2. This function should accept &lt;span class=&#34;math inline&#34;&gt;\(m_1, s_1, m_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_2\)&lt;/span&gt; as required parameters, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; as optional parameters. This function must return a list with named elements &lt;code&gt;CV&lt;/code&gt;, &lt;code&gt;PercentDiff&lt;/code&gt;, &lt;code&gt;EffectSize&lt;/code&gt; and &lt;code&gt;RequiredReplicates&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you choose to do this exercise in SAS, you will need to write a subroutine that accepts the same parameters as the R function, but also accepts output parameters &lt;code&gt;CV&lt;/code&gt;, &lt;code&gt;PercentDiff&lt;/code&gt;, &lt;code&gt;EffectSize&lt;/code&gt; and &lt;code&gt;RequiredReplicates&lt;/code&gt;. See &lt;a href=&#34;https://blogs.sas.com/content/iml/2012/08/20/how-to-return-multiple-values-from-a-sasiml-function.html&#34; class=&#34;uri&#34;&gt;https://blogs.sas.com/content/iml/2012/08/20/how-to-return-multiple-values-from-a-sasiml-function.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another option for SAS is to package the calculations in a macro and create a data table, using the code from
Course Outline SAS Source (under Course Outline &amp;gt; Outline Source and Output Files), about line 320.&lt;/p&gt;
&lt;div id=&#34;answer-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Define your function(s) in the code chunk below, the call the function with appropriate parameters in the following sections&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function definition
combined &amp;lt;- function (m1,m2, s1,s2, alpha=0.05, beta=0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2)
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  cv &amp;lt;- (sqrt((s1^2+s2^2)/2))/((m1+m2)/2)
  percentdiff &amp;lt;- ((m1-m2)/((m1+m2)/2))
  tt &amp;lt;- (list(CV=cv, PercentDiff= percentdiff, RequiredReplicates=round(n,0), EffectSize=cohens_d))
  # attributes(tt)
  # names(tt)
  attr(tt, &amp;quot;class&amp;quot;) &amp;lt;- &amp;quot;stat.power&amp;quot; #Setting a new class
  print.stat.power(tt) #use print.stat.power function below
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you define the &lt;code&gt;class&lt;/code&gt; of the list returned by your function as &lt;code&gt;stat.power&lt;/code&gt;, this function should work automatically; you shouln’t need to call the function explicity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.stat.power &amp;lt;- function(value) {
  cat(paste(&amp;quot;Coefficient of Variation :&amp;quot;,value$CV*100,&amp;quot;\n&amp;quot;))
  cat(paste(&amp;quot;Percent Difference :&amp;quot;,value$PercentDiff*100,&amp;quot;\n&amp;quot;))
  cat(paste(&amp;quot;Effect Size :&amp;quot;,value$EffectSize,&amp;quot;\n&amp;quot;))
  cat(paste(&amp;quot;Required Replicates :&amp;quot;,value$RequiredReplicates,&amp;quot;\n&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;versus-2006-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=384.4
s1=124.8
s2=168.3
combined(m1=m1, m2=m2, s1=s1, s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coefficient of Variation : 45.4115573274988 
## Percent Difference : -35.647509578544 
## Effect Size : 0.784987603958648 
## Required Replicates : 25&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-1997-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 1997&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=288.6
s1=124.8
s2=122.0
combined(m1=m1, m2=m2, s1=s1, s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coefficient of Variation : 44.3355277160504 
## Percent Difference : -7.36482845338602 
## Effect Size : 0.166115727787307 
## Required Replicates : 569&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1997 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=288.6
m2=384.4
s1=122.0
s2=168.3
combined(m1=m1, m2=m2, s1=s1, s2=s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coefficient of Variation : 43.6803881088188 
## Percent Difference : -28.4695393759287 
## Effect Size : 0.651769377712577 
## Required Replicates : 37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Calculations</title>
      <link>/achalneupane.github.io/post/calculations/</link>
      <pubDate>Sat, 25 May 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/calculations/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;general-instructions.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General instructions.&lt;/h1&gt;
&lt;p&gt;There are 5 exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve all five problems, mixing the languages as you wish. Warning - we will be reusing the formulas from the first three exercises in later homework, so if you implement them now later exercises will be easier.&lt;/p&gt;
&lt;div id=&#34;experimental&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Experimental&lt;/h4&gt;
&lt;p&gt;I’ve been arguing that this course should also include Python. To explore this idea, I’ll allow one solution (10 of your 50 points) to be implemented in Python. To get full credit for a Python solution :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Solve one of the first three exercises.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explain how the Python solution differs from the corresonding R or SAS solution. Note the differences in the languages. For example, does Python use the same assignment operator? Are all math operators the same as R or SAS? Are the math or statistics functions loaded by default? For the first exercise, I’ve found three important differences between R and Python.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can include Python in RMarkdown by replacing &lt;code&gt;r&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt; in the code chunk prefix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I won’t be teaching Python this summer, but if you’re familiar with Python, this may help understand the inner workings of R or SAS.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1&lt;/h1&gt;
&lt;p&gt;Cohen gives a formula for effect size, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, for the difference between two means &lt;span class=&#34;math inline&#34;&gt;\(m_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m_2\)&lt;/span&gt;, as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{|m_1-m_2|}{s_{pooled}}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(s_{pooled}\)&lt;/span&gt; is a pooled standard deviation. Use the formula &lt;span class=&#34;math inline&#34;&gt;\(s_{pooled} = \sqrt{(s_1^2 + s_2^2)/2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Calculate the effect size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; for the differences among calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006. Use the values from Wansink, Table 1 as given in Homework 1 or in the course outline.&lt;/p&gt;
&lt;div id=&#34;answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Enter the R code in the chunks below. If you choose SAS for this exercise, use the marked portion in the SAS homework template.&lt;/p&gt;
&lt;div id=&#34;versus-2006&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calories per serving
#for this calcuation, m1 and m2 are mean for calories per serving for 1936 and 2006 respectively. s1 and s2 are standard deviation for 1936 vs 2006.
m1=268.1
m2=384.4
s1=124.8
s2=168.3
d_1936_2006 &amp;lt;- (abs(m1-m2)/sqrt((s1^2+s2^2)/2))
d_1936_2006&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-1997&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 1997&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calories per serving
#for this calcuation, m1 and m2 are mean for calories per serving for 1936 and 1997 respectively. s1 and s2 are standard deviation for 1936 and 1997.
m1=268.1
m2=288.6
s1=124.8
s2=122.0
d_1936_1997 &amp;lt;- (abs(m1-m2)/sqrt((s1^2+s2^2)/2))
d_1936_1997&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1661157&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1997 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calories per serving
#for this calcuation, m1 and m2 are mean for calories per serving for 1997 and 2006 respectively. s1 and s2 are standard deviation for 1997 vs 2006.
m1=288.6
m2=384.4
s1=122.0
s2=168.3
d_1997_2006 &amp;lt;- (abs(m1-m2)/sqrt((s1^2+s2^2)/2))
d_1997_2006&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6517694&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check your work, consider that Cohen recommends that &lt;span class=&#34;math inline&#34;&gt;\(d=0.2\)&lt;/span&gt; be considered a small effect, &lt;span class=&#34;math inline&#34;&gt;\(d=0.5\)&lt;/span&gt; a medium effect and &lt;span class=&#34;math inline&#34;&gt;\(d=0.8\)&lt;/span&gt; a large effect. I don’t find any of these to be fully large effects.&lt;/p&gt;
&lt;p&gt;Here, I also found none of these to be of larger effect.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2.&lt;/h1&gt;
&lt;p&gt;Suppose you are planning an experiment and you want to determine how many observations you should make for each experimental condition. One simple formula (see Kuehl, “Design of Experiments : Statistical Principles of Research Design and Analysis”) for the required replicates &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
n \ge 2\times \left( \frac{CV}{\%Diff} \right)^2 \times \left(z_{\alpha/2}+ z_\beta \right)^2
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\%Diff = \frac{m_1 - m_2}{(m_1 + m_2)/2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CV = \frac{sd_{pooled}}{(m_1 + m_2)/2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Use this formula to calculate the number of replicates required to detect differences between calories per serving, 1936 versus 2006, 1936 vs 1997 and 1997 vs 2006. You will need to research how to use the normal distribution functions (&lt;code&gt;*norm&lt;/code&gt; in R, ). Use &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.8\)&lt;/span&gt; for probabilities, and let &lt;code&gt;mean = 0&lt;/code&gt; and &lt;code&gt;sd = 1&lt;/code&gt; (both &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; should be positive).&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; must be an integer, you will need to round up. Look up the built in functions for this.&lt;/p&gt;
&lt;div id=&#34;answer-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Enter the R code in the chunks below. If you choose SAS for this exercise, use the marked portion in the SAS homework template.&lt;/p&gt;
&lt;div id=&#34;versus-2006-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha = 0.05
beta = 0.8
m1=268.1
m2=384.4
s1=124.8
s2=168.3
z.half.alpha=abs(qnorm(alpha/2, 0, 1 ))
z.half.alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.959964&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z.beta=abs(qnorm(beta, 0, 1))
z.beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8416212&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 2* (((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * ((z.half.alpha + z.beta)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-1997-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1936 versus 1997&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=268.1
m2=288.6
s1=124.8
s2=122.0
z.half.alpha=abs(qnorm(alpha/2, 0, 1 ))
z.half.alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.959964&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z.beta=abs(qnorm(beta, 0, 1))
z.beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8416212&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 2* (((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * ((z.half.alpha + z.beta)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;versus-2006-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1997 versus 2006&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1=288.6
m2=384.4
s1=122.0
s2=168.3
z.half.alpha=abs(qnorm(alpha/2, 0, 1 ))
z.half.alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.959964&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z.beta=abs(qnorm(beta, 0, 1))
z.beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8416212&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 2* (((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * ((z.half.alpha + z.beta)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check your work, use the rule of thumb suggested by van Belle (“Statistical Rules of Thumb”), where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
n= \frac{16}{\Delta^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\Delta = \frac{\mu_1 - \mu_2}{\sigma}\)&lt;/span&gt;. How does this compare with your results? Why does this rule of thumb work? How good is this rule of thumb?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rule of thumb
sigma=1
# Here, use either s1 or s2 or pooled sd for the sake of estimating. Comment from the instructor: Can we agree that the formula given in to check your work will be an approximation, and not an exact answer? If so, then does it matter if the approximate answer is based on the larger of two sd, the smaller of two sd, or some pooled value?
Delta=(m1 - m2)/s2

n= 16/Delta^2
n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 49.38069&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;p&gt;The probablity of an observation &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, when taken from a normal population with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is calculated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L (x ; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}^{}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
\]&lt;/span&gt;
For values of &lt;span class=&#34;math inline&#34;&gt;\(x = \{-0.1, 0.0, 0.1 \}\)&lt;/span&gt;, write code to calculate &lt;span class=&#34;math inline&#34;&gt;\(L (x ; \mu = 0, \sigma = 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;answer-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Enter the R code in the chunks below. If you choose SAS for this exercise, use the marked portion in the SAS homework template.&lt;/p&gt;
&lt;div id=&#34;x-0.1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=-0.1\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to calcuate the values for l_1 from above equation, first we define the values for sigma, mu and x as below and the code for the formula is shown. 
sigma=1
mu=0
x=-0.1
l_1 &amp;lt;- 1/ (sigma*sqrt(2*pi))*exp(-(x-mu)^2/2*sigma^2)
l_1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3969525&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;x0.0&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=0.0\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to calcuate the values for l_2 from above equation, first we define the values for sigma, mu and x as below and the code for the formula is shown. 
sigma=1
mu=0
x=0.0
l_2 &amp;lt;- 1/ (sigma*sqrt(2*pi))*exp(-(x-mu)^2/2*sigma^2)
l_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3989423&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;x0.1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=0.1\)&lt;/span&gt;&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to calcuate the values for l_3 from above equation, first we define the values for sigma, mu and x as below and the code for the formula is shown. 
sigma=1
mu=0
x=0.1
l_3 &amp;lt;- 1/ (sigma*sqrt(2*pi))*exp(-(x-mu)^2/2*sigma^2)
l_3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3969525&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can confirm your results using the built in normal distribution function. Look up &lt;code&gt;dnorm&lt;/code&gt; in R help and use the same values for &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt; as above. You should get matching results to at least 12 decimal places.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(abs(l_1 -dnorm(-0.1,0, 1))&amp;lt;1e-12) {
  print(&amp;quot;likelihood for x = -0.1 correct&amp;quot;)
}else{
  print(&amp;quot;likelihood for x = -0.1 incorrect&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;likelihood for x = -0.1 correct&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(abs(l_2 -dnorm(0,0, 1))&amp;lt;1e-12) {
  print(&amp;quot;likelihood for x = 0.0 correct&amp;quot;)
}else{
  print(&amp;quot;likelihood for x = 0.0 incorrect&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;likelihood for x = 0.0 correct&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(abs(l_3 -dnorm(0.1,0, 1))&amp;lt;1e-12) {
  print(&amp;quot;likelihood for x = 0.1 correct&amp;quot;)
} else {
  print(&amp;quot;likelihood for x = 0.1 incorrect&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;likelihood for x = 0.1 correct&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, this shows that the matching resuls are less than 12 decimal places.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part a&lt;/h2&gt;
&lt;p&gt;Write code to compute&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[7 - 1 \times 0 + 3 \div 3\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Type this in verbatim, using only numbers, &lt;code&gt;-&lt;/code&gt;,&lt;code&gt;*&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt;, with no parenthesis. Do you agree with the result? Explain why, one or two sentences.&lt;/p&gt;
&lt;div id=&#34;answer-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Answer&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a=7-1*0+3/3
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes I agree with the results because it follows BODMAS rule. The order of calculation should be brackets, Order, Division/Multiplication, Addition/Subtraction.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part b&lt;/h2&gt;
&lt;p&gt;According to “Why Did 74% of Facebook Users Get This Wrong?” (&lt;a href=&#34;https://profpete.com/blog/2012/11/04/why-did-74-of-facebook-users-get-this-wrong/&#34; class=&#34;uri&#34;&gt;https://profpete.com/blog/2012/11/04/why-did-74-of-facebook-users-get-this-wrong/&lt;/a&gt;), most people would compute the result as 1.
Use parenthesis &lt;code&gt;( )&lt;/code&gt; to produce this result.&lt;/p&gt;
&lt;div id=&#34;answer-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Answer&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#For this, we can get 1 if we do the substraction at first and then the multiplication later by addition and division. Therefore, the value would be wrong as it doesnot follow the rule of calculations (BODMAS). 
a=(7-1)*0+3/3
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part c&lt;/h2&gt;
&lt;p&gt;Several respondents to the survey cited in Part 2 gave the answer 6. Add &lt;em&gt;one&lt;/em&gt; set of parenthesis to produce this result.&lt;/p&gt;
&lt;div id=&#34;answer-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Answer&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a=7-1*(0+3/3)
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5.&lt;/h1&gt;
&lt;div id=&#34;part-a-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a&lt;/h3&gt;
&lt;p&gt;Quoting from Wansink and Payne&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because of changes in ingredients, the mean average calories in a recipe increased by 928.1 (from
2123.8 calories … to 3051.9 calories … ), representing a 43.7% increase.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Show how 43.7% is calculated from 2123.8 and 3051.9, and confirm W&amp;amp;P result.&lt;/p&gt;
&lt;div id=&#34;answer-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Answer&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#here we define the variable m1 and m2 as mean for total caloires for 1936 and 2006 respectively
m1=2123.8
m2=3051.9
percentchange=(m2-m1)/m1*100
round(percentchange, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 43.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 43.7&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;The resulting increase of 168.8 calories (from 268.1 calories … to 436.9 calories …) represents a 63.0% increase … in calories per serving.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;Repeat the calculations from above and confirm the reported 63.0% increase in calories per serving. Why is there such a difference between the change in calories per recipe and in calories per serving?&lt;/p&gt;
&lt;div id=&#34;answer-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Answer&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#let m1 and m2 be the mean for calories per serving for 1936 and 2006 respectively. 
m1=268.1
m2=436.9
percentchange=abs(m1-m2)/m1*100
round(percentchange, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 63&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 63.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value for the percentage change in average calories per recipe from 1936 to 2006 is 43.7% which is lower than the percent change in average calories per serving (63.0 %) because the percent change for the year 1936 to 2006 is higher for average calories per serving compared to average calories per recipe.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c&lt;/h3&gt;
&lt;p&gt;Calculate an average calories per serving by dividing average calories per recipe by average servings per recipe, for years 1936 and 2006, then calculate a percent increase. Which of the two reported increases (a or b) are consistent with this result?&lt;/p&gt;
&lt;div id=&#34;answer-8&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Answer&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Here, we divide average calories per recipe by average servings per recipe to get m1 and m2 (average calories per serving for year 1936 and 2006, respectively).
m1=2123.8/12.9
m2=3051.9/12.7
percentchange=abs(m1-m2)/m1*100
round(percentchange, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 45.96&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reported percent change here (45.96%) is somewhat consistent with 43.7 %. So it is somewhat consistent with a.&lt;/p&gt;
&lt;p&gt;Finally, I choose to work on exercise 1, 3, 4 and 5 using R and also excercise 1 using SAS.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Arrays</title>
      <link>/achalneupane.github.io/post/r_arrays/</link>
      <pubDate>Mon, 20 May 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/r_arrays/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;instructions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Instructions&lt;/h1&gt;
&lt;p&gt;There are six exercises below. You are required to provide solutions for at least four of the five. You are required to solve at least one exercise in R, and at least one in SAS. You are required to provide five solutions, each solution will be worth 10 points. Thus, you may choose to provide both R and SAS solutions for a single exercise, or you may solve all five problems, mixing the languages as you wish. The first four exercise refer to formula from the previous homework, you may reuse code as you wish.&lt;/p&gt;
&lt;div id=&#34;experimental&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Experimental&lt;/h4&gt;
&lt;p&gt;Again, you will be allowed to provide one solution using Python. Elaborate on the similarities and differences between Python arrays vs R or IML.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-1.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1.&lt;/h1&gt;
&lt;div id=&#34;part-a.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;We will calculate a number of required replicates for a range of mean differences, comparable to calories per serving estimates found in Wansink, Table 1.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(m_1\)&lt;/span&gt; be a sequence of means from 320-420, incremented by 10. Let &lt;span class=&#34;math inline&#34;&gt;\(m_2\)&lt;/span&gt; be 270. Assume a pooled standard deviation of 150.&lt;/p&gt;
&lt;p&gt;Calculate Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; for the pairs of means &lt;span class=&#34;math inline&#34;&gt;\({320 - 270, 330 - 270, ...., 420 - 270}\)&lt;/span&gt;, letting &lt;span class=&#34;math inline&#34;&gt;\(s_i = s_j = s_{pooled}\)&lt;/span&gt;. Calculate the required replicates for these same pairs of means. You may reuse code or functions from previous homework at your discretion.&lt;/p&gt;
&lt;p&gt;To show your results, either create and print a matrix with one colum for effect size and one column for replicates, or plot required replicates versus effect size (effect size will be the independent variable). What does this tell you about the number of observations required to detect medium-size effects? You may include reference lines in your plot to illustrate.&lt;/p&gt;
&lt;p&gt;Since we know that &lt;span class=&#34;math inline&#34;&gt;\(s_{pooled} = \sqrt{(s_1^2 + s_2^2)/2}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating a function with constant m2, pooled sd, alpha and beta.
combined &amp;lt;- function (m1,m2 = 270, s_pooled = 150, alpha = 0.05, beta = 0.2){
  cv &amp;lt;- (s_pooled)/((m1+m2)/2)
  percent.diff &amp;lt;- ((m1-m2)/((m1+m2)/2))
  cohens_d &amp;lt;-(abs(m1-m2)/(s_pooled))
  n &amp;lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  n &amp;lt;- round(n,0)
  value &amp;lt;- (list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0), EffectSize = cohens_d))
  return(value)
  }


m1 &amp;lt;- seq(320,420,10)
m1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 320 330 340 350 360 370 380 390 400 410 420&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- combined(m1 = m1)
# cal.lm &amp;lt;- lm(data$RequiredReplicates ~ data$EffectSize)
# or even quadratic term
# cal.lm &amp;lt;- lm(data$RequiredReplicates ~ poly(data$EffectSize, 3, raw = TRUE))
plot(data$EffectSize, data$RequiredReplicates)
# abline(cal.lm)
# As per rule of thumb for medium-size effect, we can choose v= 0.5 as medium-size effects
# http://staff.bath.ac.uk/pssiw/stats2/page2/page14/page14.html
# Also, http://staff.bath.ac.uk/pssiw/stats2/page2/page14/page14.html
abline(v = 0.5, col= &amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note: Cohen suggested that d = 0.2 be considered a ‘small’ effect size, 0.5 represents a ‘medium’ effect size and 0.8 a ‘large’ effect size. This means that if two groups’ means don’t differ by 0.2 standard deviations or more, the difference is trivial, even if it is statistically signficant. This plot tells us that with the increasing effect-size, we need fewer replicates. Since effect-size of |0.5| or (v= 0.5) is intersects with the abline at approximately 78 replicates in the plot, it also tells us that for medium-size effect we need about 78 replicates.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2&lt;/h1&gt;
&lt;p&gt;Create a table to show the required replicates for a range of combinations of &lt;span class=&#34;math inline&#34;&gt;\(\%Diff\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CV\)&lt;/span&gt;. Do this in steps as follows:&lt;/p&gt;
&lt;div id=&#34;part-a.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Define two matrices, one for &lt;code&gt;CV&lt;/code&gt; and one for &lt;code&gt;Diff&lt;/code&gt;. Each will matrix will be 5 rows by 6 columns. Let the rows in CV be the sequence &lt;span class=&#34;math inline&#34;&gt;\(8, 12, ..., 28\)&lt;/span&gt; and let the columns of &lt;code&gt;Diff&lt;/code&gt; be the squence &lt;span class=&#34;math inline&#34;&gt;\(5,10, ... , 25\)&lt;/span&gt;. The matrices should look like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} 
 CV &amp;amp; = \left\{
 \begin{array}{cccccc}
     8 &amp;amp; 12 &amp;amp; 16 &amp;amp; 20 &amp;amp; 24 &amp;amp; 28  \\
     8 &amp;amp; 12 &amp;amp; 16 &amp;amp; 20 &amp;amp; 24 &amp;amp; 28  \\
     \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots  &amp;amp; \vdots \\
     8 &amp;amp; 12 &amp;amp; 16 &amp;amp; 20 &amp;amp; 24 &amp;amp; 28  \\
   \end{array}
   \right\} \\
   &amp;amp; \\
 \%Diff &amp;amp; = \left\{
 \begin{array}{ccccc}
     5 &amp;amp; 5 &amp;amp; 5 &amp;amp; 5 &amp;amp; 5  \\
     10 &amp;amp; 10 &amp;amp; 10 &amp;amp; 10 &amp;amp; 10 \\
     \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \\
     25 &amp;amp; 25 &amp;amp; 25 &amp;amp; 25 &amp;amp; 25 \\
   \end{array}
   \right\}
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Define and print your matrices in the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create sequence data and then matrix for CV
rep.cv &amp;lt;- rep(seq(8,28,4),5)
rep.cv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  8 12 16 20 24 28  8 12 16 20 24 28  8 12 16 20 24 28  8 12 16 20 24 28  8
## [26] 12 16 20 24 28&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv &amp;lt;- matrix(rep.cv, nrow = 5, ncol = 6, byrow = TRUE)
cv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    8   12   16   20   24   28
## [2,]    8   12   16   20   24   28
## [3,]    8   12   16   20   24   28
## [4,]    8   12   16   20   24   28
## [5,]    8   12   16   20   24   28&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create sequence data and then matrix for %Diff
rep.Diff &amp;lt;- rep(seq(5,25,5), 6)
rep.Diff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  5 10 15 20 25  5 10 15 20 25  5 10 15 20 25  5 10 15 20 25  5 10 15 20 25
## [26]  5 10 15 20 25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Diff &amp;lt;- matrix(rep.Diff, nrow = 5, ncol = 6, byrow = FALSE)
Diff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    5    5    5    5    5    5
## [2,]   10   10   10   10   10   10
## [3,]   15   15   15   15   15   15
## [4,]   20   20   20   20   20   20
## [5,]   25   25   25   25   25   25&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Calculate require replicates for each combination of &lt;code&gt;CV&lt;/code&gt; and &lt;code&gt;Diff&lt;/code&gt;. Use the same values for &lt;span class=&#34;math inline&#34;&gt;\(z_\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_\beta\)&lt;/span&gt; as from Homework 2 and 3. You should be able to reuse coce from previous exercises, and you should not use iteration.&lt;/p&gt;
&lt;p&gt;Print the result below. The result should be a &lt;span class=&#34;math inline&#34;&gt;\(5 \times 6\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now vectorize the matrices CV and %Diff from above
cv.vector &amp;lt;- as.vector(cv) 
cv.vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  8  8  8  8  8 12 12 12 12 12 16 16 16 16 16 20 20 20 20 20 24 24 24 24 24
## [26] 28 28 28 28 28&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Diff.vector &amp;lt;- as.vector(Diff)
Diff.vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  5 10 15 20 25  5 10 15 20 25  5 10 15 20 25  5 10 15 20 25  5 10 15 20 25
## [26]  5 10 15 20 25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can then use the function in exercise 1 by slightly modifying it as:
combined &amp;lt;- function (cv, percent.diff, alpha = 0.05, beta = 0.2){
  cv &amp;lt;- cv
  percent.diff &amp;lt;- percent.diff
  n &amp;lt;- 2*(((cv/percent.diff)^2)*(qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  n &amp;lt;- round(n,0)
  value &amp;lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0))
  return(value)
}
value &amp;lt;- combined(cv = cv.vector, percent.diff = Diff.vector)

RequiredReplicates &amp;lt;- matrix(value$RequiredReplicates, nrow = 5, ncol = 6, byrow = FALSE)
RequiredReplicates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]   40   90  161  251  362  492
## [2,]   10   23   40   63   90  123
## [3,]    4   10   18   28   40   55
## [4,]    3    6   10   16   23   31
## [5,]    2    4    6   10   14   20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check your work, repeat the calculations using the rule of thumb from the previous exercises. What is largest deviation of the rule of thumb from the exact calculation?&lt;/p&gt;
&lt;p&gt;For this, first we can simplify the equations as follows:
first for CV,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(CV = \frac{sd_{pooled}}{(m_1 + m_2)/2}\)&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\((m_1 + m_2)/2) = \frac{sd_{pooled}}{CV/2}\)&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\((m_1 + m_2)/2) = {2} \times\frac{sd_{pooled}}{CV}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then, for &lt;span class=&#34;math inline&#34;&gt;\(\%diff\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\%Diff = \frac{m_1 - m_2}{(m_1 + m_2)/2}\)&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\%Diff\times\frac{(m_1 + m_2)}{2} = m_1-m_2\)&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(m_1-m_2 = \frac{\%Diff}{2}\times(m_1 + m_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, if we replace &lt;span class=&#34;math inline&#34;&gt;\(m_1 + m_2\)&lt;/span&gt;, we get:
&lt;span class=&#34;math inline&#34;&gt;\(m_1-m_2 = \frac{\%Diff}{2}\times ({2} \times\frac{sd_{pooled}}{CV})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or, &lt;span class=&#34;math inline&#34;&gt;\(m_1-m_2 = \frac{{\%Diff } \ \times\ {sd_{pooled}}}{CV}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will give us delta (&lt;span class=&#34;math inline&#34;&gt;\(\triangle\)&lt;/span&gt;):
&lt;span class=&#34;math inline&#34;&gt;\(\triangle = \frac{m_1-m_2}{sd_{pooled}} = \frac{1}{sd_{pooled}} \times \frac{{\%Diff } \ \times\ {sd_{pooled}}}{CV} = \frac{\%Diff}{CV}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can use, rule of thumb as:
&lt;span class=&#34;math inline&#34;&gt;\(n = \frac{16}{\triangle^2}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# As stated in rule of thumb: http://www.nrcse.washington.edu/research/struts/chapter2.pdf
# We can then use the simplified equation
rule.of.thumb.n &amp;lt;- function (cv, percent.diff){
  cv &amp;lt;- cv
  percent.diff &amp;lt;- percent.diff
  delta &amp;lt;- percent.diff/cv
  n &amp;lt;- (16/(delta^2))
  value &amp;lt;- list(CV = cv, PercentDiff= percent.diff, RequiredReplicates = round(n,0))
  return(value)
}

value &amp;lt;- rule.of.thumb.n(cv= cv.vector, percent.diff = Diff.vector)
Rule.of.Thumb.matrix &amp;lt;- matrix(value$RequiredReplicates, nrow = 5, ncol = 6, byrow = FALSE)
colnames(Rule.of.Thumb.matrix) &amp;lt;- paste0(&amp;quot;CV&amp;quot;,unique(value$CV))
rownames(Rule.of.Thumb.matrix) &amp;lt;- paste0(&amp;quot;Diff&amp;quot;,unique(value$PercentDiff))
# This matrix gives you the combination of all pairs of CV and %Diff for rule of thumb
Rule.of.Thumb.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        CV8 CV12 CV16 CV20 CV24 CV28
## Diff5   41   92  164  256  369  502
## Diff10  10   23   41   64   92  125
## Diff15   5   10   18   28   41   56
## Diff20   3    6   10   16   23   31
## Diff25   2    4    7   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can then compare the matrix from exact calculation and required replicates
# calculated using rule of thumb method by calculating the percent difference of
# two data matrices
percent_difference_between_two_df &amp;lt;- data.frame((abs(RequiredReplicates-Rule.of.Thumb.matrix)/(abs(RequiredReplicates+Rule.of.Thumb.matrix)/2))*100)
percent_difference_between_two_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              CV8     CV12      CV16     CV20     CV24     CV28
## Diff5   2.469136 2.197802  1.846154 1.972387 1.915185 2.012072
## Diff10  0.000000 0.000000  2.469136 1.574803 2.197802 1.612903
## Diff15 22.222222 0.000000  0.000000 0.000000 2.469136 1.801802
## Diff20  0.000000 0.000000  0.000000 0.000000 0.000000 0.000000
## Diff25  0.000000 0.000000 15.384615 0.000000 6.896552 0.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, %Diff of 15 and CV of 8 has the highest deviation of 22.22% between calculated vs rule of thumb method for required replicates.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 3&lt;/h1&gt;
&lt;p&gt;In this exercise, we’ll use your &lt;code&gt;norm.pdf&lt;/code&gt; function to illustrate how the formula for required replicates finds a compromise between Type I and Type II error rates. This is also a way to test your normal probability function over a range of arguments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do not print the vectors you create for this exercise in the final typeset submission&lt;/strong&gt; We will check the results by examining the plots, and printing the vectors themselves will unnecessarily clutter your report. If you get stuck, use the built normal functions to create your plots.&lt;/p&gt;
&lt;div id=&#34;part-a.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a.&lt;/h3&gt;
&lt;p&gt;Generate a squence of values from &lt;span class=&#34;math inline&#34;&gt;\(-3,...,4\)&lt;/span&gt; incremented by &lt;span class=&#34;math inline&#34;&gt;\(0.1\)&lt;/span&gt;; let this be &lt;code&gt;x&lt;/code&gt;.
Calculate the probability of each value of &lt;code&gt;x&lt;/code&gt; using the &lt;code&gt;norm.pdf&lt;/code&gt; function from Homework 3, letting &lt;code&gt;mu = 0&lt;/code&gt; and &lt;code&gt;sd = 1&lt;/code&gt;. Name the result &lt;code&gt;p.null&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Calculate the effect size for 1936 versus 2006, calories per serving, as in Homework 2 and 3. Repeat the calculation for the probability of &lt;code&gt;x&lt;/code&gt;, but this time use &lt;code&gt;mean=&lt;/code&gt; effect size. Name this result &lt;code&gt;p.alt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The results will be the distribution of the expected value of the difference between means; the first is the expectation under the null hypothesis (the true effect size is 0) while the second is the expectation assuming the measured &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the true &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function to calcuate the values for log liklihood.
# First, we define the values for sigma as variance, 
# mu as mean of a normal population to be used for a liklihood of a x observation.
norm.pdf &amp;lt;- function(x,mu = 0,sigma = 1){
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}

x &amp;lt;- seq(-3, 4, 0.1)
p.null &amp;lt;- norm.pdf(x)

#calculating effect size to be used as &amp;#39;mean= &amp;#39; for the analysis below
combined &amp;lt;- function (m1,m2,s1,s2, alpha = 0.05, beta = 0.2){
  cohens_d &amp;lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  value &amp;lt;- (list(EffectSize = cohens_d))
}

#calculate the effect size for 1936 vs 2006, calories per serving
m1 = 268.1
m2 = 384.4
s1 = 124.8
s2 = 168.3
value &amp;lt;- combined(m1 = m1, m2 = m2, s1 = s1, s2 = s2)
value$EffectSize&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now repeating the calculation 
mean = value$EffectSize
mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p.alt &amp;lt;- norm.pdf(x, mu = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b.&lt;/h3&gt;
&lt;p&gt;Repeat the calculations of &lt;code&gt;p.null&lt;/code&gt; and &lt;code&gt;p.alt&lt;/code&gt;, but this time let &lt;code&gt;sigma =&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{2/n}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt;. Name these &lt;code&gt;p.null.10&lt;/code&gt; and &lt;code&gt;p.alt.10&lt;/code&gt;. These calculations narrow the distribbutions by an amount proportional to standard error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10
norm.pdf &amp;lt;- function(x, n, mu = 0){
  sigma = (sqrt(2/n))
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}

x &amp;lt;- seq(-3, 4, 0.1)
p.null.10 &amp;lt;- norm.pdf(x = x, n = n)

mean = value$EffectSize
mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p.alt.10 &amp;lt;- norm.pdf(x = x, n = n, mu = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part c.&lt;/h3&gt;
&lt;p&gt;Repeat the calculations of &lt;code&gt;p.null&lt;/code&gt; and &lt;code&gt;p.alt&lt;/code&gt;, but this time let &lt;code&gt;sigma =&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{2/n}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the minimun mumber of replicates for calories per recipe, 1936 versus 2006, as calculated previously. Call these &lt;code&gt;p.null.req&lt;/code&gt; and &lt;code&gt;p.alt.req&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Re-using the function to calculate minimum required replicates
required.replicates &amp;lt;- function (m1,m2, s1,s2, alpha = 0.05, beta = 0.2){
  n &amp;lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}

m1 = 268.1
m2 = 384.4
s1 = 124.8
s2 = 168.3

n &amp;lt;- required.replicates(m1 = m1, m2 = m2, s1 = s1, s2 = s2)
n.min.replicates &amp;lt;- n
n.min.replicates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.pdf &amp;lt;- function(x, n, mu = 0){
  sigma = (sqrt(2/n))
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}

x &amp;lt;- seq(-3, 4, 0.1)
p.null.req &amp;lt;- norm.pdf(x = x, n = n)


mean = value$EffectSize
mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7849876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p.alt.req &amp;lt;- norm.pdf(x = x, n = n, mu = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-d.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part d.&lt;/h3&gt;
&lt;p&gt;Plot &lt;code&gt;p.null&lt;/code&gt; versus &lt;code&gt;x&lt;/code&gt; as a black line and in the same plot add &lt;code&gt;p.alt&lt;/code&gt; vs &lt;code&gt;x&lt;/code&gt; as a red line. Add a green vertical line at &lt;span class=&#34;math inline&#34;&gt;\(z_\alpha\)&lt;/span&gt; and a blue vertical line at &lt;span class=&#34;math inline&#34;&gt;\(z_\beta\)&lt;/span&gt;, using values as in previous exercises. The green line represents the critical value for Type I error, and the error under the black curve to the left of the green line is the probability of that error (97.5%). The area under the red curve, to the left of the green line, represents the achieved Type II error rate, the blue line represents the desired Type II rate.&lt;/p&gt;
&lt;p&gt;Repeat the plot with &lt;code&gt;p.null.10&lt;/code&gt; and &lt;code&gt;p.alt.10&lt;/code&gt;, but this time add vertical lines at &lt;span class=&#34;math inline&#34;&gt;\(z_\alpha \times \sqrt{2/10}\)&lt;/span&gt; and at &lt;span class=&#34;math inline&#34;&gt;\(z_\beta \times \sqrt{2/10}\)&lt;/span&gt;. The lines representing critical values for Type I and Type II error should move closer as the distributions narrow.&lt;/p&gt;
&lt;p&gt;Repeat the plot with &lt;code&gt;p.null.req&lt;/code&gt; and &lt;code&gt;p.alt.req&lt;/code&gt;, but this time add vertical lines at &lt;span class=&#34;math inline&#34;&gt;\(z_\alpha \times \sqrt{2/n}\)&lt;/span&gt; and at &lt;span class=&#34;math inline&#34;&gt;\(z_\beta \times \sqrt{2/n}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the minimum replicates. Do the lines for Type I and Type II error overlap?&lt;/p&gt;
&lt;p&gt;It will improve the readability of the three plots if you plot all three in the chunk below. The arguments inside the braces specify the dimensions of the plot, while &lt;code&gt;par(mfrow = c(3,1))&lt;/code&gt; combines three plots into one graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(3,1))
# using base plot function part A
alpha = 0.05; beta = 0.2
Zalpha &amp;lt;- qnorm(1-alpha/2)
Zbeta &amp;lt;- qnorm(1-beta)
plot(x,p.null,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x,p.alt,col=&amp;quot;red&amp;quot;)
abline(v = Zalpha, col= &amp;#39;green&amp;#39;)
abline(v = value$EffectSize-Zbeta, col= &amp;#39;blue&amp;#39;)

### using ggplot
# plot1 &amp;lt;- ggplot()+
#   geom_line(aes(x = x, y = p.null), color = &amp;quot;black&amp;quot; ) +
#   geom_line(aes(x = x, y =  p.alt), color = &amp;quot;red&amp;quot;) +
#   geom_vline(xintercept = Zalpha, color = &amp;quot;green&amp;quot;)+
#   geom_vline(xintercept = value$EffectSize - Zbeta, color = &amp;quot;blue&amp;quot;)
# plot1

# using base plot function Part B
alpha = 0.05; beta = 0.2; n &amp;lt;- 10
Zalpha &amp;lt;- qnorm(1-alpha/2)
Zalpha &amp;lt;- Zalpha * sqrt(2/n)
Zbeta &amp;lt;- qnorm(1-beta)
Zbeta &amp;lt;- Zbeta * sqrt(2/n)
plot(x,p.null.10,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x,p.alt.10,col=&amp;quot;red&amp;quot;)
abline(v = Zalpha, col= &amp;#39;green&amp;#39;)
abline(v = (value$EffectSize-Zbeta), col= &amp;#39;blue&amp;#39;)

## Using ggplot
# plot2 &amp;lt;- ggplot()+
#   geom_line(aes(x = x, y = p.null.10), color = &amp;quot;black&amp;quot; ) +
#   geom_line(aes(x = x, y =  p.alt.10), color = &amp;quot;red&amp;quot;) +
#   geom_vline(xintercept = Zalpha, color = &amp;quot;green&amp;quot;) +
#   geom_vline(xintercept = value$EffectSize - Zbeta, color = &amp;quot;blue&amp;quot;)
# plot2

# using base plot function Part C
alpha = 0.05; beta = 0.2; n &amp;lt;- n.min.replicates
Zalpha &amp;lt;- qnorm(1-alpha/2)
Zalpha &amp;lt;- Zalpha * sqrt(2/n)
Zbeta &amp;lt;- qnorm(1-beta)
Zbeta &amp;lt;- Zbeta * sqrt(2/n)
plot(x,p.null.req,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x,p.alt.req,col=&amp;quot;red&amp;quot;)
abline(v = Zalpha, col= &amp;#39;green&amp;#39;)
abline(v = value$EffectSize-Zbeta, col= &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Using ggplot
# plot3 &amp;lt;- ggplot()+
#   geom_line(aes(x = x, y = p.null.req), color = &amp;quot;black&amp;quot; ) +
#   geom_line(aes(x = x, y =  p.alt.req), color = &amp;quot;red&amp;quot;) +
#   geom_vline(xintercept = Zalpha, color = &amp;quot;green&amp;quot;) +
#   geom_vline(xintercept = value$EffectSize - Zbeta, color = &amp;quot;blue&amp;quot;)
# plot3

# # Then merge all ggplots:
# library(grid)
# library(gridExtra)
# grid.newpage()
# grid.draw(arrangeGrob(plot1, plot2, plot3, heights = c(1/3, 1/3, 1/3)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, The lines representing critical values for Type I and Type II error move closer as the distributions narrow. Yes, the lines for Type I and Type II error overlap.&lt;/p&gt;
&lt;p&gt;Note: z_beta should be plotted relative to the alternative hypothesis value (in this case, d), since z_beta is the critical value to control for Type II error, where we would (fail to reject)* the null hypothesis and miss a true effect. Thus, the line for z_beta needs to be plotted relative to d, and it needs to be offset to the left, so you should be plotting a line at d-z_beta, etc. The alternative hypothesis is that the effect size we measured, d, is the true effect size.
Perhaps another way to phrase this is, the right-hand distribution illustrates values of the true effect size d that are consistent with a measured effect size. There’s a big debate in statistics on the use of null hypothesis tests. Perhaps a better visualization would be to demonstrate how narrowing the distributions (2, 10, n) changes likelihood ratio or Bayes factor.&lt;/p&gt;
&lt;p&gt;If you choose to solve this with SAS, I’ve included code in the SAS template to create the graphs, since combining plots in IML is not as easy as in R.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 4&lt;/h1&gt;
&lt;p&gt;In this, we compare the normal and Poisson distributions, using the functions you’ve written previously. This is also a way to test your normal and Poisson functions over a range of arguments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do not print the vectors you create for this exercise in the final typeset submission&lt;/strong&gt; We will check the results by examining the plots, and printing the vectors themselves will unnecessarily clutter your report. If you get stuck, use the built functions to create your plots. However, the final submission must call your functions.&lt;/p&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a&lt;/h3&gt;
&lt;p&gt;Create a sequence of &lt;span class=&#34;math inline&#34;&gt;\(x_a\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(( -5 ... 5 )\)&lt;/span&gt;, incremented by 0.1. Calculate the normal likelihood for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, assuming &lt;span class=&#34;math inline&#34;&gt;\(\mu = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1\)&lt;/span&gt;. Also calculate Poisson probability of each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; given a &lt;code&gt;lambda = 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;NOTE: The Poisson parameter Lambda (λ) is the total number of events (k) divided by the number of units (n) in the data (λ = k/n)&lt;/p&gt;
&lt;p&gt;Plot both sets of probablities against &lt;code&gt;x&lt;/code&gt; as lines, using a different color for each curve. Make sure that both curves fit in the plot; you may need to determine minimum and maximum values and set these as graphic parameters (see &lt;code&gt;ylim&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Warning: if you do this in SAS, you may have to adjust the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Suppressing warnings that may be generated by Poisson function for negative values
# options(warn=-1)

x_a &amp;lt;- seq(-5, 5, 0.1)
x_a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] -5.0 -4.9 -4.8 -4.7 -4.6 -4.5 -4.4 -4.3 -4.2 -4.1 -4.0 -3.9 -3.8 -3.7 -3.6
##  [16] -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 -2.2 -2.1
##  [31] -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6
##  [46] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
##  [61]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4
##  [76]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9
##  [91]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.pdf &amp;lt;- function(x,mu=0,sigma=1){
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}
normal.liklihood.x_a &amp;lt;- norm.pdf(x_a)


# The function to calculate probability mass function for poisson 
# data with a mean and variance lambda = 1. 
pois.pmf &amp;lt;- function(x, lambda){
  poisson.d &amp;lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda)))
  return(poisson.d)
}
lambda &amp;lt;- 1
poisson.probability.x_a &amp;lt;- pois.pmf(x=x_a, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in gamma(x + 1): NaNs produced&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x_a,normal.liklihood.x_a,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_a,poisson.probability.x_a,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Does this graph tell you if your Normal PDF function behaves properly? Does your Poisson handle negative or non-integer values as expected?&lt;/p&gt;
&lt;p&gt;No, based on this plot, the normal pdf does behave properly, but not the poisson as there NAs inserted for negative and non-integer values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;Create a sequence of &lt;span class=&#34;math inline&#34;&gt;\(x_b = \left \lfloor{\mu - 5 \times \sigma } \right \rfloor , \dots, \left \lceil{\mu+ 5 \times \sigma } \right \rceil\)&lt;/span&gt; using mean and standard deviation for servings per recipe from 1936.&lt;/p&gt;
&lt;p&gt;Calculate the normal and Poission probability for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; as in part a, again using mean and standard deviation from servings per recipe, 1936. The length of this vector should be the same length as the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; vector as in part a (&lt;span class=&#34;math inline&#34;&gt;\(\pm 1\)&lt;/span&gt;), so you will need to calculate an interval based on the range &lt;code&gt;x_b&lt;/code&gt; and the number of elements in &lt;code&gt;x_a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Show the the length of both &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; vectors are similar by calling &lt;code&gt;length&lt;/code&gt; for each.&lt;/p&gt;
&lt;p&gt;Repeat the plot from part a with this sequence.&lt;/p&gt;
&lt;p&gt;If you choose to solve this with SAS, I’ve included code in the SAS template to create the graphs, since combining plots in IML is not as easy as in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using mean and standard deviation for servings per recipe from 1936:
mu = 12.9; sigma = 13.3

# Now we find the upper and lower bounds as:
x.lower &amp;lt;- floor(mu-5*sigma)
x.upper &amp;lt;- ceiling(mu+5*sigma)

# Now taking the length of x_a as a reference, we create the equeally spaced
# sequence from lower to upper bound as followed:
spacer &amp;lt;- (x.upper - x.lower)/(length(x_a) - 1)

x_b &amp;lt;- seq(x.lower, x.upper, spacer)
# To show both x_a and x_b &amp;#39;s lenghts are equal:
length(x_a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(x_b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;norm.pdf &amp;lt;- function(x, mu = 12.9, sigma = 13.3){
  l&amp;lt;-1/(sigma*sqrt(pi*2))*exp(-((x-mu)^2)/(2*sigma^2))
  return(l)
}
normal.liklihood.x_b &amp;lt;- norm.pdf(x_b)


# The function to calculate probability mass function for poisson 
# data with a mean and variance lambda = 12. 
pois.pmf &amp;lt;- function(x, lambda){
  poisson.d &amp;lt;- exp(-lambda)*(1/(factorial(round(x,0))))*exp(round(x,0)*(log(lambda)))
  return(poisson.d)
}

#using sigma = 13.3 to compare the difference with the first plot
poisson.probability.x_b &amp;lt;- pois.pmf(x=x_b, lambda = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in gamma(x + 1): NaNs produced&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot
plot(x_b,normal.liklihood.x_b,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_b,poisson.probability.x_b,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To check you work, duplicate the plots by calling built in normal and Poisson functions. Does the system Poisson function handle negative &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; differently than your function?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using base functions for part a  
base.normal.liklihood.x_a &amp;lt;- dnorm(x_a,0, 1)
base.poiss.x_a &amp;lt;- dpois(x=x_a, lambda = lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -4.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -3.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -2.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -1.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = -0.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 0.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 1.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 2.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 3.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_a, lambda = lambda): non-integer x = 4.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x_a,base.normal.liklihood.x_a,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_a,base.poiss.x_a,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using base functions for part b
base.normal.liklihood.x_b &amp;lt;- dnorm(x_b,0, 1)
base.poiss.x_b &amp;lt;- dpois(x=x_b, lambda = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -52.660000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -51.320000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -49.980000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -48.640000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -47.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -45.960000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -44.620000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -43.280000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -41.940000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -40.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -39.260000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -37.920000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -36.580000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -35.240000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -33.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -32.560000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -31.220000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -29.880000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -28.540000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -27.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -25.860000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -24.520000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -23.180000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -21.840000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -20.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -19.160000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -17.820000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -16.480000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -15.140000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -13.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -12.460000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -11.120000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -9.780000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -8.440000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -7.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -5.760000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -4.420000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -3.080000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -1.740000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = -0.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 0.940000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 2.280000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 3.620000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 4.960000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 6.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 7.640000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 8.980000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 10.320000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 11.660000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 14.340000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 15.680000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 17.020000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 18.360000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 19.700000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 21.040000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 22.380000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 23.720000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 25.060000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 26.400000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 27.740000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 29.080000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 30.420000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 31.760000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 33.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 34.440000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 35.780000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 37.120000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 38.460000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 39.800000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 41.140000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 42.480000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 43.820000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 45.160000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 46.500000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 47.840000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 49.180000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 50.520000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 51.860000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 53.200000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 54.540000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 55.880000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 57.220000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 58.560000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 59.900000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 61.240000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 62.580000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 63.920000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 65.260000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 66.600000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 67.940000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 69.280000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 70.620000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 71.960000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 73.300000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 74.640000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 75.980000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 77.320000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in dpois(x = x_b, lambda = 12): non-integer x = 78.660000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x_b,base.normal.liklihood.x_b,type=&amp;quot;l&amp;quot;,col=&amp;quot;black&amp;quot;)
lines(x_b,base.poiss.x_b,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, the system Poisson function handles negative values and non-integers differently (inserts zero’s) whereas the function we wrote inserts NAs for negative and non-integer values.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 5&lt;/h1&gt;
&lt;p&gt;Consider the table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Rate&lt;/th&gt;
&lt;th&gt;23000&lt;/th&gt;
&lt;th&gt;24000&lt;/th&gt;
&lt;th&gt;25000&lt;/th&gt;
&lt;th&gt;26000&lt;/th&gt;
&lt;th&gt;27000&lt;/th&gt;
&lt;th&gt;28000&lt;/th&gt;
&lt;th&gt;29000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Yield&lt;/td&gt;
&lt;td&gt;111.4216&lt;/td&gt;
&lt;td&gt;155.0326&lt;/td&gt;
&lt;td&gt;181.1176&lt;/td&gt;
&lt;td&gt;227.5800&lt;/td&gt;
&lt;td&gt;233.4623&lt;/td&gt;
&lt;td&gt;242.1753&lt;/td&gt;
&lt;td&gt;231.3890&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Suppose we wish to determine the linear relationship between per Rate and Yield. We can determine this by solving a system of linear equations, of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
111.4216 &amp;amp; = \beta_1 + \beta_2 \times 23000 \\
155.0326 &amp;amp; = \beta_1 + \beta_2 \times 24000  \\
\vdots &amp;amp; = \vdots \\
231.3890 &amp;amp; = \beta_1 + \beta_2 \times 29000 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We write this in matrix notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left(\begin{array}{c}
111.4216 \\
155.0326 \\
\vdots \\
231.3890 
 \end{array}\right) 
 =
 \left(\begin{array}{rr}
 1 &amp;amp; 23000 \\
 1 &amp;amp; 24000  \\
\vdots &amp;amp; \vdots \\
 1 &amp;amp; 29000
 \end{array}\right) 
 \left(\begin{array}{c}
 \beta_1 \\
 \beta_2
 \end{array}\right)^t
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We might write this as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{y} = \mathbf{X} \mathbf{\beta}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and find a solution by computing &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\hat{\beta}} = \mathbf{X}^{- 1}\mathbf{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, an exact solution for the inverse, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^{- 1}\)&lt;/span&gt; require square matrices, so commonly we use the &lt;em&gt;normal&lt;/em&gt; equations,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mathbf{X}^{t}  \mathbf{y} = \mathbf{X}^{t} \mathbf{X}  \mathbf{\beta} \]&lt;/span&gt;
(where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^{t}\)&lt;/span&gt; is the transpose of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;). We then find &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{\beta}} = \mathbf{X}^{t} \mathbf{X} ^{-1} \mathbf{X}^{t} \mathbf{y}\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;answer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Answer&lt;/h3&gt;
&lt;p&gt;Define appropriate &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; matrices (&lt;code&gt;y&lt;/code&gt; can be a vector in R) in the chunk below.&lt;/p&gt;
&lt;p&gt;Multiply the transpose of &lt;code&gt;X&lt;/code&gt; by &lt;code&gt;X&lt;/code&gt;, then use &lt;code&gt;solve&lt;/code&gt; (R) or &lt;code&gt;inv&lt;/code&gt; (IML) to find the inverse. Multiply this by the product of transpose &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to find &lt;code&gt;hat.beta&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Print your &lt;code&gt;hat.beta&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- matrix( c(111.4216, 155.0326, 181.1176, 227.5800, 233.4623, 242.1753, 231.3890), byrow = FALSE)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]
## [1,] 111.4216
## [2,] 155.0326
## [3,] 181.1176
## [4,] 227.5800
## [5,] 233.4623
## [6,] 242.1753
## [7,] 231.3890&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#creating a matrix for bias term
bias=rep(1:1, length.out=length(y))
bias&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cx &amp;lt;- c(23000, 24000, 25000, 26000 , 27000, 28000, 29000)
X=matrix(c(bias,cx), ncol = 2)
X&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]  [,2]
## [1,]    1 23000
## [2,]    1 24000
## [3,]    1 25000
## [4,]    1 26000
## [5,]    1 27000
## [6,]    1 28000
## [7,]    1 29000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#multplication of transpose of x and x
tX=t(X)
tX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]
## [1,]     1     1     1     1     1     1     1
## [2,] 23000 24000 25000 26000 27000 28000 29000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Xm=tX%*%X
Xm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]     [,2]
## [1,]      7 1.82e+05
## [2,] 182000 4.76e+09&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A=solve(Xm)

hat.beta=A%*%(tX%*%y)
hat.beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               [,1]
## [1,] -347.18307857
## [2,]    0.02094758&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check your work, calculate the values predicted by your statistical model. Compute &lt;code&gt;hat.y&lt;/code&gt; by multiplying &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;hat.beta&lt;/code&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \mathbf{X}  \hat{\beta}\]&lt;/span&gt;
Plot &lt;code&gt;y&lt;/code&gt; vs the independent variable (the second column of &lt;code&gt;X&lt;/code&gt;) as points, and &lt;code&gt;hat.y&lt;/code&gt; vs independent variable as a line, preferably a different colors. The &lt;code&gt;hat.y&lt;/code&gt; values should fall a straight line that interpolates &lt;code&gt;y&lt;/code&gt; values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute hat.y 
hat.y &amp;lt;- X %*% hat.beta
# plot
plot(X[,2], y, type = &amp;#39;l&amp;#39;, col = &amp;quot;black&amp;quot;)
lines(X[,2], hat.y, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/R_Arrays_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also compare your result to the R function (set &lt;code&gt;eval = TRUE&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y~X))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##        1        2        3        4        5        6        7 
## -23.1897  -0.5263   4.6111  30.1259  15.0607   2.8261 -28.9078 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept) -3.472e+02  1.110e+02  -3.127   0.0260 * 
## X1                  NA         NA      NA       NA   
## X2           2.095e-02  4.257e-03   4.920   0.0044 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 22.53 on 5 degrees of freedom
## Multiple R-squared:  0.8288, Adjusted R-squared:  0.7946 
## F-statistic: 24.21 on 1 and 5 DF,  p-value: 0.004396&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on my calculation the beta estimates are similar to the beta estimates from the liner model&lt;/p&gt;
&lt;div id=&#34;alternative-methods&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Alternative methods&lt;/h4&gt;
&lt;p&gt;You can also compute &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; by passing both &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^{t} \mathbf{X} ^{-1}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^{t} \mathbf{y}\)&lt;/span&gt; as arguments to &lt;code&gt;solve&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Alternatively, you can install the &lt;code&gt;MASS&lt;/code&gt; library and use &lt;code&gt;ginv&lt;/code&gt; to compute a generalized inverse &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^{- 1}\)&lt;/span&gt;. Use this to compute &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\hat{\beta}} = \mathbf{X}^-\mathbf{y}\)&lt;/span&gt; in the chunk below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 6&lt;/h1&gt;
&lt;p&gt;Given a vector of mean estimates &lt;span class=&#34;math inline&#34;&gt;\(x = x_1, x_2, \dots, x_k\)&lt;/span&gt;, a vector of standard deviations &lt;span class=&#34;math inline&#34;&gt;\(s = s_1, s_2, \dots, s_k\)&lt;/span&gt; and a vector of sample sizes &lt;span class=&#34;math inline&#34;&gt;\(n = n_1, n_2, \dots, n_k\)&lt;/span&gt;, we can calculate a one-way analysis of variance by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSB = \frac{n_1(x_1-\bar{x})^2 + n_2(x_2-\bar{x})^2 + \dots + n_k(x_k-\bar{x})^2} {k-1} = \frac{\sum_i n_i(x_i-\bar{x})^2}{k-1}
\]&lt;/span&gt;
and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSW = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \dots (n_k-1)s_k^2 }{N-k} = \frac{\sum_i (n_i-1)s_i^2}{N-k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; is the mean of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_i n_i\)&lt;/span&gt;. The test statistic is &lt;span class=&#34;math inline&#34;&gt;\(F = \frac{MSB}{MSW}\)&lt;/span&gt; which is distributed as &lt;span class=&#34;math inline&#34;&gt;\(F_{\alpha,k-1,N-k}\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;part-a-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part a&lt;/h3&gt;
&lt;p&gt;Calculate MSW and MSB for Calories per Serving from Wansink Table 1. You can use the variables &lt;code&gt;CaloriesPerServingMean&lt;/code&gt; and &lt;code&gt;CaloriesPerServingSD&lt;/code&gt; defined below. Let &lt;span class=&#34;math inline&#34;&gt;\(n_1 = n_2 ... = n_k = 18\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Use array functions and arithmetic for your calculations, you should not need iteration (for loops). Do not hard code values for &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, calculate these from the &lt;code&gt;CaloriesPerServingMean&lt;/code&gt; or &lt;code&gt;CaloriesPerServingSD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Print both MSB and MSW.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CaloriesPerServingMean &amp;lt;- c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4)
CaloriesPerServingSD &amp;lt;- c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3)
#mean for servingperrecipe
mean = mean(CaloriesPerServingMean)
mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 296.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- length(CaloriesPerServingMean)
k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We have 18 samples in each year group; so our N is:
n &amp;lt;- rep(18, k)
N &amp;lt;- sum(n)
#calculating MSB 
mean.CaloriesPerServingMean &amp;lt;- mean(CaloriesPerServingMean)
MSB = (sum(n*(CaloriesPerServingMean-mean.CaloriesPerServingMean)^2))/(k-1)
MSB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 28815.96&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MSW &amp;lt;- sum((n-1) * CaloriesPerServingSD^2)/(N-k)
MSW&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16508.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part b&lt;/h3&gt;
&lt;p&gt;Calculate an F-ratio and a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for this &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, using the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; distribution with &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N-k\)&lt;/span&gt; degrees of freedom. Use &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;. Compare these values to the corresponding values reported in Wansink Table 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculating F.ratio
F.ratio &amp;lt;- MSB/MSW
df1 &amp;lt;- k-1
df2 &amp;lt;- N-k
p.value &amp;lt;- pf(F.ratio, df1, df2, lower.tail = FALSE)
p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1163133&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can check your results by entering appropriate values in an online calculator like &lt;a href=&#34;http://statpages.info/anova1sm.html&#34; class=&#34;uri&#34;&gt;http://statpages.info/anova1sm.html&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;Group Name N(count) Mean Std.Dev
Group1 18 268.1 124.8
Group2 18 271.1 124.2
Group3 18 280.9 116.2
.
.
.
Group7 18 384.4 168.3&lt;/p&gt;
&lt;p&gt;Then set Desired confidence level for post-hoc confidence intervals: 5&lt;/p&gt;
&lt;p&gt;Finally, cross-check your answers!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started</title>
      <link>/achalneupane.github.io/post/getting_started/</link>
      <pubDate>Wed, 15 May 2019 17:26:23 -0500</pubDate>
      <guid>/achalneupane.github.io/post/getting_started/</guid>
      <description>


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;instructions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Instructions&lt;/h1&gt;
&lt;p&gt;This document runs a simple analysis of the Table 1 from &lt;span class=&#34;citation&#34;&gt;[@wansink2009joy]&lt;/span&gt;. Edit the header information to show your name and the date you complete the assignment.&lt;/p&gt;
&lt;p&gt;Modify this document to analyze either Calories per Serving or Servings per Recipe. Document any changes you make in the literate portion of the file. Comment on your choice of measure to analyze.&lt;/p&gt;
&lt;p&gt;Change the name of this file to match your user name on D2L, keeping the ‘Rmd’ extension, and include week number in the title (for example, &lt;code&gt;Peter.Claussen.1.Rmd&lt;/code&gt;). Upload this file to D2L. Typeset this file to Word or PDF and upload the result to D2L as well.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;table&gt;
&lt;caption&gt;Mean and (SD) for selected recipes from “Joy of Cooking”&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Measure&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1936&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1946&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1951&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1963&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1975&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1997&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2006&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;calories per recipe (SD)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2123.8 (1050.0)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2122.3 (1002.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2089.9 (1009.6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2250.0 (1078.6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2234.2 (1089.2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2249.6 (1094.8)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3051.9 (1496.2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;calories per serving (SD)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.1 (124.8)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;271.1 (124.2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;280.9 (116.2)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;294.7 (117.7)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;285.6 (118.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;288.6 (122.0)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;384.4 (168.3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;servings per recipe (SD)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.9 (13.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.9 (13.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.0 (14.5)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.7 (14.6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.4 (14.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.4 (14.3)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.7 (13.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis&lt;/h1&gt;
&lt;div id=&#34;enter-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Enter data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CookingTooMuch.dat &amp;lt;- data.frame(
  Year=c(1936, 1946, 1951, 1963, 1975, 1997, 2006),
  CaloriesPerRecipeMean = c(2123.8, 2122.3, 2089.9, 2250.0, 2234.2, 2249.6, 3051.9),
  CaloriesPerRecipeSD = c(1050.0, 1002.3, 1009.6, 1078.6, 1089.2, 1094.8, 1496.2),
  CaloriesPerServingMean = c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4),
  CaloriesPerServingSD = c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3),
  ServingsPerRecipeMean = c(12.9, 12.9, 13.0, 12.7, 12.4, 12.4, 12.7),
  ServingsPerRecipeSD = c(13.3, 13.3, 14.5, 14.6, 14.3, 14.3, 13.0)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-values-for-confidence-interval-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create values for confidence interval plot&lt;/h3&gt;
&lt;p&gt;Wansink reports that 18 recipes were analyzed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 18&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assume a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; of 5%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use standard formula for standard error &lt;span class=&#34;math inline&#34;&gt;\(\sigma / \sqrt{n}\)&lt;/span&gt; and confidence interval &lt;span class=&#34;math inline&#34;&gt;\(t_{\alpha/2} \times s.e.\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StandardError &amp;lt;- function(sigma, n) {
  sigma/sqrt(n)
}
ConfidenceInterval &amp;lt;- function(sigma, n) {
  qt(1-alpha/2, Inf)*StandardError(sigma,n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a variable for plotting and calculate upper and lower bounds using confidence intervals.
For this assignment, I am plotting ServingsPerRecipe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PlotCookingTooMuch.dat &amp;lt;- CookingTooMuch.dat
PlotCookingTooMuch.dat$ServingsPerRecipe &amp;lt;-
  PlotCookingTooMuch.dat$ServingsPerRecipeMean
PlotCookingTooMuch.dat$Lower &amp;lt;-
  PlotCookingTooMuch.dat$ServingsPerRecipe - ConfidenceInterval(CookingTooMuch.dat$ServingsPerRecipeSD, n)
PlotCookingTooMuch.dat$Upper &amp;lt;-
  PlotCookingTooMuch.dat$ServingsPerRecipe + ConfidenceInterval(CookingTooMuch.dat$ServingsPerRecipeSD, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we are selecting only ServingsPerRecipe variable for plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PlotCookingTooMuch.dat &amp;lt;-
  PlotCookingTooMuch.dat[, c(&amp;quot;Year&amp;quot;, &amp;quot;ServingsPerRecipe&amp;quot;, &amp;quot;Lower&amp;quot;, &amp;quot;Upper&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examine the values to make sure we’ve entered correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(PlotCookingTooMuch.dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Year ServingsPerRecipe    Lower    Upper
## 1 1936              12.9 6.755826 19.04417
## 2 1946              12.9 6.755826 19.04417
## 3 1951              13.0 6.301465 19.69854
## 4 1963              12.7 5.955268 19.44473
## 5 1975              12.4 5.793858 19.00614
## 6 1997              12.4 5.793858 19.00614
## 7 2006              12.7 6.694417 18.70558&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Couldn’t find the confidence intervals for ServingsPerRecipe in Wanskins report
for 1936 and 2006. So I am using the Reference CI as calculated CI using
confidence interval function above. So here, I am using ComValues as
ReferneceValues&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CompValues &amp;lt;- PlotCookingTooMuch.dat[c(1, 7), c(&amp;quot;Lower&amp;quot;, &amp;quot;Upper&amp;quot;)]
#ReferenceValues &amp;lt;- matrix(c(1638.7, 2608.9, 2360.7, 3743.1),nrow=2,byrow=TRUE)
ReferenceValues &amp;lt;- CompValues
CompValues&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Lower    Upper
## 1 6.755826 19.04417
## 7 6.694417 18.70558&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ReferenceValues&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Lower    Upper
## 1 6.755826 19.04417
## 7 6.694417 18.70558&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;any(abs(CompValues - ReferenceValues) &amp;gt; 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We no longer need the original data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CookingTooMuch.dat &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-table&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot the table&lt;/h1&gt;
&lt;p&gt;#Here, changed the title and y and x labels to Servings Per Recipe&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(
  ServingsPerRecipe ~ Year,
  data = PlotCookingTooMuch.dat,
  col = &amp;quot;blue&amp;quot;,
  pch = 19,
  main = &amp;quot;Servings per Recipe&amp;quot;,
  ylab = &amp;quot;Servings&amp;quot;,
  ylim = c(
    min(PlotCookingTooMuch.dat$Lower),
    max(PlotCookingTooMuch.dat$Upper)
  )
)
lines(
  ServingsPerRecipe ~ Year,
  data = PlotCookingTooMuch.dat,
  lty = &amp;quot;dashed&amp;quot;,
  col = &amp;quot;blue&amp;quot;,
  lend = 2
)
segments(
  x0 = PlotCookingTooMuch.dat$Year,
  y0 = PlotCookingTooMuch.dat$Lower,
  x1 = PlotCookingTooMuch.dat$Year,
  y1 = PlotCookingTooMuch.dat$Upper
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/achalneupane.github.io/achalneupane.github.io/post/Getting_started_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comments&lt;/h1&gt;
&lt;p&gt;From this plot, it appears that average servings per recipe doesn’t seem to
change from 1936 to 2006.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Identification and Characterization of Mitogen-Activated Protein Kinase (MAPK) Genes in Sunflower (Helianthus annuus L.)</title>
      <link>/achalneupane.github.io/publication/surendra_et_al_2019_plants/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/publication/surendra_et_al_2019_plants/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mitogen-Activated Protein Kinase (MAPK) genes encode proteins that regulate biotic and abiotic stresses in plants through signaling cascades comprised of three major subfamilies: MAP Kinase (MPK), MAPK Kinase (MKK), and MAPKK Kinase (MKKK). The main objectives of this research were to conduct genome-wide identification of MAPK genes in Helianthus annuus and examine functional divergence of these genes in relation to those in nine other plant species (Amborella trichopoda, Aquilegia coerulea, Arabidopsis thaliana, Daucus carota, Glycine max, Oryza sativa, Solanum lycopersicum, Sphagnum fallax, and Vitis vinifera), representing diverse taxonomic groups of the Plant Kingdom. A Hidden Markov Model (HMM) profile of the MAPK genes utilized reference sequences from A. thaliana and G. max, yielding a total of 96 MPKs and 37 MKKs in the genomes of A. trichopoda, A. coerulea, C. reinhardtii, D. carota, H. annuus, S. lycopersicum, and S. fallax. Among them, 28 MPKs and eight MKKs were confirmed in H. annuus. Phylogenetic analyses revealed four clades within each subfamily. Transcriptomic analyses showed that at least 19 HaMPK and seven HaMKK genes were induced in response to salicylic acid (SA), sodium chloride (NaCl), and polyethylene glycol (Peg) in leaves and roots. Of the seven published sunflower microRNAs, five microRNA families are involved in targeting eight MPKs. Additionally, we discussed the need for using MAP Kinase nomenclature guidelines across plant species. Our identification and characterization of MAP Kinase genes would have implications in sunflower crop improvement, and in advancing our knowledge of the diversity and evolution of MAPK genes in the Plant Kingdom.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Roles of Dicers and Argonautes on Sclerotinia sclerotiorum Antiviral Small RNA Processing</title>
      <link>/achalneupane.github.io/talk/pag_2019/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/talk/pag_2019/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ProfilingRNA silencing, also known as RNA interference, is an essential mechanism in plants, animals and fungi that functions in gene regulation and defense against foreign nucleic acids. In fungi, RNA silencing has been shown to function primarily in defense against invasive nucleic acids. RNA-silencing- deficient fungi show increased susceptibility to virus infection. Plant pathogenic fungi also utilize RNA silencing to silence plant host immunity genes through the delivery of fungal small RNAs into plants. This cross-kingdom RNA silencing facilitates fungal infection of plants. Overall, these findings demonstrate the significant contributions of fungal RNA silencing pathways to fungal virulence and viral defense. This study dissects the RNA silencing pathway in S. sclerotiorum by disrupting its key silencing genes using the split-marker recombination method in order to probe the contributions of these genes, specifically argonautes, to fungal virulence and viral defense mechanisms. Following gene disruption, mutants were studied for changes in phenotype, pathogenicity, viral susceptibility, and small RNA processing compared to the wild-type strain, DK3. Among the argonaute mutants, the ∆agl-2 mutant had significantly slower growth and virulence prior to and following virus infection. Additional studies indicated that the virus-infected wild-type strain accumulated virus-derived smallRNAs(vsiRNAs) with distinct patterns of internal and terminal nucleotide mismatches. Additionally, dicer 1 mutant produced less vsiRNA compared to dicer 2 mutant and the wild type strain. These results together support that S. sclerotiorum has robust RNAsilencing mechanisms that function primarily in antiviral defense but also in endogenous gene regulation processes. This finding expands our overall understanding of S. sclerotiorum and has important implications for any current or future uses of mycoviruses as biological control agents, an emerging area of interest in fungal control research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Metatranscriptomic analysis and in silico approach identified mycoviruses in the arbuscular mycorrhizal fungus Rhizophagus spp.</title>
      <link>/achalneupane.github.io/publication/neupane_etal_viruses_2018/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/publication/neupane_etal_viruses_2018/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Arbuscular mycorrhizal fungi (AMF), including Rhizophagus spp., can play important roles in nutrient cycling of the rhizosphere. However, the effect of virus infection on AMF’s role in nutrient cycling cannot be determined without first knowing the diversity of the mycoviruses in AMF. Therefore, in this study, we sequenced the R. irregularis isolate-09 due to its previously demonstrated high efficiency in increasing the N/P uptake of the plant. We identified one novel mitovirus contig of 3685 bp, further confirmed by reverse transcription-PCR. Also, publicly available Rhizophagus spp. RNA-Seq data were analyzed to recover five partial virus sequences from family Narnaviridae, among which four were from R. diaphanum MUCL-43196 and one was from R. irregularis strain-C2 that was similar to members of the Mitovirus genus. These contigs coded genomes larger than the regular mitoviruses infecting pathogenic fungi and can be translated by either a mitochondrial translation code or a cytoplasmic translation code, which was also reported in previously found mitoviruses infecting mycorrhizae. The five newly identified virus sequences are comprised of functionally conserved RdRp motifs and formed two separate subclades with mitoviruses infecting Gigaspora margarita and Rhizophagus clarus, further supporting virus-host co-evolution theory. This study expands our understanding of virus diversity. Even though AMF is notably hard to investigate due to its biotrophic nature, this study demonstrates the utility of whole root metatranscriptome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transcriptional and small RNA responses of the white mold fungus Sclerotinia sclerotiorum to infection by a virulence-attenuating hypovirus</title>
      <link>/achalneupane.github.io/publication/marzano_etal_viruses_2018/</link>
      <pubDate>Mon, 10 Dec 2018 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/publication/marzano_etal_viruses_2018/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mycoviruses belonging to the family Hypoviridae cause persistent infection of many different host fungi. We previously determined that the white mold fungus, Sclerotinia sclerotiorum, infected with Sclerotinia sclerotiorum hypovirus 2-L (SsHV2-L) exhibits reduced virulence, delayed/reduced sclerotial formation, and enhanced production of aerial mycelia. To gain better insight into the cellular basis for these changes, we characterized changes in mRNA and small RNA (sRNA) accumulation in S. sclerotiorum to infection by SsHV2-L. A total of 958 mRNAs and 835 sRNA-producing loci were altered after infection by SsHV2-L, among which &amp;gt;100 mRNAs were predicted to encode proteins involved in the metabolism and trafficking of carbohydrates and lipids. Both S. sclerotiorum endogenous and virus-derived sRNAs were predominantly 22 nt in length suggesting one dicer-like enzyme cleaves both. Novel classes of endogenous small RNAs were predicted, including phasiRNAs and tRNA-derived small RNAs. Moreover, S. sclerotiorum phasiRNAs, which were derived from noncoding RNAs and have the potential to regulate mRNA abundance in trans, showed differential accumulation due to virus infection. tRNA fragments did not accumulate differentially after hypovirus infection. Hence, in-depth analysis showed that infection of S. sclerotiorum by a hypovirulence-inducing hypovirus produced selective, large-scale reprogramming of mRNA and sRNA production.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Genome-Wide Identification of NBS-Encoding Resistance Genes in Sunflower (Helianthus annuus L.)</title>
      <link>/achalneupane.github.io/publication/surendra_et_al_2018_july_genes/</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/publication/surendra_et_al_2018_july_genes/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nucleotide Binding Site-Leucine-Rich Repeat (NBS-LRR) genes encode disease resistance proteins involved in plants&amp;rsquo; defense against their pathogens. Although sunflower is affected by many diseases, only a few molecular details have been uncovered regarding pathogenesis and resistance mechanisms. Recent availability of sunflower whole genome sequences in publicly accessible databases allowed us to accomplish a genome-wide identification of Toll-interleukin-1 receptor-like Nucleotide-binding site Leucine-rich repeat (TNL), Coiled Coil (CC)-NBS-LRR (CNL), Resistance to powdery mildew 8 (RPW8)-NBS-LRR (RNL) and NBS-LRR (NL) protein encoding genes. Hidden Markov Model (HMM) profiling of 52,243 putative protein sequences from sunflower resulted in 352 NBS-encoding genes, among which 100 genes belong to CNL group including 64 genes with RX_CC like domain, 77 to TNL, 13 to RNL, and 162 belong to NL group. We also identified signal peptides and nuclear localization signals present in the identified genes and their homologs. We found that NBS genes were located on all chromosomes and formed 75 gene clusters, one-third of which were located on chromosome 13. Phylogenetic analyses between sunflower and Arabidopsis NBS genes revealed a clade-specific nesting pattern in CNLs, with RNLs nested in the CNL-A clade, and species-specific nesting pattern for TNLs. Surprisingly, we found a moderate bootstrap support (BS = 50%) for CNL-A clade being nested within TNL clade making both the CNL and TNL clades paraphyletic. Arabidopsis and sunflower showed 87 syntenic blocks with 1049 high synteny hits between chromosome 5 of Arabidopsis and chromosome 6 of sunflower. Expression data revealed functional divergence of the NBS genes with basal level tissue-specific expression. This study represents the first genome-wide identification of NBS genes in sunflower paving avenues for functional characterization and potential crop improvement.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/achalneupane.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/achalneupane.github.io/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/achalneupane.github.io/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/achalneupane.github.io/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mycoviruses as Triggers and Targets of RNA Silencing in White Mold Fungus Sclerotinia sclerotiorum</title>
      <link>/achalneupane.github.io/publication/mochama_et_al/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/publication/mochama_et_al/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This study aimed to demonstrate the existence of antiviral RNA silencing mechanisms in Sclerotinia sclerotiorum by infecting wild-type and RNA-silencing-deficient strains of the fungus with an RNA virus and a DNA virus. Key silencing-related genes were disrupted to dissect the RNA silencing pathway. Specifically, dicer genes (dcl-1, dcl-2, and both dcl-1/dcl-2) were displaced by selective marker(s). Disruption mutants were then compared for changes in phenotype, virulence, and susceptibility to virus infections. Wild-type and mutant strains were transfected with a single-stranded RNA virus, SsHV2-L, and copies of a single-stranded DNA mycovirus, SsHADV-1, as a synthetic virus constructed in this study. Disruption of dcl-1 or dcl-2 resulted in no changes in phenotype compared to wild-type S. sclerotiorum; however, the double dicer mutant strain exhibited significantly slower growth. Furthermore, the Δdcl-1/dcl-2 double mutant, which was slow growing without virus infection, exhibited much more severe debilitation following virus infections including phenotypic changes such as slower growth, reduced pigmentation, and delayed sclerotial formation. These phenotypic changes were absent in the single mutants, Δdcl-1 and Δdcl-2. Complementation of a single dicer in the double disruption mutant reversed viral susceptibility to the wild-type state. Virus-derived small RNAs were accumulated from virus-infected wild-type strains with strand bias towards the negative sense. The findings of these studies indicate that S. sclerotiorum has robust RNA silencing mechanisms that process both DNA and RNA mycoviruses and that, when both dicers are silenced, invasive nucleic acids can greatly debilitate the virulence of this fungus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gene Discovery in Acute Myeloid Leukaemia: somatic and germline mutations.</title>
      <link>/achalneupane.github.io/talk/leo_et_al/</link>
      <pubDate>Sun, 21 Aug 2016 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/leo_et_al/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;
:&lt;/p&gt;

&lt;p&gt;Gene and mutation discovery in cancers is typically performed by sequencing somatic and germline samples from the same individuals and subtracting germline mutations.
However, in biobanked acute myeloid leukaemia (AML) samples, matched germline DNA is
frequently unavailable. When germline samples are available, sequencing both germline and somatic samples for each patient significantly increases costs. This study explores whether it is possible to utilise unrelated germline controls to identify ocogenic drivers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Methods&lt;/strong&gt;
:&lt;/p&gt;

&lt;p&gt;Whole exome sequencing was performed on somatic samples from 188 adults and children with AML (n=144 and n=44 respectively) and 429 control germline samples. The discovery process includes a rigorous statistical genetics approach whereby the AML cases and controls are age and ethnically matched. Residual population stratification is controlled by appropriate covariates. Burden tests are performed to detect mutations enriched in somatic samples compared to the unrelated germline controls. Statistical significance is determined by Q-Q plots against the null hypothesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;
:&lt;/p&gt;

&lt;p&gt;Burden tests that included damaging protein coding mutations identified genes
previously described with recurrent mutations in AML (e.g.
NPM1, DNMT3A, IDH2, NRAS, RUNX1, FLT3, IDH1, TET2, ASXL1) thus validating this bioinformatics approach.
Furthermore, this methodology resulted in the detection of several additional novel genes not previously identified in AML.
These mutations are currently undergoing conventional validation by Sanger sequencing, as well as bioinformatic validation using whole genome sequencing data from 128 of the same AML samples sequenced using the Complete Genomics platform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;
:&lt;/p&gt;

&lt;p&gt;We demonstrate that this methodology can:
1) Identify technical artefacts from sequencing and alignment.
2) Enable statistical modelling of allele frequencies that can identify low frequency clones which are undetectable using standard genotyping approaches.
3) Provide a purely statistical approach to gene discovery, agnostic to mutation type
(synonymous, coding, non-coding etc).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;
:&lt;/p&gt;

&lt;p&gt;In summary, we have performed whole exome and whole genome sequencing on the largest cohort of AML samples in Australia. Innovative bioinformatic analysis has detected all previously identified, somatically mutated AML genes and has discovered a number of potentially significant novel mutations. The functional and prognostic impact of these events and the subsequent functional investigations will be discussed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Updates</title>
      <link>/achalneupane.github.io/project/statistical-programming/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/achalneupane.github.io/project/statistical-programming/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MEK inhibitor resistance in acute myeloid leukaemia</title>
      <link>/achalneupane.github.io/talk/mek_brisbane_2016/</link>
      <pubDate>Wed, 16 Mar 2016 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/mek_brisbane_2016/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mutations of the RAS family of genes are frequent events in AML, occurring in 10% of children and 10-20% of adults.
NRASmutations promote proliferation through activation of the Ras/Raf/MEK/ERK signalling pathway. Several MEK
inhibitors have shown promising pre-clinical activity in AML, with a number of compounds currently in adult phase I/II
clinical trials, including AZD6244, GSK1120212 and AS703026. Given not all patients respond to MEK inhibitor
treatment we undertook a comprehensive preclinical evaluation of MEK inhibitors in AML and developed a clinically
relevant model of resistance.
The in vitroefficacy of 7 MEK inhibitors was determined using a diverse panel of 6 paediatric and 5 adult AML cell lines
(Table 1). All AML cell lines were sensitive to at least one MEK inhibitor with the exception of the Down syndrome
associated AML line, CMK, and the adult erythroblastic line, HEL, that showed overt resistance (IC50 &amp;gt;20μM) to MEK
inhibition (Table 1). In sensitive cell lines, the reduced proliferation was associated with apoptosis, as assessed by
Annexin V+ staining. To confirm mechanism of action, inhibition of MEK phosphorylation as well as the downstream
kinase, pERK, were assessed by immunoblotting. The level of basal MEK activation was variable across the cell line
panel and pMEK was increased upon exposure to active MEK inhibitors. In contrast, levels of pERK were reduced
suggesting that MEK inhibitors may disrupt the interaction of MEK with its downstream transducers rather than a direct
inhibition of MEK phosphorylation.
Molecular and clinical resistance to kinase inhibitors is well described for targets such as FLT3 and BCR-ABL1. Since
clinical responses to MEK inhibitors have been variable, we investigated the potential mechanisms of resistance to
MEK inhibitors in vitro. Long-term culture of THP-1 cells (MLL-rearranged, NRASmutated) with AZD6244 and
AS703026, resulted in high-level resistance. Importantly, cells displayed cross-resistance not only to these two
compounds but also a third MEK inhibitor, GSK1120212 (Table 2). Resistance was associated with reduced basal
pMEK expression. In order to establish the mechanism of resistance we performed comprehensive mutation and gene
expression analyses utilising whole-exome sequencing and RNAseq respectively. These data revealed a spectrum of
acquired molecular aberrations common to both resistant cell lines compared to the parental THP1 cells. Together,
these data indicate that whilst MEK inhibition is a promising strategy to treat AML, resistance to one MEK inhibitor may
lead to cross-resistance to other compounds targeting MEK.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sensitivity of Whole Exome Sequencing for the Detection of FLT3-ITD and NPM1 Mutations in Acute Myeloid Leukaemia</title>
      <link>/achalneupane.github.io/talk/esh_2015_buddapest/</link>
      <pubDate>Thu, 10 Sep 2015 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/esh_2015_buddapest/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Whole exome sequencing (WES) can detect a high proportion of cancer causing genetic mutations, and therefore has the potential to replace targeted molecular assays and identify additional novel, patient-specific mutations. To test the ability of WES to detect variants of clinical consequence, we compared WES findings to results for FLT3-ITD and NPM1mut obtained by routine pathology testing (high-resolution fragment analysis (HRFA) and Matrix Assisted Laser Desorption/Ionization – Time of Flight Mass Spectrometry (MALDI-TOF) respectively), and to point mutation findings in DNMT3A, IDH1/2, JAK2, KRAS, NRAS and MPL, as determined by Sequenom, across a cohort of 188 AML samples from adults (n
= 144) and children (n = 44). All subjects provided written informed consent, and the study protocols had been approved by institutional research ethics committees. Median age at diagnosis was 51 years, (range 1-89) and median bone marrow blast percentage was 76% (range 8-100).
A discovery phase using 96 samples developed genotype algorithms and ascertained their sensitivity. NPM1mut were reliably detected using WES, with 100% (n=25 positive) concordance with MALDI-TOF. WES and Sequenom were 99.4% concordant over the selected gene panels (1594 measurements). The FLT3-ITD false negative rate initially exceeded 50%, leading us to optimize our discovery algorithms and develop a calibration curve which specified the sequence coverage needed over the FLT3-ITD region to reliably genotype at any allelic ratio (AR). We demonstrate that our algorithms can reliably detect FLT3-ITD with AR &amp;gt; 0.05 using exome capture with less than 100x coverage over the FLT3-ITD region.
Additionally, we tested the algorithms over another 900 germline controls with no false positive measurements.
In a validation phase using 78 samples, all NPM1mut samples (n=20 positive) were detected, including an additional NPM1mut (type A) initially missed using MALDI-TOF. Concordance with Sequenom again exceeded 99.5% (comparable to the error rate expected in this Sequenom panel). For FLT3-ITD, &lt;sup&gt;18&lt;/sup&gt;&amp;frasl;&lt;sub&gt;21&lt;/sub&gt; HRFA-positive samples were detected with all 3 false-negatives falling outside the predicted sensitivity threshold; two samples with AR &amp;lt;0.01 (which would require sequencing coverage of 500x for detection) and one with AR = 0.4 but with a sequence coverage of only 38x over the FLT3-ITD.
In conclusion, we have performed WES on one of the largest cohorts of adult and paediatric AML described to date and demonstrate that clinically relevant mutations, including FLT3-ITD and NPM1mut, can be simultaneously detected with a single exome capture platform with accuracy similar to or better than current methods.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transcriptome analysis of western corn rootworm larvae and eggs</title>
      <link>/achalneupane.github.io/talk/wcr_neupane/</link>
      <pubDate>Mon, 14 Apr 2014 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/wcr_neupane/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The     western     corn     rootworm,     Diabrotica     virgifera     virgifera(Coloeptera: Chrysomelidae), is one of the most devastating pests of corn causing nearly a billion dollars  of  financial  loss  both  in  terms  of  yield  loss  and  treatment  costs.  Although  Coleoptera  is  the  most  diverse  order  of  insects  comprising  more  than  400,000  species,  only  a  few  coleopteran  genomes  and  transcriptomes  (e.g.,  from  &lt;em&gt;Tribolium castaneum&lt;/em&gt;  and &lt;em&gt;Dendroctonus  ponderosae&lt;/em&gt;)  have  been  published  to  date.  The  genome size of haploid D. v.   virgifera is estimated to be ~2.58 GB, one of the largest among beetle species. Its complete genome sequence is currently in the draft stage. In this study, in order to identify the gene sets expressed in their larval stages (when the  most  damage  to  corn  is  caused)  and  to  contribute  to  improving  the  genome  assembly,  we  have  sequenced  and  assembled  transcriptomes  from  egg,  neonate,  and third-instar larval stages of D. v. virgifera using next-generation technologies. In total  ~700  gigabases  were  sequenced.  De  novo  transcriptome  assembly  was  performed using four different short read assemblers for individual  and  pooled  sets  of  reads.  Hybrid  assembly  using  both  Illumina  and  454  reads  was  also  performed.  After  examining  the  assembly  quality  based  on  contig  length  and  annotation  effectiveness  with  similarity  search,  we  chose  the  Trinity  assembly  from  the  pooled dataset  including  163,871  contigs  (the  average  length:  914  bp)  as  the  most  inclusive.  We  identified  and  annotated  genes  encoding  chemoreceptors,  gamma-aminobutyric  acid  (GABA)  type  A  receptor,  and  glycoside  hydrolase  families.  Compared  to  the  sequences  found  in  the  draft  genome,  we  observed  variations  in  sequences  as  well  as  in  the  number  of  introns.  We  also  examined  conservation  of  gene   structures   in   chemoreceptors   from   closely   related   insect   lineages.   Our   transcriptome sequences can contribute toward improved quality of the &lt;em&gt;D&lt;/em&gt;. &lt;em&gt;v&lt;/em&gt;. &lt;em&gt;virgiferagenome&lt;/em&gt; assembly and annotations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Genetic diversity and population structure of sickleweed(Falcaria vulgaris; Apiaceae) in the upper Midwest USA</title>
      <link>/achalneupane.github.io/publication/piya_et_al_biological_invasions/</link>
      <pubDate>Sat, 22 Feb 2014 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/publication/piya_et_al_biological_invasions/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sickleweed (Falcaria vulgaris), an intro-duced species native to Europe and Asia, grows as anaggressive weed in some areas of the upper Midwest inthe United States. We are reporting genetic diversityand  population  structure  of  sickleweed  populationsusing microsatellite markers and nuclear and chloro-plast   DNA   sequences.   Populations   showed   highgenetic  differentiation  but  did  not  show  significantgeographic  structure,  suggesting  random  establish-ment of different genotypes at different sites was likelydue to human mediated multiple introductions. Threegenetic  clusters  revealed  by  microsatellite  data  andpresence of six chlorotypes supported our hypothesisof multiple introductions. Chloroplast DNA sequencedata  revealed  six  chlorotypes  nested  into  two  mainlineages   suggesting   at   least   two   introductions   ofsickleweed  in  the  upper  Midwest.  Some  individualsexhibited more than two alleles at several microsatel-lite  loci suggesting  occurrence  of polyploidy, whichcould be a post-introduction development to mitigatethe  inbreeding  effects.  High  genetic  variation  in  theintroduced range attributable to multiple introductionsand  polyploidy  may  be  inducing  the  evolution  ofinvasiveness  in  sickleweed.  Results  of  this  studyprovide valuable insights into the evolution of sickle-weed and baseline data for designing proper manage-ment practices for controlling sickleweed in the UnitedStates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolutionary history of mitogen-activated protein kinase (MAPK) genes in Lotus, Medicago, and Phaseolus</title>
      <link>/achalneupane.github.io/publication/mapk_legume/</link>
      <pubDate>Mon, 02 Dec 2013 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/publication/mapk_legume/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mitogen-Activated Protein Kinase (MAPK) genes encode proteins that mediate various signaling pathways associated with biotic and abiotic stress responses in eukaryotes. The MAPK genes form a 3-tier signal transduction cascade between cellular stimuli and physiological responses. Recent identification of soybean MAPKs and availability of genome sequences from other legume species allowed us to identify their MAPK genes. The main objectives of this study were to identify MAPKs in 3 legume species, Lotus japonicus, Medicago truncatula, and Phaseolus vulgaris, and to assess their phylogenetic relationships. We used approaches in comparative genomics for MAPK gene identification and named the newly identified genes following Arabidopsis MAPK nomenclature model. We identified 19, 18, and 15 MAPKs and 7, 4, and 9 MAPKKs in the genome of Lotus japonicus, Medicago truncatula, and Phaseolus vulgaris, respectively. Within clade placement of MAPKs and MAPKKs in the 3 legume species were consistent with those in soybean and Arabidopsis. Among 5 clades of MAPKs, 4 founder clades were consistent to MAPKs of other plant species and orthologs of MAPK genes in the fifth clade-&amp;ldquo;Clade E&amp;rdquo; were consistent with those in soybean. Our results also indicated that some gene duplication events might have occurred prior to eudicot-monocot divergence. Highly diversified MAPKs in soybean relative to those in 3 other legume species are attributable to the polyploidization events in soybean. The identification of the MAPK genes in the legume species is important for the legume crop improvement; and evolutionary relationships and functional divergence of these gene members provide insights into plant genome evolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identification, Nomenclature, and Evolutionary Relationships of Mitogen-Activated Protein Kinase (MAPK) Genes in Soybean</title>
      <link>/achalneupane.github.io/publication/mapk_soybean/</link>
      <pubDate>Sun, 22 Sep 2013 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/publication/mapk_soybean/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mitogen-activated protein kinase (MAPK) genes in eukaryotes regulate various developmental and physiological processes
including those associated with biotic and abiotic stresses. Although MAPKs in some plant species including Arabidopsis have been
identified, they are yet to be identified in soybean. Major objectives of this study were to identify GmMAPKs, assess their evolutionary relationships, and analyze their functional divergence. We identified a total of 38 MAPKs, eleven MAPKKs, and 150 MAPKKKs in
soybean. Within the GmMAPK family, we also identified a new clade of six genes: four genes with TEY and two genes with TQY motifs
requiring further investigation into possible legume-specific functions. The results indicated the expansion of the GmMAPK families
attributable to the ancestral polyploidy events followed by chromosomal rearrangements. The GmMAPK and GmMAPKKK families
were substantially larger than those in other plant species. The duplicated GmMAPK members presented complex evolutionary relationships and functional divergence when compared to their counterparts in Arabidopsis. We also highlighted existing nomenclatural issues,
stressing the need for nomenclatural consistency. GmMAPK identification is vital to soybean crop improvement, and novel insights into
the evolutionary relationships will enhance our understanding about plant genome evolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identification of Mitogen Activated Protein Kinase Family Members in Soybean</title>
      <link>/achalneupane.github.io/talk/bsa_neupane/</link>
      <pubDate>Wed, 11 Jul 2012 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/bsa_neupane/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mitogen Activated Protein Kinases (MAPKs) are serine/threonine specific kinases that are induced by various extracellular and intracellular stimuli, and are involved in signaling pathways in all eukaryotes. In plants, MAPKs are known to regulate various signal transduction pathways including those associated with biotic and abiotic stresses. The MAPK gene members belong to three functionally linked gene families called MAPKs, MAPKKs (MAPK Kinases) and MAPKKKs (MAPKK Kinases). Although, MAPKs of model plants such as &lt;em&gt;Arabidopsis&lt;/em&gt; and &lt;em&gt;Oryza&lt;/em&gt; have been identified and characterized, these genes in soybean (&lt;em&gt;Glycine max&lt;/em&gt;) are yet to be identified. In this study, we used approaches in comparative genomics and bioinformatics for the genome-wide identification of MAPKs, MAPKKs and MAPKKKs in soybean. &lt;em&gt;Arabidopsis&lt;/em&gt; reference sequences were used in protein BLAST to search the putative MAPKs of soybean from publicly available databases. In our in-silico analysis, the redundant sequences were removed to perform unbiased and rigorous phylogenetic analyses. We verified the presence of unique conserved domains and active sites in each putative MAPK gene member manually and by using Geneious and Pfam programs. From our three different confirmatory analyses, we identified 38 MAP Kinases, 11 MAPK Kinases and 115 MAPKK Kinases in soybean. &lt;em&gt;Arabidopsis&lt;/em&gt; nomenclature model was followed to assign numeric subscript to MAPK sequences grouped with &lt;em&gt;Arabidopsis&lt;/em&gt; sequences in phylogenetic tree. Lack of effective code of nomenclature lead us to a conundrum, which will be discussed in the meeting. Universal codes of gene nomenclature are crucial for understanding various signaling pathways. Therefore, an effective code of gene nomenclature is warranted. The results from this study will help us characterize soybean MAP Kinases paving avenue for the functional analyses of different cellular and physiological pathways in the order these genes function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Population Genetics of *Falcaria vulgaris* (Sickleweed) in North America</title>
      <link>/achalneupane.github.io/talk/piya_bsa/</link>
      <pubDate>Wed, 11 Jul 2012 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/piya_bsa/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction history and spread of Falcaria vulgaris Bernh. (Apiaceae) in the United States based on herbarium records</title>
      <link>/achalneupane.github.io/talk/piya_sd_academy/</link>
      <pubDate>Fri, 13 Apr 2012 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/piya_sd_academy/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Nomenclatural Conundrum: Applying Existing Nomenclature to the Identification of Soybean (*Glycine max*) MAP Kinase Genes</title>
      <link>/achalneupane.github.io/talk/neupane_aspb/</link>
      <pubDate>Sat, 24 Mar 2012 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/neupane_aspb/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inferring Introduction History and Spread of Falcaria vulgaris Bernh.(Apiaceae) in the United States Based on Herbarium Records</title>
      <link>/achalneupane.github.io/publication/piya_et_al_proceedings_sd/</link>
      <pubDate>Wed, 22 Feb 2012 00:00:00 -0600</pubDate>
      <guid>/achalneupane.github.io/publication/piya_et_al_proceedings_sd/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Herbarium records were studied to infer the introduction history and spread of the exotic Eurasian sickleweed (Falcaria vulgaris Bernh.) in the United States. The spread of the plant was reconstructed using the location of early collections as the possible sites of primary introduction, and the location of subsequent collections as potential pathways along which this species spread. Herbarium records indicate that sickleweed was first introduced no later than 1922, and independent introduction of this plant took place in the East Coast and in the Midwest of the United States. The species has spread to 37 counties of 15 states of the United States. No recent sickleweed record has been reported for the last 17 years in the US except Iowa, Nebraska and South Dakota. The plant has been characterized as an aggressive weed by experts in the latter two states, where it is already well established and has infested the Fort Pierre National Grassland and Buffalo Gap National Grassland in South Dakota, and is reported from several sites along Nebraska roadsides. It is essential to verify the existence of sickleweed in the areas from where the herbarium specimens were previously collected to help identify the areas at risk. Control strategies need to be implemented and policy should be developed to establish the participation of public lands managers, transportation departments and private land-owners to control and manage this species before it becomes a more widespread invader.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anticancer Activity of Novel Ionic Triorganotin Derivatives against MDA-MB 231 Breast Cancer Cells</title>
      <link>/achalneupane.github.io/talk/hbcu_neupane/</link>
      <pubDate>Thu, 29 Oct 2009 00:00:00 -0500</pubDate>
      <guid>/achalneupane.github.io/talk/hbcu_neupane/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The goal of this project was to study the responses of antibodies
against four influenza A viruses; PR8, RV6, Swine and J1. RV6,
Swine and J1 differ from PR8 by 1 amino acid (99.9% similarity),
20 amino acids, and more than 100 amino acids, respectively.
Despite several attempts of early diagnosis and chemotherapy,
breast cancer is one of the leading causes of cancer deaths in
United States claiming almost 40610 lives per year. Breast cancer
is more common in white women than in African American women
but the survival rate for 5 years for African American women is
comparatively less which is 77% than 90 % for white women.
Higher concentration of estrogen secretion has also been
associated with the risk factors of breast cancer.
These viruses resemble the different viruses that circulate in
people and that can have varying numbers of amino acid
differences, and we wanted to examine how these difference
affect the ability of antibodies to bind to different viruses using
different techniques. ELISA (Enzyme Linked Immunosorbent
Assay) was used to determine whether a particular antibody is
present in a blood sample and ELISPOT (ELISPOT-Enzyme-linked
Immunospot assay) was used for enumeration of B cells secreting
specific antibody.
In recent studies, metal-based anticancer drugs are found to be
very effective in the death of cancer cells proving it as a very
useful cancer chemotherapeutic. In this study we have
synthesized ionic triorganotin compounds with increased solubility
due to their partially ionic characteristic and tested their anticancer activity using MDA-MB 231 breast cancer cells.
Methods: Three compounds (1) Triphenyltin Hydroxide&lt;a href=&#34;2&#34; target=&#34;_blank&#34;&gt;parent
compound&lt;/a&gt; CA11 and (3) CA 32 derivatives were tested.
MDA-MB 231 cells were plated in 96-wellplate and treated with
varying concentration (1ng to 100ug) of various triorganotin
derivatives for 24, 48 and 72 hours. A combination of trypan blue
dye exclusion and WST-1 cell proliferation reagent was used to&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
